





# NoSQL

> NoSQL(NoSQL = Not Only SQL )，意即"不仅仅是SQL"。在现代的计算系统上每天网络上都会产生庞大的数据量。这些数据有很大一部分是由关系数据库管理系统（RDBMS）来处理。 1970年 E.F.Codd's提出的关系模型的论文 "A relational model of data for large shared data banks"，这使得数据建模和应用程序编程更加简单。

> 通过应用实践证明，关系模型是非常适合于客户服务器编程，远远超出预期的利益，今天它是结构化数据存储在网络和商务应用的主导技术。NoSQL 是一项全新的数据库革命性运动，早期就有人提出，发展至2009年趋势越发高涨。NoSQL的拥护者们提倡运用非关系型的数据存储，相对于铺天盖地的关系型数据库运用，这一概念无疑是一种全新的思维的注入。

## 关系型数据库遵循ACID规则

事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性：

**1、A (Atomicity) 原子性**

原子性很容易理解，也就是说事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚。比如银行转账，从A账户转100元至B账户，分为两个步骤：1）从A账户取100元；2）存入100元至B账户。这两步要么一起完成，要么一起不完成，如果只完成第一步，第二步失败，钱会莫名其妙少了100元。

**2、C (Consistency) 一致性**

一致性也比较容易理解，也就是说数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束。例如现有完整性约束a+b=10，如果一个事务改变了a，那么必须得改变b，使得事务结束后依然满足a+b=10，否则事务失败。

**3、I (Isolation) 独立性**

所谓的独立性是指并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。比如现在有个交易是从A账户转100元至B账户，在这个交易还未完成的情况下，如果此时B查询自己的账户，是看不到新增加的100元的。

**4、D (Durability) 持久性**

持久性是指一旦事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失。

## 分布式系统

分布式系统（distributed system）由多台计算机和通信的软件组件通过计算机网络连接（本地网络或广域网）组成。

分布式系统是建立在网络之上的软件系统。正是因为软件的特性，所以分布式系统具有高度的内聚性和透明性。

因此，网络和分布式系统之间的区别更多的在于高层软件（特别是操作系统），而不是硬件。

分布式系统可以应用在不同的平台上如：Pc、工作站、局域网和广域网上等。

## 分布式计算的优点

> **可靠性（容错） ：分布式计算系统中的重要的优点是可靠性。一台服务器的系统崩溃不影响到其余服务器**
>
> **可扩展性：**在分布式计算系统可以根据需要增加更多的机器。
>
> **资源共享：**共享数据是必不可少的应用，如银行，预订系统。
>
> **灵活性：**由于该系统是非常灵活的，它很容易安装，实施和调试新的服务。
>
> **更快的速度：**分布式计算系统可以有多台计算机的计算能力，使得它比其他系统有更快的处理速度。
>
> **开放系统：**由于它是开放的系统，本地或者远程都可以访问到该服务。
>
> **更高的性能：**相较于集中式计算机网络集群可以提供更高的性能（及更好的性价比）。

## 分布式计算的缺点

> **故障排除：**故障排除和诊断问题。
>
> **软件：**更少的软件支持是分布式计算系统的主要缺点。
>
> **网络：**网络基础设施的问题，包括：传输问题，高负载，信息丢失等。
>
> **安全性：**开放系统的特性让分布式计算系统存在着数据的安全性和共享的风险等问题。

## 什么是NoSQL?

NoSQL，指的是非关系型的数据库。NoSQL有时也称作Not Only SQL的缩写，是对不同于传统的关系型数据库的数据库管理系统的统称。

NoSQL用于超大规模数据的存储。（例如谷歌或Facebook每天为他们的用户收集万亿比特的数据）。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。

## 为什么使用NoSQL ?

> 今天我们可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL 数据库的发展却能很好的处理这些大的数据。

## CAP定理（CAP theorem）

在计算机科学中, CAP定理（CAP theorem）, 又被称作 布鲁尔定理（Brewer's theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点:

- **一致性(Consistency)** (所有节点在同一时间具有相同的数据)
- **可用性(Availability)** (保证每个请求不管成功或者失败都有响应)
- **分区容错性(Partition tolerance)** (系统中任意信息的丢失或失败不会影响系统的继续运作)

CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。

因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类：

- CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。
- CP - 满足一致性，分区容忍性的系统，通常性能不是特别高。
- AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304231047195.png" alt="image-20230423104739112" style="zoom:67%;" />

## NoSQL的优点/缺点

优点:

- \- 高可扩展性
- \- 分布式计算
- \- 低成本
- \- 架构的灵活性，半结构化数据
- \- 没有复杂的关系

缺点:

- \- 没有标准化
- \- 有限的查询功能（到目前为止）
- \- 最终一致是不直观的程序

## BASE

BASE：Basically Available, Soft-state, Eventually Consistent。 由 Eric Brewer 定义。

CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。BASE是NoSQL数据库通常对可用性及一致性的弱要求原则:

- Basically Available --基本可用
- Soft-state --软状态/柔性事务。 "Soft state" 可以理解为"无连接"的, 而 "Hard state" 是"面向连接"的
- Eventually Consistency -- 最终一致性， 也是 ACID 的最终目的。

## ACID vs BASE

| ACID                    | BASE                                  |
| :---------------------- | :------------------------------------ |
| 原子性(**A**tomicity)   | 基本可用(**B**asically **A**vailable) |
| 一致性(**C**onsistency) | 软状态/柔性事务(**S**oft state)       |
| 隔离性(**I**solation)   | 最终一致性 (**E**ventual consistency) |
| 持久性 (**D**urable)    |                                       |

## NoSQL 数据库分类

| 类型          | 部分代表                                         | 特点                                                         |
| ------------- | ------------------------------------------------ | ------------------------------------------------------------ |
| 列存储        | HbaseCassandraHypertable                         | 顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。 |
| 文档存储      | MongoDBCouchDB                                   | 文档存储一般用类似json的格式存储，存储的内容是文档型的。这样也就有机会对某些字段建立索引，实现关系数据库的某些功能。 |
| key-value存储 | Tokyo Cabinet / TyrantBerkeley DBMemcacheDBRedis | 可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能） |
| 图存储        | Neo4JFlockDB                                     | 图形关系的最佳存储。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。 |
| 对象存储      | db4oVersant                                      | 通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。 |
| xml数据库     | Berkeley DB XMLBaseX                             | 高效的存储XML数据，并支持XML的内部查询语法，比如XQuery,Xpath。 |



# 安装、启动、配置

## Linux安装mongodb

下载官网[MongoDB Community Download | MongoDB

https://www.mongodb.com/try/download/community

https://www.mongodb.com/download-center/community/releases/archive

学习：Centos8安装MongoDB(https://www.cnblogs.com/phdeblog/p/14076066.html)

注意选择版本，centos8有对应centos8的版本

### 安装配置

上传压缩包到Linux中，解压到当前目录： 

```apl
tar -xvf mongodb-linux-x86_64-4.0.10.tgz
```

移动解压后的文件夹到指定的目录中： 

```apl
mv mongodb-linux-x86_64-4.0.10 /usr/local/mongodb
```

新建几个目录，分别用来存储数据和日志：

注意：他们是在根目录里

```apl
#数据存储目录
mkdir -p /mongodb/single/data/db
#日志存储目录
mkdir -p /mongodb/single/log
```

添加环境变量并刷新

```apl
# 1、打开配置文件
vim  ~/.bash_profile
# 2、在配置文件中加入
export PATH=/usr/local/mongodb/bin:$PATH 
# 3、保存退出，刷新
source ~/.bash_profile
```

配置yml格式的配置文件

```apl
vi /mongodb/single/mongod.conf
```

```yml
systemLog:
  #MongoDB发送所有日志输出的目标指定为文件
  # #The path of the log file to which mongod or mongos should send all diagnostic logging information
  destination: file
  #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径
  path: "/mongodb/single/log/mongod.log"
  #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。
  logAppend: true
storage:
  #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。
  ##The directory where the mongod instance stores its data.Default Value is "/data/db".
  dbPath: "/mongodb/single/data/db"
  journal:
    #启用或禁用持久性日志以确保数据文件保持有效和可恢复。
    enabled: true
processManagement:
  #启用在后台运行mongos或mongod进程的守护进程模式。
  fork: true
net:
  #服务实例绑定的IP，默认是localhost
  bindIp: 0.0.0.0
  #bindIp
  #绑定的端口，默认是27017
  port: 27017
```

注意：上面配置的ip地址是要在linux有网才能运行，不然会报错

```sh
# 正常启动
/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf 
# 配置完环境变量启动
mongod -f /mongodb/single/mongod.conf 
```

出现successful就是成功

此时再输入mongo就能进入数据库

```apl
mongo
```

连接测试

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20211109091015159.png" alt="image-20211109091015159" style="zoom:80%;" />

关闭数据库

```c
mongod --shutdown --dbpath=/mongodb/single/data/db
```

### Navicate显示系统库

### 配置环境变量

```sh
# 1、打开配置文件
vim  ~/.bash_profile
# 2、在配置文件中加入
export PATH=/usr/local/mongo/bin:$PATH 
# 3、保存退出，刷新
source ~/.bash_profile
```

配置完成后启动

```c
mongod
```

```c
mongo
```

### 查看系统进程(Linux)

```
ps -ef |grep mongod
```

如果连接不上，关闭防火墙

```sh
#查看防火墙状态
systemctl status firewalld
#临时关闭防火墙
systemctl stop firewalld
#开机禁止启动防火墙
systemctl disable firewalld
```

### 停止关闭数据库 

停止服务的方式有两种：

快速关闭和标准关闭，下面依次说明： 

快速关闭方法（快速，简单，数据可能会出错） 

目标：通过系统的kill命令直接杀死进程： 杀完要检查一下，避免有的没有杀掉。

```
#通过进程编号关闭节点
kill -2 54410
```

【补充】 如果一旦是因为数据损坏，则需要进行如下操作（了解）： 

1）删除lock文件：

```typescript
rm -f /mongodb/single/data/db/*.lock
```

2）修复数据：

```
/usr/local/mongo/bin/mongod --repair --dbpath=/mongodb/single/data/db
```

标准的关闭方法（数据不容易出错，但麻烦）

目标：通过mongo客户端中的shutdownServer命令来关闭服务 主要的操作步骤参考如下：

```sh
# 客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。
mongo --port 27017
#切换到admin库
use admin
# 关闭服务
db.shutdownServer()
```

## win版本安装

官网：https://www.mongodb.com/try/download/community

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304221551041.png" alt="image-20230422155146963" style="zoom:80%;" />

> 选择complete完整安装，设置data和log路径

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304221555926.png" alt="image-20230422155525853" style="zoom:80%;" />

> 搜索进入服务，查看mongoDB是否启动

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304221559803.png" alt="image-20230422155916737" style="zoom:80%;" />



# MongoDB 基础

## 基本描述

> MongoDB 是一个基于 **分布式文件存储** 的开源 NoSQL 数据库系统，由 **C++** 编写的。MongoDB 提供了 **面向文档** 的存储方式，操作起来比较简单和容易，支持“**无模式**”的数据建模，可以存储比较复杂的数据类型，是一款非常流行的 **文档类型数据库** 。

> 在高负载的情况下，MongoDB 天然支持水平扩展和高可用，可以很方便地添加更多的节点/实例，以保证服务性能和可用性。在许多场景下，MongoDB 可以用于代替传统的关系型数据库或键/值存储方式，皆在为 Web 应用提供可扩展的高可用高性能数据存储解决方案。

## 概念解析

| SQL术语/概念 | MongoDB术语/概念 | 解释/说明                           |
| :----------- | :--------------- | :---------------------------------- |
| database     | database         | 数据库                              |
| table        | collection       | 数据库表/集合                       |
| row          | document         | 数据记录行/文档                     |
| column       | field            | 数据字段/域                         |
| index        | index            | 索引                                |
| table joins  |                  | 表连接,MongoDB不支持                |
| primary key  | primary key      | 主键,MongoDB自动将_id字段设置为主键 |

通过下图实例，我们也可以更直观的了解Mongo中的一些概念：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304231055934.png" alt="image-20230423105505870" style="zoom:80%;" />

## 数据库、文档、集合

### 数据库

> 数据库用于存储所有集合，而集合又用于存储所有文档。一个 MongoDB 中可以创建多个数据库，每一个数据库都有自己的集合和权限。mongodb中可以建立多个数据库。MongoDB的默认数据库为"db"，该数据库存储在data目录中。单个实例可以容纳多个独立数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中MongoDB 预留了几个特殊的数据库。
>

> **admin** : admin 数据库主要是保存 root 用户和角色。例如，system.users 表存储用户，system.roles 表存储角色。一般不建议用户直接操作这个数据库。将一个用户添加到这个数据库，且使它拥有 admin 库上的名为 dbAdminAnyDatabase 的角色权限，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如关闭服务器。

> **local** : local 数据库是不会被复制到其他分片的，因此可以用来存储本地单台服务器的任意 collection。一般不建议用户直接使用 local 库存储任何数据，也不建议进行 CRUD 操作，因为数据无法被正常备份与恢复。

> **config** : 当 MongoDB 使用分片设置时，config 数据库可用来保存分片的相关信息。

> **test** : 默认创建的测试库，连接 mongod 服务时，如果不指定连接的具体数据库，默认就会连接到 test 数据库

### 文档

> 文档是一组键值(key-value)对(即 BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。一个简单的文档例子如下：

```json
{
    "site":"www.runoob.com", 
    "name":"菜鸟教程"
}
```

下表列出了 RDBMS 与 MongoDB 对应的术语：

| RDBMS              | MongoDB                           |
| :----------------- | :-------------------------------- |
| 数据库             | 数据库                            |
| 表格               | 集合                              |
| 行                 | 文档                              |
| 列                 | 字段                              |
| 表联合             | 嵌入文档                          |
| 主键               | 主键 (MongoDB 提供了 key 为 _id ) |
| 数据库服务和客户端 |                                   |
| Mysqld/Oracle      | mongodb                           |
| mysql/sqlplus      | mongo                             |

需要注意的是：

> 1. 文档中的键/值对是有序的。
> 2. 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。
> 3. MongoDB区分类型和大小写。
> 4. MongoDB的文档不能有重复的键。
> 5. 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。

### 集合

> MongoDB 集合存在于数据库中，**没有固定的结构**，也就是 **无模式** 的，这意味着可以往集合插入不同格式和类型的数据。不过，通常情况相爱插入集合中的数据都会有一定的关联性。
>

比如，我们可以将以下不同数据结构的文档插入到集合中：

```sql
{"site":"www.baidu.com"}
{"site":"www.google.com","name":"Google"}
{"site":"www.runoob.com","name":"菜鸟教程","num":5}
```

## 应用场景

> 社交场景，使用 MongoDB **存储存储用户信息**，以及**用户发表的朋友圈信息**，通过**地理位置索引实现附近的人、地点**等功能。
>

> 游戏场景，使用 MongoDB **存储游戏用户信息**，**用户的装备**、**积分**等直接以内嵌文档的形式存储，方便查询、高效率存储和访问。
>

> 物流场景，使用 MongoDB 存储订单信息，**订单状态在运送过程中会不断更新**，以 MongoDB 内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来。

> 物联网场景，使用 MongoDB **存储所有接入的智能设备信息**，以及**设备汇报的日志信息**，并对这些信息进行多维度的分析。
>

> 视频直播，使用 MongoDB **存储用户信息**、**点赞互动信息**等。
>

这些应用场景中，数据操作方面的共同特点是：

> （1）**数据量大**
>
> （2）**写入操作频繁**（读写都很频繁）
>
> （3）**价值较低的数据，对事务性要求不高**

对于这样的数据，我们更适合使用MongoDB来实现数据的存储。

## MySQL表转换

在mysql数据库中，表鼠标右键选择导出向导

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20211123105335479.png" alt="image-20211123105335479" style="zoom:67%;" />

后面就是选择自己要导出的表一直下一步即可

在mongodb数据库，右键导入向导，同样选择Excel

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20211123105525824.png" alt="image-20211123105525824" style="zoom:80%;" />

一直下一步即可

## 体系架构

![image-20220410153019208](https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220410153019208.png)

# 基本常用命令

## shell连接

注意要提前配置好环境变量

Linux上启动

```c
/usr/local/mongo/bin/mongod -f /mongodb/single/mongod.conf
```

Linux和win上都可以

```c
mongo --host=127.0.0.1 --port=27017 
```

```c
mongo
```

```c
mongo --port=27017 
```

加上端口号的目的是以后连接linux的mongodb就可以把127.0.0.1换成linux的端口号

## 特殊数据库

> **admin**： 从权限的角度来看，这是"root"数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。

> **local:** 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合

> **config**: 当Mongo用于分片设置时，confifig数据库在内部使用，用于保存分片的相关信息。

## 数据库操作

### 选择和创建数据库

> 注意 在 MongoDB 中，集合只有在内容插入后才会创建! 就是说，创建集合(数据表)后要再插入一个文档(记录)，集合才会真正创建。**如果数据库不存在则自动创建**

```sql
# 选择和创建数据库的语法格式
use 数据库名称 
# 创建 articledb 数据库
use articledb
```

### 查看数据库

> MongoDB 中默认的数据库为 test，如果你没有选择数据库，集合将存放在 test 数据库中

```sql
# 查看正在使用的数据库
db  
# 查看所有数据库
show dbs
```

### 删除数据库

MongoDB 删除数据库的语法格式如下：

```apl
db.dropDatabase()  
```

提示：主要用来删除已经持久化的数据库

### 查看当前库中的表

```apl
show collections 或  show tables 
```

## 集合操作

集合，类似关系型数据库中的表。可以显示的创建，也可以隐式的创建。

### 集合的显式创建(了解)

```sql
db.createCollection(name) 
```

参数说明：name: 要创建的集合名称

例如：创建一个名为 mycollection 的普通集合。

```sql
db.createCollection("mycollection")
```

### 集合的隐式创建⭐

> 当向一个集合中插入一个文档的时候，**如果集合不存在，则会自动创建集合**。通常我们使用隐式创建文档即可。
>

### 集合的删除

集合删除语法格式如下：

```sql
db.collection.drop()  或 db.集合.drop() 
```

## 固定集合

> MongoDB 固定集合（Capped Collections）是性能出色且有着固定大小的集合，对于大小固定，我们可以想象其就像一个环形队列，当集合空间用完后，再插入的元素就会覆盖最初始的头部的元素！应用场景：储存日志信息，缓存一些少量的文档

> 固定集合使用固定大小的预分配空间（Pre-allocated Space），它们在创建时指定集合最多可以存储的文档数量或者是数据占用空间的大小。当集合的大小达到限制时，如果继续插入文档，最早插入的文档将会被覆盖。因此，固定集合适用于需要轮询（轮询的间隔时间可以是任意时间段）数据的应用场景，例如日志文件、定时任务等等。

### 创建固定集合

```sh
db.createCollection(name, { capped: true, size: size, max: max })
```

其中，`name`是固定集合的名称，`capped`设置为`true`表示创建的是固定集合，`size`表示固定集合的大小，以字节为单位，`max`表示固定集合中可以存储的最大文档数量。

**例如，如果要创建一个最多可以存储1000个文档的固定集合**：

```sql
db.createCollection("myCollection", { capped: true, size: 1048576, max: 1000 })
```

上述示例中，`size`参数设置为1 MB，`max`参数设置为1000个文档。

> 需要注意的是，一旦创建了固定集合，就无法再将其转换为非固定集合。如果您需要更改固定集合的大小，需要先删除集合，然后重新创建一个新的固定集合。固定集合的优点是它们对磁盘空间的利用率更高，可以在某些场景下提高性能，但是由于其大小是固定的，因此在一些应用场景下可能会有限制。

**判断集合是否为固定集合**

```sql
db.cappedLogCollection.isCapped()
```

**如果需要将已存在的集合转换为固定集合可以使用以下命令**

```sql
db.runCommand({"convertToCapped":"posts",size:10000})
```

以上代码将我们已存在的 posts 集合转换为固定集合。

### 固定集合查询

固定集合文档按照插入顺序储存的,默认情况下查询就是按照插入顺序返回的,也可以使用$natural调整返回顺序。

```sql
db.cappedLogCollection.find().sort({$natural:-1})
```

> 可以插入及更新,但更新不能超出collection的大小,否则更新失败,不允许删除,但是可以调用drop()删除集合中的所有行,但是drop后需要显式地重建集合。在32位机子上一个cappped collection的最大值约为482.5M,64位上只受系统文件大小的限制。

## GridFS

> GridFS 用于存储和恢复那些超过16M（BSON文件限制）的文件(如：图片、音频、视频等)。GridFS 也是文件存储的一种方式，但是它是存储在MonoDB的集合中。GridFS 可以更好的存储大于16M的文件。

> GridFS 会将大文件对象分割成多个小的chunk(文件片段),一般为256k/个,每个chunk将作为MongoDB的一个文档(document)被存储在chunks集合中。GridFS 用两个集合存储一个文件：fs.files与fs.chunks。每个文件的实际内容被存在chunks(二进制数据)中,和文件有关的meta数据(filename,content_type,还有用户自定义属性)会被存在files集合

> 存在问题：比如它读取文件的性能并不是很好，因为我们需要查询两次集合（先是fs.files，再是fs.chunks）才能将文件拼凑出来。如果我们的文件需要经常修改，那么GridFS也不合适，因为GridFS没法修改单独的某个分块，要修改文件的话，需要先将该文件删除，然后重新上传。

> 在Navicat中直接可以进行操作

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304221729727.png" alt="image-20230422172957654" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304221729501.png" alt="image-20230422172921420" style="zoom:80%;" />

# 数据类型

| 数据类型           | 描述                                                         |
| :----------------- | :----------------------------------------------------------- |
| String             | 字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。 |
| Integer            | 整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。 |
| Boolean            | 布尔值。用于存储布尔值（真/假）。                            |
| Double             | 双精度浮点值。用于存储浮点值。                               |
| Min/Max keys       | 将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。 |
| Array              | 用于将数组或列表或多个值存储为一个键。                       |
| Timestamp          | 时间戳。记录文档修改或添加的具体时间。                       |
| Object             | 用于内嵌文档。                                               |
| Null               | 用于创建空值。                                               |
| Symbol             | 符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。 |
| Date               | 日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。 |
| Object ID          | 对象 ID。用于创建文档的 ID。                                 |
| Binary Data        | 二进制数据。用于存储二进制数据。                             |
| Code               | 代码类型。用于在文档中存储 JavaScript 代码。                 |
| Regular expression | 正则表达式类型。用于存储正则表达式。                         |

## 主键ID

> 在 MongoDB 中，`ObjectId` 是一种特殊的数据类型，用于唯一标识一个文档（document）。每个 `ObjectId` 都是一个 12 字节的十六进制字符串。MongoDB采用ObjectId，而不是其他比较常规的做法（比如自动增加的主键）的主要原因，**因为在多个 服务器上同步自动增加主键值既费力还费时。**

### 组成部分

> 4 字节的时间戳（timestamp），表示文档创建时间，精确到秒级别。
>
> 3 字节的机器标识码（machine identifier），表示生成 `ObjectId` 的机器的标识。
>
> 2 字节的进程 ID（process id），表示在同一秒内生成 `ObjectId` 的不同进程的标识。
>
> 3 字节的随机数（random number），保证在同一秒内生成 `ObjectId` 的不同进程生成不同的值，避免冲突

> 由于 `ObjectId` 包含了文档的创建时间，因此可以按时间范围查询文档，还可以按 `_id` 字段进行排序。此外，由于 `ObjectId` 在分布式系统中具有唯一性，因此可以用作分布式系统中文档的唯一标识符。

### 生成ID

> 插入数据时，无需写id，写其他字段就行，id会自动生成的

```sql
db.users.insert({
  name: "Alice",
  age: 30,
  _id: ObjectId()
});
```

### 获取时间戳⭐

> 由于 ObjectId 中存储了 4 个字节的时间戳，所以你不需要为你的文档保存时间戳字段，你可以通过 getTimestamp 函数来获取文档的创建时间

```sql
# 结果：ISODate("2023-04-22T09:00:21.000Z")
ObjectId("6443a225ce79a828af66a64e").getTimestamp()
```

### 转成字符串

> 在某些情况下，您可能需要将ObjectId转换为字符串格式。你可以使用下面的代码

```sql
new ObjectId().str
```

## 基础数据类型

String：字符串类型，最常用的数据类型之一，用于存储文本信息。例如：

```json
"name": "Alice"
```

Number：数值类型，可以分为整数和浮点数。例如：

```json
"age": 28
"score": 88.5
```

Boolean：布尔类型，只有两个值：`true`和`false`。例如：

```json
"isStudent": true
```

Object：文档类型，用于嵌套存储子文档。例如：

```json
"address": {
    "city": "Beijing",
    "country": "China"
}
```

Array：数组类型，用于存储一组有序的值。例如：

```json
"scores": [80, 85, 90]
```

Null：空类型，表示不存在的值。例如：

```json
"middleName": null
```

## 高级数据类型

Date：日期类型，用于存储日期和时间。例如：

```json
"birthday": ISODate("1995-06-20T00:00:00Z")
new Date() 
ISODate()
```

ObjectId：文档ID类型，是MongoDB中每个文档的唯一标识符。例如：

```json
"_id": ObjectId("615f6807e8a1f814143e7fa6")
```

Binary：二进制数据类型，用于存储二进制数据。例如：

```json
"avatar": BinData(0, "iVBORw0KGg...")
```

Regular Expression：正则表达式类型，用于存储正则表达式。例如：

```json
"pattern": /hello world/i
```

## 实战演练

> 这个文档包括了字符串、整数、浮点数、布尔值、null、ObjectID、日期、数组、对象、二进制数据、正则表达式、DBPointer、JavaScript代码、Symbol、Int32、Timestamp和Decimal128等多种数据类型。

```sql
db.myCollection.insertOne({
   "stringField": "Hello World",
   "intField": 42,
   "doubleField": 3.14,
   "booleanField": true,
   "nullField": null,
   "objectIdField": ObjectId(),
   "dateField": new Date(),
   "arrayField": [1, 2, 3],
   "objectField": {"foo": "bar"},
   "binaryDataField": new BinData(0, "AQ=="),
   "regexField": /test/,
   "dbPointerField": new DBPointer("myOtherCollection", ObjectId()),
   "javascriptField": function() { return "hello world"; },
   "symbolField": Symbol("mySymbol"),
   "int32Field": NumberInt(42),
   "timestampField": Timestamp(1, 0),
   "decimalField": NumberDecimal("2.5")
})
```

# 增删改查

## 插入文档

### 单个插入

使用insert() 或 save() 方法向集合中插入文档，语法如下：

```sql
db.users.insertOne({
    "name": "Alice",
    "age": 25,
    "email": "alice@example.com",
    "isStudent": true,
    "address": {
        "city": "New York",
        "state": "NY",
        "country": "USA"
    },
    "hobbies": ["reading", "traveling", "swimming"],
    "createdAt": new Date(),
    "updatedAt": new Date(),
})
```

### 批量插入

> **如果某条数据插入失败，将会终止插入，但已经插入成功的数据不会回滚掉**

```mariadb
db.users.insertMany([
  { name: "Alice", age: 28 },
  { name: "Bob", age: 35 },
  { name: "Charlie", age: 42 }
])
```

### 插入失败处理

因为批量插入由于数据较多容易出现失败，因此，可以使用try catch进行异常捕捉处理，测试的时候可以不处理。如（了解）：

```mariadb
try { 
	db.comment.insertMany([ 
		{"_id":"4",
		"articleid":"100001",
		"content":"专家说不能空腹吃饭，影响健康。",
		"userid":"1003","nickname":"凯 撒",
		"createdatetime":new Date("2019-08-06T08:18:35.288Z"),
		"likenum":NumberInt(2000),
		"state":"1"
		}, 
		{"_id":"5",
		"articleid":"100001",
		"content":"研究表明，刚烧开的水千万不能喝，因为烫嘴。",
		"userid":"1003",
		"nickname":"凯撒",
		"createdatetime":new Date("2019-08- 06T11:01:02.521Z"),
		"likenum":NumberInt(3000),
		"state":"1"
		} 
	]); 
} catch (e) { 
	print (e); 
}
```

### 注意事项

> **集合如果不存在，则会隐式创建**
>
> mongo数字，**默认情况下是double类型**，**如果要存整型，必须使用函数NumberInt(整型数字)**，否则取就有问题
>
> **插入当前日期使用 new Date()** 
>
> **插入的数据没有指定 _id ，会自动生成主键值**
>
> **如果某字段没值，可以赋值为null，或不写该字段**

> 文档中的**键值对是有序的**。
>
> **文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型**（甚至可以是整个嵌入的文档)。 
>
> MongoDB**区分类型和大小写**。
>
> MongoDB的**文档不能有重复的键**。
>
> **文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符**

## 删除文档 

MongoDB通过collection对象的`remove、delete`方法来删除集合中的文档

```apl
# 单个删除
db.users.deleteOne({"name": "John Doe"})
```

```mariadb
# 批量删除
db.users.deleteMany({"age": {"$gte": 30}})
db.users.remove({"name": "Alice"})
# 以下语句可以将数据全部删除，请慎用 
db.comment.remove({})
```

## 更新文档⭐

更新时，如果没有该字段，就会增加该字段，更新文档的语法： 第一个字段是查询，第二个字段是修改

```mariadb
db.collection.update(query, update, options)
# 或
# query：修改的查询条件，类似于SQL中的WHERE部分
# update：更新属性的操作符，类似与SQL中的SET部分
# multi：设置为true时会更新所有符合条件的文档，默认为false只更新找到的第一条
db.collection.update(
	<query>,
	<update>,
	{
		upsert: <boolean>,
		multi: <boolean>,
		writeConcern: <document>,
		collation: <document>,
		arrayFilters: [ <filterdocument1>, ... ],
		hint: <document|string> // Available starting in MongoDB 4.2
	}
)
```

### 1、覆盖的修改 

> 覆盖修改，不好用，他会删除多余的字段，一般都使用局部修改

如果我们想修改_id为1的记录，点赞量为1001，输入以下语句： 

```mariadb
db.comment.update(
	{articleid:'10001'},
	{likenum:NumberInt(1001)}
)
```

执行后，我们会发现，这条文档除了likenum字段其它字段都不见了 

### 2、局部修改(重要) 

为了解决这个问题，我们需要使用修改器$set来实现，

命令如下：

```mariadb
db.comment.update(
	{_id:"1"},
	{$set:{password:"javac++",sort:22}}
)
```

将title为`MongoDB 教程`的所有文档的title修改为`MongoDB`；

```apl
db.article.update({'title':'MongoDB 教程'},{$set:{'title':'MongoDB'}},{multi:true})
```

### 3、批量的修改

 更新所有用户为 1003 的用户的昵称为 凯撒大帝 。 

```mariadb
# 默认只修改第一条数据
db.comment.update({userid:"1003"},{$set:{nickname:"凯撒2"}})
# 修改所有符合条件的数据
db.comment.update({userid:"1003"},{$set:{nickname:"凯撒大帝"}},{multi:true})
```

提示：如果不加后面的参数，则只更新符合条件的第一条记录

### 4、列值增长的修改 

如果我们想**实现对某列值在原有值的基础上进行增加或减少**，可以使用 $inc 运算符来实现。 

需求：**对3号数据的点赞数，每次递增1** ，注意：这时进行更新不会覆盖

```mariadb
db.comment.update({_id:"3"},{$inc:{likenum:NumberInt(1)}})
db.users.update({name:"Alice"},{$inc:{age:NumberInt(1)}})
```

## 查询文档

### 查询语法

查询数据的语法格式如下

```mariadb
# query：查询条件，类似于SQL中的WHERE部分
# projection：可选，使用投影操作符指定返回的键
db.collection.find(<query>, [projection])
```

### 查询所有

```sql
db.users.find()
db.users.find({})
```

> 这里你会发现每条文档会有一个叫**_id的字段**，**这个相当于我们原来关系数据库中表的主键**，**当你在插入文档记录时没有指定该字段， MongoDB会自动创建，其类型是ObjectID类型**。如果我们在插入文档记录时指定该字段也可以，其类型可以是ObjectID类型，也可以是MongoDB支持的任意类型。 

### 条件查询

> 如果我想**按一定条件来查询**，只要在**find()中添加参数即可，参数也是json格式**
>

```mariadb
# 注意：数据类型必须对应，不然会查不到数据
db.users.find({age:26})

db.users.find({"name": "Alice"})

db.users.find({age : 26,"name": "Alice"})
```

### 查询单条

> 如果你**只需要返回符合条件的第一条数据，我们可以使用findOne命令来实现**，语法和find一样。

```sql
# 如：查询用户编号是1003的记录，但只最多返回符合条件的第一条记录： 
db.users.findOne({"name": "Alice"})
```

### 查询指定字段

> 即投影查询，1表示显示该字段、0表示不显示该字段，如果要查询结果**返回部分字段**，则需要使用投影查询（不显示所有字段，只显示指定的字段）

```mariadb
# 设置显示的字段
db.users.find({"name": "Alice"},{name:1,address:1})
db.users.find({},{name:1,address:1})

# 设置不显示的字段
db.users.find({"name": "Alice"},{name:0,address:0})

# 不可以既设置1又设置0,这样会出错，只有_id才能进行同时设置0
db.users.find({},{name:1,address:1,_id:0})
```

### 统计查询

```sql
# 统计comment集合的所有的记录数
db.users.count()
# 查询符合条件的个数
db.users.find({"name": "Alice"}).count()
db.users.count({"name": "Alice"})
```

### 分页查询

```mariadb
# 如果你想返回指定条数的记录，可以在find方法后调用limit来返回结果(TopN)，默认值20
db.comment.find().limit(3)
# skip方法同样接受一个数字参数作为跳过的记录条数。（前N个不要）,默认值是0
db.comment.find().skip(3)
# 可以使用limit方法来读取指定数量的数据，使用skip方法来跳过指定数量的数据
db.集合名.find().limit(数字).skip(数字)
```

> 分页查询：需求：每页2个，第二页开始：跳过前两条数据，接着值显示3和4条数据
>

```mariadb
# 第一页
db.comment.find().skip(0).limit(2)
# 第二页
db.comment.find().skip(2).limit(2)
# 第三页
db.comment.find().skip(4).limit(2)
```

> 需要注意的是，使用 `skip` 方法可能会导致查询性能下降，特别是在跳过大量数据时。因此，最好尽可能避免使用 `skip` 方法，或者使用其他方法来实现分页查询，例如使用 `ObjectId` 进行分页。
>

### 排序查询

> **sort() 方法对数据进行排序**，**sort() 方法可以通过参数指定排序的字段**，**使用 1 和 -1 来指定排序的方式**

其中 1 为升序排列，而 -1 是用 于降序排列。

```mariadb
db.集合名称.find().sort(排序方式)
```

```mariadb
# 按照score字段进行降序排序
db.students.find().sort({score: -1})
# 按照score字段降序排序，如果两个文档score字段相同，则按照name字段进行升序排序
db.students.find().sort({score: -1, name: 1})
# 使用limit()方法限制返回的文档数量
db.students.find().sort({score: -1}).limit(10)
```

> 提示： skip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()，和命令编写顺序无关。
>

## 高级查询

### 正则查询

MongoDB使用`$regex`操作符来设置匹配字符串的正则表达式，可以用来模糊查询，类似于SQL中的like操作；

```sql
# 例如查询title中包含`教程`的文档；
db.article.find({title:{$regex:"教程"}})

# 不区分大小写的模糊查询，使用`$options`操作符；
db.article.find({title:{$regex:"elasticsearch",$options:"$i"}})

# MongoDB的模糊查询是通过正则表达式的方式实现的
db.collection.find({field:/正则表达式/})
db.集合.find({字段:/正则表达式/})
# 我要查询评论内容包含“开水”的所有文档
db.comment.find({content:/开水/})
# 如果要查询评论的内容中以“专家”开头的
db.comment.find({content:/^专家/})
```

### 比较查询

MongoDB中的条件操作符，通过与SQL语句的对比来了解下；

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205261050248.png" alt="image-20220526105053146" style="zoom: 67%;" />

<, <=, >, >= 这个操作符也是很常用的

```mariadb
db.集合名称.find({ "field" : { $gt: value }}) // 大于: field > value
db.集合名称.find({ "field" : { $lt: value }}) // 小于: field < value
db.集合名称.find({ "field" : { $gte: value }}) // 大于等于: field >= value
db.集合名称.find({ "field" : { $lte: value }}) // 小于等于: field <= value
db.集合名称.find({ "field" : { $ne: value }}) // 不等于: field != value
```

示例：查询评论点赞数量大于700的记录

```apl
db.comment.find({likenum:{$gt:NumberInt(700)}})
```

查询title为`MongoDB 教程`的所有文档；

```apl
db.article.find({'title':'MongoDB 教程'})
```

查询likes大于50的所有文档；

```apl
db.article.find({'likes':{$gt:50}})
```

AND条件可以通过在`find()`方法传入多个键，以逗号隔开来实现，例如查询title为`MongoDB 教程`并且by为`Andy`的所有文档；

```apl
db.article.find({'title':'MongoDB 教程','by':'Andy'})
```

OR条件可以通过使用`$or`操作符实现，例如查询title为`Redis 教程`或`MongoDB 教程`的所有文档；

```apl
db.article.find({$or:[{"title":"Redis 教程"},{"title": "MongoDB 教程"}]})
```

AND 和 OR条件的联合使用，例如查询likes大于50，并且title为`Redis 教程`或者`"MongoDB 教程`的所有文档。

```apl
db.article.find({"likes": {$gt:50}, $or: [{"title": "Redis 教程"},{"title": "MongoDB 教程"}]})
```



### 包含查询

包含使用$in操作符，不包含使用$nin操作符

```mariadb
# 查询评论的集合中userid字段包含1003或1004的文档
db.comment.find({userid:{$in:["1003","1004"]}})
# 查询评论集合中userid字段不包含1003和1004的文档
db.comment.find({userid:{$nin:["1003","1004"]}})
```

### 条件查询

我们如果需要查询同时满足两个以上条件，需要使用$and操作符将条件进行关联。（相 当于SQL的and） 格式为：

```
$and:[ { },{ },{ } ]
```

示例：查询评论集合中likenum大于等于700 并且小于2000的文档：

```mariadb
db.comment.find(
{$and:[
	{likenum:{$gte:NumberInt(700)}},
	{likenum:{$lt:NumberInt(2000)}}
	]
})
```

如果两个以上条件之间是或者的关系，我们使用 操作符进行关联，与前面 and的使用方式相同 格式为：

```mariadb
$or:[ { },{ },{ } ]
```

示例：查询评论集合中userid为1003，或者点赞数小于1000的文档记录

```mariadb
db.comment.find(
{$or:[ 
    {userid:"1003"},
    {likenum:{$lt:1000}}
]})
```



# 索引

## 索引概述

> 索引支持在MongoDB中**高效地执行查询**。如果没有索引，MongoDB必须执行全集合扫描，即扫描集合中的每个文档，以选择与查询语句匹配的文档。这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。 
>

## 索引类型

> - 单键索引（single field index）：以单个字段为索引键的索引。
> - 复合索引（compound index）：以多个字段为索引键的索引，可以提高复合查询的性能。
> - 多键索引（multikey index）：用于数组字段的索引。
> - 全文索引（text index）：用于全文搜索。
> - 地理空间索引（geospatial index）：用于地理空间查询。

## 索引管理

### 查看索引

```mariadb
# 返回一个集合中的所有索引的数组
db.collection.getIndexes()
```

### 创建索引

```mariadb
db.collection.createIndex(keys, options)
```

```sql
# 按升序创建索引
db.comment.createIndex({userid:1})
# 对 userid 和 nickname 同时建立复合（Compound）索引
db.comment.createIndex({userid:1,nickname:-1})
```

### 删除索引

```sql
# 指定索引的移除
db.collection.dropIndex(indexName)
# 删除 comment 集合中 userid 字段上的升序索引
db.comment.dropIndex({userid:1})
```

```mariadb
# 所有索引的移除，提示： _id 的字段的索引是无法删除的，只能删除非 _id 字段的索引
db.collection.dropIndexes()
# 删除集合中所有索引
db.comment.dropIndexes()
```

### 分析查询性能

> 通常使用执行计划（解释计划、Explain Plan）来查看查询的情况，如查询耗费的时间、 否基于索引查询等。 那么，通常，我们想知道，建立的索引是否有效，效果如何，都需要通过执行计划查看。
>

```mariadb
db.collection.find(query,options).explain(options)
```

查看根据userid查询数据的情况，生成结果中：**关键点看： "stage" : "COLLSCAN", 表示全集合扫描**

```mariadb
db.comment.find({userid:"1003"}).explain()
```

## 地理索引

MongoDB 支持地理位置数据的存储和查询，可以使用地理空间索引提高地理查询的性能。下面是使用示例：

假设有一个 `locations` 集合，其中包含了每个城市的经度和纬度：

```sql
db.locations.insertMany([
  {name: 'Shanghai', location: {type: 'Point', coordinates: [121.47, 31.23]}},
  {name: 'Beijing', location: {type: 'Point', coordinates: [116.40, 39.90]}},
  {name: 'Guangzhou', location: {type: 'Point', coordinates: [113.27, 23.13]}},
  {name: 'Shenzhen', location: {type: 'Point', coordinates: [114.05, 22.55]}}
])
```

接下来，可以在 `location` 字段上创建地理空间索引：

```sql
db.locations.createIndex({location: '2dsphere'})
```

现在可以查询离某个坐标最近的城市。例如，查询距离经度为 121.47，纬度为 31.23 的坐标最近的城市：

```sql
db.locations.findOne({
  location: {
    $near: {
      $geometry: {
        type: 'Point',
        coordinates: [121.47, 31.23]
      }
    }
  }
})
```

现在可以查询某个半径内的城市。例如，查询距离经度为 121.47，纬度为 31.23 的坐标半径为 200 公里内的城市：

```sql
db.locations.find({
  location: {
    $near: {
      $geometry: {
        type: 'Point',
        coordinates: [121.47, 31.23]
      },
      $maxDistance: 200000 // 200 公里
    }
  }
})
```

## 全文检索

MongoDB 4.2 及以上版本中引入了全文索引功能，可以通过 `$search` 管道操作符在全文索引上执行搜索。

假设有一个 `articles` 集合，其中包含了三篇文章

```sql
db.articles.insertMany([
  {
    title: "Introduction to MongoDB",
    content: "MongoDB is a popular NoSQL database."
  },
  {
    title: "Getting started with MongoDB",
    content: "To use MongoDB, you need to first install it and create a database."
  },
  {
    title: "Advanced MongoDB techniques",
    content: "MongoDB supports advanced features such as aggregation pipelines and transactions."
  }
]);
```

接下来，对 `articles` 集合进行全文搜索，查找包含关键字 "MongoDB" 的文章

### 创建全文索引

首先，需要创建一个全文索引。假设有一个 `articles` 集合，需要在 `title` 和 `content` 字段上创建全文索引

```sql
db.articles.createIndex({ title: "text", content: "text" });
```

### 查询测试

然后，就可以使用 `$text` 操作符对集合进行全文搜索了。以下是一个示例，查询包含关键字 "MongoDB" 的所有文章

```sql
db.articles.find({
  $text: {
    $search: "MongoDB"
  }
});
```

### 删除全文索引

```sql
# 删除已存在的全文索引，可以使用 find 命令查找索引名：
db.articles.getIndexes()
# 通过以上命令获取索引名，本例的索引名为post_text_text，执行以下命令来删除索引：
db.articles.dropIndex("post_text_text")
```



## 实战案例

```sql
db.users.insertMany([
  {name: 'Alice', age: 20},
  {name: 'Bob', age: 25},
  {name: 'Charlie', age: 30},
  {name: 'Dave', age: 35},
  {name: 'Eve', age: 40},
])
```

这将向 `users` 集合插入 5 条文档，每个文档包含一个 `name` 字段和一个 `age` 字段。可以在 `users` 集合上创建一个 `age` 字段的升序索引，以提高按年龄查询的性能：

```sql
db.users.createIndex({age: 1})
```

现在可以查询所有年龄大于 30 的用户，看看是否使用了索引：

```sql
db.users.find({age: {$gt: 30}}).explain()
```

其中，`winningPlan` 字段表示 MongoDB 选择的最优执行计划，`usedIndex` 字段表示是否使用了索引。上述输出中的 `usedIndex` 值为 `true`，表示 MongoDB 使用了 `age` 字段的索引。



# 聚合

## 聚合语法

**基本语法**

MongoDB聚合操作可以将多个文档处理成一个结果集，类似于SQL中的GROUP BY操作。聚合操作可以对文档进行多种计算操作，如计数、求和、平均值等等。MongoDB聚合语法的基本结构如下

```sql
db.collection.aggregate( [ { <stage> }, ... ] )
```

> 其中，db.collection.aggregate是聚合操作的基本语法格式，可以对指定的集合进行聚合操作。聚合操作由多个聚合阶段（stage）组成，每个聚合阶段都会对输入文档进行一定的转换处理，并将处理结果传递给下一个聚合阶段。聚合阶段的顺序可以任意调整，以满足具体的计算需求。

下面是一些常用的聚合阶段及其用法：

$match：筛选符合条件的文档，相当于SQL中的WHERE子句。语法如下：

```sql
{ $match: { <condition> } }
```

$project：指定要返回的字段，并可以对字段进行一定的计算操作，相当于SQL中的SELECT子句。语法如下：

```sql
{ $project: { <field1>: <expression1>, <field2>: <expression2>, ... } }
```

$group：按照指定的字段对文档进行分组，并可以对分组后的文档进行一定的聚合计算，如计数、求和、平均值等等。相当于SQL中的GROUP BY子句。语法如下：

```sql
{ $group: { _id: <expression>, <field1>: { <accumulator1> : <expression1> }, ... } }
```

$sort：按照指定的字段对文档进行排序，相当于SQL中的ORDER BY子句。语法如下：

```sql
{ $sort: { <field1>: <sort order>, <field2>: <sort order>, ... } }
```

$limit：指定返回文档的数量限制，相当于SQL中的LIMIT子句。语法如下：

```sql
{ $limit: <num> }
```

$skip：指定跳过文档的数量，相当于SQL中的OFFSET子句。语法如下：

```sql
{ $skip: <num> }
```

$unwind：将数组类型的字段拆分为多个文档，并对拆分后的文档进行聚合计算，相当于SQL中的UNNEST操作。

```sql
{ $unwind: <array field> }
```

这些聚合阶段可以组合使用，形成更为复杂的聚合操作。例如，下面的聚合操作会对指定集合中的文档进行筛选、分组、计数，并按照计数结果进行排序：

```sql
phpCopy codedb.collection.aggregate([
    { $match: { status: "A" } },
    { $group: { _id: "$type",
```

**常用阶段操作符**

| 操作符   | 简述                                                         |
| :------- | :----------------------------------------------------------- |
| $match   | 匹配操作符，用于对文档集合进行筛选                           |
| $project | 投射操作符，用于重构每一个文档的字段，可以提取字段，重命名字段，甚至可以对原有字段进行操作后新增字段 |
| $sort    | 排序操作符，用于根据一个或多个字段对文档进行排序             |
| $limit   | 限制操作符，用于限制返回文档的数量                           |
| $skip    | 跳过操作符，用于跳过指定数量的文档                           |
| $count   | 统计操作符，用于统计文档的数量                               |
| $group   | 分组操作符，用于对文档集合进行分组                           |
| $unwind  | 拆分操作符，用于将数组中的每一个值拆分为单独的文档           |
| $lookup  | 连接操作符，用于连接同一个数据库中另一个集合，并获取指定的文档，类似于 populate |

## 实战示例

假设有一个 `orders` 集合，其中包含了客户、订单日期和订单总额：

```sqlite
db.orders.insertMany([
  {customer: 'Alice', date: new Date('2022-01-01'), total: 50},
  {customer: 'Bob', date: new Date('2022-01-02'), total: 100},
  {customer: 'Charlie', date: new Date('2022-01-02'), total: 150},
  {customer: 'Alice', date: new Date('2022-01-03'), total: 200},
  {customer: 'Charlie', date: new Date('2022-01-03'), total: 250}
])
```

> 可以使用聚合框架对该集合进行分组和汇总操作,将根据 `customer` 字段进行分组，并计算每个客户的订单总额和订单数。然后，按照订单总额进行排序，以便获取订单总额最高的客户

```sql
db.orders.aggregate([
  {
    $group: {
      _id: '$customer',
      total: { $sum: '$total' },
      count: { $sum: 1 }
    }
  },
  {
    $sort: { total: -1 }
  }
])
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304222044045.png" alt="image-20230422204440949" style="zoom:80%;" />

> 聚合框架支持多种操作管道，包括投影、过滤、分组、排序、限制等等。可以根据实际需求，组合使用不同的操作管道，以实现各种复杂的查询需求。

计算集合中某个字段的平均值、最大值和最小值

```sql
db.orders.aggregate([
  {
    $group: {
      _id: null,
      avgAmount: { $avg: "$amount" },
      maxAmount: { $max: "$amount" },
      minAmount: { $min: "$amount" }
    }
  }
])
```

根据字段进行分组，并计算每个分组中文档数量

```sql
db.orders.aggregate([
  {
    $group: {
      _id: "$status",
      count: { $sum: 1 }
    }
  }
])
```

查找最高销售额的顾客姓名和销售额

```sql
db.orders.aggregate([
  {
    $group: {
      _id: "$customerName",
      totalAmount: { $sum: "$amount" }
    }
  },
  {
    $sort: {
      totalAmount: -1
    }
  },
  {
    $limit: 1
  },
  {
    $project: {
      _id: 0,
      customerName: "$_id",
      totalAmount: 1
    }
  }
])
```

连接多个集合，查询顾客的订单和订单明细信息

```sql
db.customers.aggregate([
  {
    $lookup: {
      from: "orders",
      localField: "_id",
      foreignField: "customerId",
      as: "orders"
    }
  },
  {
    $unwind: "$orders"
  },
  {
    $lookup: {
      from: "orderDetails",
      localField: "orders._id",
      foreignField: "orderId",
      as: "orderDetails"
    }
  },
  {
    $project: {
      _id: 0,
      customerName: 1,
      orderId: "$orders._id",
      orderDate: "$orders.date",
      productId: "$orderDetails.productId",
      quantity: "$orderDetails.quantity"
    }
  }
])
```



# 日志

## 慢日志

可以在MongoDB的配置文件（通常是/etc/mongod.conf或/etc/mongodb.conf）中添加以下配置来启用慢查询日志：

### 配置文件

可以在MongoDB的配置文件（通常是/etc/mongod.conf或/etc/mongodb.conf）中添加以下配置来启用慢查询日志：

```yaml
systemLog:
  slowOpThresholdMs: 100
  destination: file
  path: /var/log/mongodb/mongod.log
```

### 命令行配置

在启动MongoDB实例时，可以使用--slowms参数来记录慢查询日志。例如，以下命令将记录运行时间超过50毫秒的所有慢查询：在这个例子中，MongoDB将记录运行时间超过50毫秒的所有慢查询，并将它们写入标准输出（控制台）。您可以根据需要调整slowms参数的值。

```sql
mongod --profile=1 --slowms=200
```







# 集群

## 集群

### 创建集群并启动

1、准备数据目录

```apl
mkdir -p /data/mongodb/node{1,2,3,4,5}
mkdir -p /data/mongodb/log/log{1,2,3,4,5}
```

或者删除文件夹,重新创建

```apl
rm -rf /data/mongodb/
```

2、启动这5个节点

```sh
mongod --replSet rep --dbpath /data/mongodb/node1 --port 101 --bind_ip 0.0.0.0 --logpath /data/mongodb/log/log1/config.log --fork
mongod --replSet rep --dbpath /data/mongodb/node2 --port 102 --bind_ip 0.0.0.0 --logpath /data/mongodb/log/log2/config.log --fork
mongod --replSet rep --dbpath /data/mongodb/node3 --port 103 --bind_ip 0.0.0.0 --logpath /data/mongodb/log/log3/config.log --fork
mongod --replSet rep --dbpath /data/mongodb/node4 --port 104 --bind_ip 0.0.0.0 --logpath /data/mongodb/log/log4/config.log --fork
mongod --replSet rep --dbpath /data/mongodb/node5 --port 105 --bind_ip 0.0.0.0 --logpath /data/mongodb/log/log5/config.log --fork
```

### 登录任意节点并初始化

登录节点

```apl
mongo --port=101
```

初始化

```apl
rs.initiate({"_id":"myrs","members":[ {"_id":1,"host":"localhost:101"}, {"_id":2,"host":"localhost:102"}, {"_id":3,"host":"localhost:103","arbiterOnly" : true}, {"_id":4,"host":"localhost:104"} ,{"_id":5,"host":"localhost:105"}]})
```

查看状态

```apl
rs.status();
```

### 配置命令执行出错解决方式

注意，执行出错，则要执行如下命令

官网地址：[setDefaultRWConcern — MongoDB Manual](https://docs.mongodb.com/manual/reference/command/setDefaultRWConcern/)

```mariadb
db.adminCommand({
  "setDefaultRWConcern" : 1,
  "defaultWriteConcern" : {
    "w" : 2
  }
})
```

### 允许外网连接

注意：端口号要是更改必须全部更改，如果集群如果有节点不更改则会出错

```mariadb
config = rs.conf()
config.members[0].host="101.43.33.227:101"
config.members[1].host="101.43.33.227:102"
config.members[2].host="101.43.33.227:103"
config.members[3].host="101.43.33.227:104"
config.members[4].host="101.43.33.227:105"
```

再执行

```mariadb
rs.reconfig(config,{"force":true})
```

### 进入主节点插入数据

```sql
use articledb
db.comment.insert(
 {"articleid":"100000",
   "content":"今天天气真好，阳光明媚",
   "userid":"1001",
   "nickname":"Rose",
   "createdatetime":new Date(),
   "likenum":NumberInt(10),
   "state":null
}) 
```

### 查询测试

**进入其他从节点都能查到数据**，**但先得设置从节点可读**

```sql
-- 登录从节点
mongo --port=102
-- 设置从节点可读
rs.secondaryOk()
-- 进行查看
use articledb
db.comment.find()
```

### 主节点掉线测试

首先查到主节点

```sql
rs.status()
```

查找并杀掉主节点

```sql
ps -ef |grep mongod
kill -2 port
```

再次查看，看哪个节点变成了主节点,可以发现，**已经成功选出主节点**

进行插入测试

```apl
use articledb
db.comment.insert( 
 {"articleid":"100001",
   "content":"今天天气不好，乌云密布",
   "userid":"1002",
   "nickname":"Jake",
   "createdatetime":new Date(),
   "likenum":NumberInt(10),
   "state":null
}) 
```

其他从节点进行查询测试

```sql
mongo --port=102
```

```sql
rs.secondaryOk()
use articledb
db.comment.find()
```

曾经的主节点重连，查询数据看是否成功同步

```apl
mongod --replSet rep --dbpath /data/mongodb/node1 --port 101 --bind_ip 0.0.0.0 --logpath /data/mongodb/log/log1/config.log --fork

-- 进入数据库
mongo --port=101
```

查询数据方式同上，**可以发现数据已经成功同步**

### 从节点掉线测试

首先查到主节点

```apl
rs.status()
```

查找并杀掉端口号105从节点

```apl
ps -ef |grep mongod
kill -2 port
```

主节点中插入数据

进行插入测试

```apl
use articledb
db.comment.insert( 
 {"articleid":"100003",
   "content":"今天天气不好号，乌云密布",
   "userid":"1003",
   "nickname":"Jake",
   "createdatetime":new Date(),
   "likenum":NumberInt(10),
   "state":null
}) 
```

再次启动该从节点

```apl
mongod --replSet rep --dbpath /data/mongodb/node5 --port 105 --bind_ip 0.0.0.0 --logpath /data/mongodb/log/log5/config.log --fork
```

```sql
mongo --port=105
```

```sql
rs.secondaryOk()
use articledb
db.comment.find()
```

查询数据看是否成功同步

### 节点优先级调整测试

在某些情况下，可能希望将一个数据中心中的成员选为主数据中心的成员，然后再选择其他数据中心的成员。您可以修改成员的[`优先级`](https://docs.mongodb.com/manual/reference/replica-configuration/#mongodb-rsconf-rsconf.members-n-.priority)，以便一个数据中心中的成员的[`优先级`](https://docs.mongodb.com/manual/reference/replica-configuration/#mongodb-rsconf-rsconf.members-n-.priority)高于其他数据中心中的成员。

副本集的某些成员（如具有网络限制或资源有限的成员）不应能够在故障转移中成为主[成员](https://docs.mongodb.com/manual/reference/glossary/#std-term-failover)。将不应成为主成员的成员配置为[优先级为 0](https://docs.mongodb.com/manual/core/replica-set-priority-0-member/#std-label-replica-set-secondary-only-members)

**节点优先级设置为0，那么该节点永远不能成为主节点**

进入主节点，执行如下命令

注意：通过members[num]来设置对应成员

```apl
cfg = rs.conf()
cfg.members[1].priority = 3
rs.reconfig(cfg)
```

稍等一会儿，102节点就会变成主节点了

### 新增节点测试同步

注意对应数字要改变，不要重复

第一步：在primary节点执行

```apl
rs.add("101.43.33.227:106")
```

第二步

```apl
mkdir -p /data/mongodb/node6
mkdir -p /data/mongodb/log/log6
```

第三步

```apl
mongod --replSet rep --dbpath /data/mongodb/node6 --port 106 --bind_ip 0.0.0.0 --logpath /data/mongodb/log/log6/config.log --fork
```

出现successful表示进入成功

进入该数据库

```apl
mongo --port=106
```

设置该节点可读

```apl
rs.secondaryOk()
```

接下来进行查看数据是否同步

```apl
show dbs
use articledb
db.comment.find()
```

测试，数据已经同步成功

### 移除节点测试

进入主节点

```apl
rs.remove("101.43.33.227:106")
```

查看集群状态

```apl
rs.status()
```

106节点成功移除

### 无投票权成员测试

一个集群最多有7个投票成员，因此需要设置一些节点为无投票成员

进入主节点后，选择配置一个节点为无投票成员

```apl
cfg = rs.conf()
cfg.members[1].votes = 0
cfg.members[1].priority = 0
rs.reconfig(cfg)
```

进行查看是否成功

```apl
rs.conf()
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220220204559631.png" alt="image-20220220204559631" style="zoom:67%;" />

### 主节点降级测试

指示副本集的主节点成为辅助。主节点退出后，其他节点开始进行选举。

该方法**不会立即降低主数据库**。如果没有[`可选举的`](https://docs.mongodb.com/manual/reference/replica-configuration/#mongodb-rsconf-rsconf.members-n-.priority)辅助数据库与主数据库保持同步，则主数据库将等待最多（默认为 10 秒）以赶上辅助数据库。一旦有可选择的辅助数据库可用，该方法就会逐步降低主数据库。

一旦降级，原始主数据库将成为辅助数据库，并且在 指定的剩余时间内没有资格再次成为主数据库

```apl
rs.stepDown(30)
```

在Mongodb复制集主从切换的时候，使用`rs.stepDown(seconds)`，将当前主库实例“降级”，则`seconds`时间内，这个实例不会把自己选为primary角色。

通过执行"rs.stepDown（30）"命令将当前主库“降级”，30秒内这个实例不会把自己选为primary角色，相当于在30秒内此实例被“降级”了

**执行完这个命令，当有从节点追赶上主节点后，主节点立刻降级为从节点，同时会有其他从节点选举为主节点**

**30s之后，如果曾经主节点优先级比现任主节点优先级高的话，则会再次触发选举，曾经主节点可能再次成为主节点**

主要目的是方便主节点维护

### 关闭mongodb节点

```
mongod --shutdown --dbpath=/data/mongodb/node1
```

### 查看系统进程

```
ps -ef |grep mongod
kill -2 port
```

### 启动方式2

注意：因为配置了公网ip，所以确定有网才不会报错

网络图标消失解决： nmcli networking on

```
mkdir -p /data/mongodb/conf
```

```
vi /data/mongodb/conf/node1.conf
```

```
vi /data/mongodb/conf/node2.conf
```

```
vi /data/mongodb/conf/node3.conf
```

```
vi /data/mongodb/conf/node4.conf
```

```yml
systemLog:
  #MongoDB发送所有日志输出的目标指定为文件
  destination: file
  #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径
  path: "/data/mongodb/log/log1/config.log"
  #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。
  logAppend: true
storage:
  #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。
  dbPath: "/data/mongodb/node1"
  journal:
    #启用或禁用持久性日志以确保数据文件保持有效和可恢复。
    enabled: true
processManagement:
  #启用在后台运行mongos或mongod进程的守护进程模式。
  fork: true
  #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID
  pidFilePath: "/data/mongodb/log/log1/mongod.pid"
net:
  #服务实例绑定的IP，默认是localhost
  bindIp: 0.0.0.0
  #bindIp
  #绑定的端口，默认是27017
  port: 101
replication:
  #副本集的名称
  replSetName: myrs
```

进行启动

```apl
mongod -f /data/mongodb/conf/node1.conf
mongod -f /data/mongodb/conf/node2.conf
mongod -f /data/mongodb/conf/node3.conf
mongod -f /data/mongodb/conf/node4.conf
mongod -f /data/mongodb/conf/node5.conf
mongod -f D:\mongoDB\node5.conf
```

```apl
mongo --port=101
mongo --port=102
mongo --port=103
mongo --port=104
mongo --port=105
```

登录任意节点进行操作:登录任意节点进行初始化操作

**注意_id必须与–replSet  参数保持一致**

```mariadb
rs.initiate({"_id":"myrs","members":[ {"_id":1,"host":"localhost:1001"}, {"_id":2,"host":"localhost:1002"},{"_id":3,"host":"localhost:1003"}]})   
```



## 日志

[(38条消息) MongoDB如何查oplog.rs集合里的操作日志_Bertram的博客-CSDN博客_mongodb 查看oplog](https://blog.csdn.net/chj_1224365967/article/details/115318523)

### 查看操作日志

```sql
-- ns：表示查询的数据库名.方法名
use local
db.oplog.rs.find({ns:'articledb.comment'})
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220312152447797.png" alt="image-20220312152447797" style="zoom:67%;" />

### 查看 Oplog 的状态

```sql
rs.printReplicationInfo()
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220314191739404.png" alt="image-20220314191739404" style="zoom:67%;" />

### 查看当前日志存储设置大小

```sql
use local
db.oplog.rs.stats().maxSize
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220314191846903.png" alt="image-20220314191846903" style="zoom:67%;" />

查看 Oplog 最大大小和现在占用的大小，以及记录时长和时间

```sql
use local
db.getReplicationInfo()
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220314191954464.png" alt="image-20220314191954464" style="zoom:67%;" />

更改副本集成员的 Oplog 大小

```sql
use local
db.adminCommand({replSetResizeOplog:1,size:3000})
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220314192031292.png" alt="image-20220314192031292" style="zoom:67%;" />

## 客户端连接

更改host(重要)

[(39条消息) Mongodb副本集端口连接超时问题（修改副本集host）_Zakza的博客-CSDN博客](https://blog.csdn.net/qq_18453581/article/details/120507908)

首先，进入primary节点

```
rs.conf()
```

注意看host值为localhost，这样使用SpringData是连接不上的，要进行修改

在primary节点，**依次执行**如下命名

注意：端口号要是更改必须全部更改，如果集群如果有节点不更改则会出错

```mariadb
config = rs.conf()
config.members[0].host="192.168.220.128:1001"
config.members[1].host="192.168.220.128:1002"
config.members[2].host="192.168.220.128:1003"
config.members[3].host="192.168.220.128:1004"
config.members[4].host="192.168.220.128:1005"

config.members[0].host="101.43.33.227:101"
config.members[1].host="101.43.33.227:102"
config.members[2].host="101.43.33.227:103"
config.members[3].host="101.43.33.227:104"
config.members[4].host="101.43.33.227:105"
```

再执行

```mariadb
rs.reconfig(config,{"force":true})
```

注意，执行出错，则要执行如下命令

官网地址：[setDefaultRWConcern — MongoDB Manual](https://docs.mongodb.com/manual/reference/command/setDefaultRWConcern/)

```mariadb
db.adminCommand({
  "setDefaultRWConcern" : 1,
  "defaultWriteConcern" : {
    "w" : 2
  }
})
```

在外面，进行连接测试

进行连接

只需要修改application.yaml即可

```yaml
spring:
  #数据源配置
  data:
    mongodb:
      uri: mongodb://192.168.220.128:1001,192.168.220.128:1002,192.168.220.128:1003/articledb?connect=replicaSet?slaveOk=true&replicaSet=nod
```

进行测试时查询所有即可发现测试成功(前面配置了SpringData的环境)

```java
//查询所有数据
@Test
public void testFindAll(){
    List<Comment> list = commentService.findCommentList();
    System.out.println(list);
}
```



# 分片集群

## 概念

分片（sharding）是一种**跨多台机器分布数据**的方法， MongoDB使用分片来支持具有非常大的数据集和高吞吐量操作的部署。

**换句话说：分片(sharding)是指将数据拆分，将其分散存在不同的机器上的过程**。有时也用分区 (partitioning)来表示这个概念。

**将数据分散到不同的机器上，不需要功能强大的大型计算机就可以储存更多的数据，处理更多的负载**。 具有大型数据集或高吞吐量应用程序的数据库系统可以会挑战单个服务器的容量。例如，高查询率会耗 尽服务器的CPU容量。工作集大小大于系统的RAM会强调磁盘驱动器的I/O容量。 

有两种解决系统增长的方法：**垂直扩展和水平扩展**。 

**垂直扩展**意味着**增加单个服务器的容量**，例如使用更强大的CPU，添加更多RAM或增加存储空间量。可 用技术的局限性可能会限制单个机器对于给定工作负载而言足够强大。此外，基于云的提供商基于可用 的硬件配置具有硬性上限。结果，垂直缩放有实际的最大值。

**水平扩展**意味着**划分系统数据集并加载多个服务器，添加其他服务器以根据需要增加容量**。虽然单个机器的总体速度或容量可能不高，但每台机器处理整个工作负载的子集，可能提供比单个高速大容量服务 器更高的效率。扩展部署容量只需要根据需要添加额外的服务器，这可能比单个机器的高端硬件的总体 成本更低。权衡是基础架构和部署维护的复杂性增加。**MongoDB支持通过分片进行水平扩展**。

## 分片集群组件

MongoDB分片群集包含以下组件：

- **分片**（存储）：每个分片包含分片数据的子集。 每个分片都可以部署为副本集。 
- **mongos**（路由）：mongos充当查询路由器，在客户端应用程序和分片集群之间提供接口。 
- **config servers**（“调度”的配置）：配置服务器存储群集的元数据和配置设置。 从MongoDB 3.4开 始，必须将配置服务器部署为副本集（CSRS）。

下图描述了分片集群中组件的交互

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220115174432575.png" alt="image-20220115174432575" style="zoom:80%;" />

MongoDB在集合级别对数据进行分片，将集合数据分布在集群中的分片上。



## 分片集群架构目标

两个分片节点副本集（3+3）+一个配置节点副本集（3）+两个路由节点（2），共11个服务节点

#### 创建第一个分片节点

##### 创建文件夹

```mariadb
mkdir -p /mongodb/sharded_cluster/myshardrs01_27018/log \ &
mkdir -p /mongodb/sharded_cluster/myshardrs01_27018/data/db \ &
mkdir -p /mongodb/sharded_cluster/myshardrs01_27118/log \ &
mkdir -p /mongodb/sharded_cluster/myshardrs01_27118/data/db \ &
mkdir -p /mongodb/sharded_cluster/myshardrs01_27218/log \ &
mkdir -p /mongodb/sharded_cluster/myshardrs01_27218/data/db
```

##### 创建配置文件

```mariadb
vim /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf
vim /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf
vim /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf
```

##### 配置文件详情

注意：日志不是文件夹，日志是文件config.log，写成文件夹启动会出错

```yml
systemLog:
  #MongoDB发送所有日志输出的目标指定为文件
  destination: file
  #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径
  path: "/mongodb/sharded_cluster/myshardrs01_27018/config.log"
  #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。
  logAppend: true
storage:
  #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。
  dbPath: "/mongodb/sharded_cluster/myshardrs01_27018/data/db"
  journal:
    #启用或禁用持久性日志以确保数据文件保持有效和可恢复。
    enabled: true
processManagement:
  #启用在后台运行mongos或mongod进程的守护进程模式。
  fork: true
  #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID
  pidFilePath: "/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.pid"
net:
  #服务实例绑定的IP，默认是localhost
  bindIp: 0.0.0.0
  #bindIp
  #绑定的端口，默认是27017
  port: 27018
replication:
  #副本集的名称
  replSetName: myshardrs01
sharding:
  #分片角色
  clusterRole: shardsvr
```

##### 配置文件

其他两个只需要修改下面这4个27018即可

```yml
systemLog:
  destination: file
  path: "/mongodb/sharded_cluster/myshardrs01_27218/config.log"
  logAppend: true
storage:
  dbPath: "/mongodb/sharded_cluster/myshardrs01_27218/data/db"
  journal:
    enabled: true
processManagement:
  fork: true
  pidFilePath: "/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.pid"
net:
  bindIp: 0.0.0.0
  port: 27218
replication:
  replSetName: myshardrs01
sharding:
  clusterRole: shardsvr
```

依次启动服务

```apl
mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf
mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf
mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf
```

任意进入一个节点

```apl
mongo --port=27018
```

进行初始化操作

```apl
rs.initiate({"_id":"myshardrs01","members":[ {"_id":1,"host":"localhost:27018"}, {"_id":2,"host":"localhost:27118"}, {"_id":3,"host":"localhost:27218","arbiterOnly" : true}]})
```



#### 创建第二个分片节点

**内容同第一个分片节点，只是端口号不同**

##### 创建文件夹

```apl
mkdir -p /mongodb/sharded_cluster/myshardrs02_27318/log \ &
mkdir -p /mongodb/sharded_cluster/myshardrs02_27318/data/db \ &
mkdir -p /mongodb/sharded_cluster/myshardrs02_27418/log \ &
mkdir -p /mongodb/sharded_cluster/myshardrs02_27418/data/db \ &
mkdir -p /mongodb/sharded_cluster/myshardrs02_27518/log \ &
mkdir -p /mongodb/sharded_cluster/myshardrs02_27518/data/db
```

##### 创建配置文件

```apl
vim /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf
vim /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf
vim /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf
```

```yml
systemLog:
  destination: file
  path: "/mongodb/sharded_cluster/myshardrs02_27318/config.log"
  logAppend: true
storage:
  dbPath: "/mongodb/sharded_cluster/myshardrs02_27318/data/db"
  journal:
    enabled: true
processManagement:
  fork: true
  pidFilePath: "/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.pid"
net:
  bindIp: 0.0.0.0
  port: 27318
replication:
  replSetName: myshardrs02
sharding:
  clusterRole: shardsvr
```

##### 依次启动服务

```
mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf
mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf
mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf
```

##### 进入和初始化节点

```apl
mongo --port=27318
```

```apl
rs.initiate({"_id":"myshardrs02","members":[ {"_id":1,"host":"localhost:27318"}, {"_id":2,"host":"localhost:27418"}, {"_id":3,"host":"localhost:27518","arbiterOnly" : true}]})
```

#### 配置节点副本集

##### 创建文件夹

```makefile
#建立数据节点data和日志目录
mkdir -p /mongodb/sharded_cluster/myconfigrs_27019/log \ &
mkdir -p /mongodb/sharded_cluster/myconfigrs_27019/data/db \ &
mkdir -p /mongodb/sharded_cluster/myconfigrs_27119/log \ &
mkdir -p /mongodb/sharded_cluster/myconfigrs_27119/data/db \ &
mkdir -p /mongodb/sharded_cluster/myconfigrs_27219/log \ &
mkdir -p /mongodb/sharded_cluster/myconfigrs_27219/data/db
```

##### 创建配置文件

```apl
vim /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf
vim /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf
vim /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf
```

```yml
systemLog:
  destination: file
  path: "/mongodb/sharded_cluster/myconfigrs_27019/log/config.log"
  logAppend: true
storage:
  dbPath: "/mongodb/sharded_cluster/myconfigrs_27019/data/db"
  journal:
    enabled: true
processManagement:
  fork: true
  pidFilePath: "/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.pid"
net:
  bindIp: 0.0.0.0
  port: 27019
replication:
  replSetName: myconfigrs
sharding:
  clusterRole: configsvr
```

##### 依次启动服务

```apl
mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf
mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf
mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf
```

##### 进入和初始化节点

```apl
mongo --port=27019
```

此时没有仲裁节点

```apl
rs.initiate({"_id":"myconfigrs","members":[ {"_id":1,"host":"localhost:27019"}, {"_id":2,"host":"localhost:27119"}, {"_id":3,"host":"localhost:27219"}]})
```

#### 创建路由节点

##### 创建第一个路由节点

```apl
mkdir -p /mongodb/sharded_cluster/mymongos_27017/log
vi /mongodb/sharded_cluster/mymongos_27017/mongos.conf
```

```apl
systemLog:
  destination: file
  path: "/mongodb/sharded_cluster/mymongos_27017/log/config.log"
  logAppend: true
processManagement:
  fork: true
  pidFilePath: "/mongodb/sharded_cluster/mymongos_27017/log/mongod.pid"
net:
  bindIp: 0.0.0.0
  port: 27017
sharding:
#指定配置节点副本集
  configDB: myconfigrs/127.0.0.1:27019,127.0.0.1:27119,127.0.0.1:27219
```

启动和进入

```apl
mongos -f  /mongodb/sharded_cluster/mymongos_27017/mongos.conf
```

```apl
mongo --port=27017
```

#### 路由节点上配置

分别将副本节点加进入

##### 添加分片

```apl
sh.addShard("IP:Port")
```

```apl
sh.addShard("myshardrs01/localhost:27018,localhost:27118,localhost:27218")
sh.addShard("myshardrs02/localhost:27318,localhost:27418,localhost:27518")
```

##### 查看状态

```apl
sh.status()
```

##### 移除分片

提示：如果添加分片失败，需要先手动移除分片，检查添加分片的信息的正确性后，再次添加分片。

```apl
use admin
db.runCommand( { removeShard: "myshardrs02" } )
```

注意：如果只剩下最后一个shard，是无法删除的，移除时会自动转移分片数据，需要一个时间过程。完成后，再次执行删除分片命令才能真正删除。

##### 开启分片功能

开启分片功能：sh.enableSharding("库名")、sh.shardCollection("库名.集合名",{"key":1})
在mongos上的articledb数据库配置sharding:

```apl
sh.enableSharding("articledb")
sh.status()
```

##### 集合分片

对集合分片，你必须使用 sh.shardCollection() 方法指定集合和分片键

```apl
sh.shardCollection(namespace, key, unique)
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220311160108243.png" alt="image-20220311160108243" style="zoom:67%;" />

对集合进行分片时,你需要选择一个片键（Shard Key） , shard key 是每条记录都必须包含的,且建立了

索引的单个字段或复合字段,MongoDB按照片键将数据划分到不同的 数据块 中,并将 数据块 均衡地分布

到所有分片中.为了按照片键划分数据块,MongoDB使用 基于哈希的分片方式（随机平均分配）或者基

于范围的分片方式（数值大小分配） 。用什么字段当片键都可以，如：nickname作为片键，但一定是必填字段。

分片规则一：哈希策略(推荐)

```apl
sh.shardCollection("articledb.comment",{"nickname":"hashed"})
sh.status()
```

分片规则二：范围策略

对于基于范围的分片 ,MongoDB按照片键的范围把数据分成不同部分.假设有一个数字的片键:想象一个

从负无穷到正无穷的直线,每一个片键的值都在直线上画了一个点.MongoDB把这条直线划分为更短的不

重叠的片段,并称之为 数据块 ,每个数据块包含了片键在一定范围内的数据.

在使用片键做范围划分的系统中,拥有”相近”片键的文档很可能存储在同一个数据块中,因此也会存储在同

一个分片中.
**如使用作者年龄字段作为片键，按照点赞数的值进行分片**

```apl
sh.shardCollection("articledb.author",{"age":1})
```

注意的是：
1）一个集合只能指定一个片键，否则报错。
2）一旦对一个集合分片，分片键和分片值就不可改变。 如：不能给集合选择不同的分片键、不能更新
分片键的值。
3）根据age索引进行分配数据。

##### 查看分片状态

显示集群的详细信息

```
db.printShardingStatus()
```

查看均衡器是否工作（需要重新均衡时系统才会自动启动，不用管它）：

```apl
sh.isBalancerRunning()
```

查看当前Balancer状态

```apl
sh.getBalancerState()
```

#### 插入数据测试

测试一（哈希规则）：登录mongs后，向comment循环插入1000条数据做测试：

```apl
mongo --port=27017
```

```apl
use articledb
```

```apl
for(var i=1;i<=1000;i++){db.comment.insert({_id:i+"",nickname:"BoBo"+i})}
```

```apl
db.comment.count()
```

提示：js的语法，因为mongo的shell是一个JavaScript的shell。

注意：从路由上插入的数据，必须包含片键，否则无法插入。

分别登陆两个片的主节点，统计文档数量

```apl
mongo --port=27018
use articledb
db.comment.count()   //506
```

```apl
mongo --port=27318
use articledb
db.comment.count()   //494
```

可以看到，1000条数据近似均匀的分布到了2个shard上。是根据片键的哈希值分配的。这种分配方式非

常易于水平扩展：一旦数据存储需要更大空间，可以直接再增加分片即可，同时提升了性能。

使用db.comment.stats()查看单个集合的完整情况，mongos执行该命令可以查看该集合的数据分片的

情况。使用sh.status()查看本库内所有集合的分片信息。

测试二（范围规则）：登录mongs后，向comment循环插入1000条数据做测试：

```apl
use articledb
for(var i=1;i<=20000;i++){db.author.save({"name":"BoBBBoBo"+i,"age":NumberInt(i%120)})}
```

如果查看状态发现没有分片，则可能是由于以下原因造成了

- 系统繁忙，正在分片中。

- 数据块（chunk）没有填满，默认的数据块尺寸（chunksize）是64M，填满后才会考虑向其他片的数据块填充数据，因此，为了测试，可以将其改小，这里改为1M，操作如下：

```apl
use config
db.settings.save( { _id:"chunksize", value: 1 } )
```

测试完改回来

```apl
db.settings.save( { _id:"chunksize", value: 64 } )
```

注意：要先改小，再设置分片。为了测试，可以先删除集合，重新建立集合的分片策略，再插入数据测
试即可。

#### 再增加一个路由节点

```apl
mkdir -p /mongodb/sharded_cluster/mymongos_27117/log
vi /mongodb/sharded_cluster/mymongos_27117/mongos.conf
```

```yml
systemLog:
  destination: file
  path: "/mongodb/sharded_cluster/mymongos_27117/log/config.log"
  logAppend: true
processManagement:
  fork: true
  pidFilePath: "/mongodb/sharded_cluster/mymongos_27117/log/mongod.pid"
net:
  bindIp: 0.0.0.0
  port: 27117
sharding:
#指定配置节点副本集
  configDB: myconfigrs/127.0.0.1:27019,127.0.0.1:27119,127.0.0.1:27219
```

连接启动

```
mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf
```

```
mongo --port=27117
```

使用mongo客户端登录27117，发现，**第二个路由无需配置**，因为分片配置都保存到了配置服务器中


外部连接只需要连接路由节点即可

```apl
192.168.220.130:27017
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220311163240541.png" alt="image-20220311163240541" style="zoom:67%;" />

SpringDataMongDB连接分片集群
Java客户端常用的是SpringDataMongoDB，其连接的是mongs路由，配置和单机mongod的配置是一
样的。
多个路由的时候的SpringDataMongoDB的客户端配置参考如下：

```yml
spring:
#数据源配置
  data:
    mongodb:
    #连接路由字符串
      uri: mongodb://180.76.159.126:27017,180.76.159.126:27117/articledb
```

通过日志发现，写入数据的时候，会选择一个路由写入



## 出错处理方案

如果在搭建分片的时候有操作失败或配置有问题，需要重新来过的，可以进行如下操作：
第一步：查询出所有的测试服务节点的进程：

```apl
ps -ef |grep mongo
```

根据上述的进程编号，依次中断进程

```apl
kill -2 进程编号
```

第二步：清除所有的节点的数据

```apl
rm -rf /mongodb/sharded_cluster/myconfigrs_27019/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/myconfigrs_27119/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/myconfigrs_27219/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/myshardrs01_27018/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/myshardrs01_27118/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/myshardrs01_27218/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/myshardrs02_27318/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/myshardrs02_27418/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/myshardrs02_27518/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/mymongos_27017/data/db/*.* \ &
rm -rf /mongodb/sharded_cluster/mymongos_27117/data/db/*.*
```

第三步：查看或修改有问题的配置

第四步：依次启动所有节点，不包括路由节点

第五步：对两个数据分片副本集和一个配置副本集进行初始化和相关配置

第六步：检查路由mongos的配置，并启动mongos

```apl
/usr/local/mongodb/bin/mongod -f
/mongodb/sharded_cluster/mymongos_27017/mongos.cfg
/usr/local/mongodb/bin/mongod -f
/mongodb/sharded_cluster/mymongos_27017/mongos.cfg
```

第七步：mongo登录mongos，在其上进行相关操作                                                                                                                                                                                                                  

# Map Reduce

> Map-Reduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。MongoDB提供的Map-Reduce非常灵活，对于大规模数据分析也相当实用。

## 基本语法

```sql
db.collection.mapReduce(
   function() {emit(key,value);},  //map 函数
   function(key,values) {return reduceFunction},   //reduce 函数
   {
      out: collection,
      query: document,
      sort: document,
      limit: number
   }
)
```

> 使用 MapReduce 要实现两个函数 Map 函数和 Reduce 函数,Map 函数调用 emit(key, value), 遍历 collection 中所有的记录, 将 key 与 value 传递给 Reduce 函数进行处理。Map 函数必须调用 emit(key, value) 返回键值对。

参数说明

- **map** ：映射函数 (生成键值对序列,作为 reduce 函数参数)。
- **reduce** 统计函数，reduce函数的任务就是将key-values变成key-value，就是把values数组变成一个单一值
- **out** 统计结果存放集合 (不指定则使用临时集合,在客户端断开后自动删除)。
- **query** 一个筛选条件，只有满足条件的文档才会调用map函数。（query。limit，sort可以随意组合）
- **sort** 和limit结合的sort排序参数（也是在发往map函数前给文档排序），可以优化分组机制
- **limit** 发往map函数的文档数量的上限（要是没有limit，单独使用sort的用处不大）

## 图解实例

以下实例在集合 orders 中查找 status:"A" 的数据，并根据 cust_id 来分组，并计算 amount 的总和。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304221654464.png" alt="image-20230422165411375" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304221654381.png" alt="image-20230422165427279" style="zoom:80%;" />

## 实战使用

### 数据准备

> 这个数据集合包含了5个订单，其中user_id为1、2、3的三个用户分别有2、2和1个订单，每个订单中包含了多个商品项。我们可以使用上面的MapReduce操作对这些订单数据进行聚合处理和分析。

```sh
db.orders.insertMany([
    {
        user_id: 1,
        total: 100.0,
        items: [
            { name: "Product A", price: 20.0, quantity: 2 },
            { name: "Product B", price: 30.0, quantity: 1 },
            { name: "Product C", price: 10.0, quantity: 3 }
        ]
    },
    {
        user_id: 2,
        total: 50.0,
        items: [
            { name: "Product A", price: 20.0, quantity: 1 },
            { name: "Product B", price: 30.0, quantity: 1 }
        ]
    },
    {
        user_id: 1,
        total: 75.0,
        items: [
            { name: "Product A", price: 20.0, quantity: 1 },
            { name: "Product C", price: 10.0, quantity: 5 }
        ]
    },
    {
        user_id: 3,
        total: 200.0,
        items: [
            { name: "Product B", price: 30.0, quantity: 3 },
            { name: "Product D", price: 40.0, quantity: 2 }
        ]
    },
    {
        user_id: 2,
        total: 80.0,
        items: [
            { name: "Product A", price: 20.0, quantity: 2 },
            { name: "Product C", price: 10.0, quantity: 4 }
        ]
    }
])
```

### 编写map函数

> map函数用于将原始数据集合映射为一系列键值对，其中键表示聚合操作的依据，值则是需要聚合的数据。下面是一个简单的例子，该例子使用map函数将订单数据按照用户ID进行分组：

```sql
var mapFunction = function() {
    emit(this.user_id, { order_total: this.total });
};
```

> 这里的emit函数用于生成键值对，this表示当前正在处理的文档，this.user_id表示文档中的用户ID，this.total表示文档中的订单总额。

### 编写reduce函数

> reduce函数用于对map函数生成的键值对进行聚合操作，将相同键的值聚合为单个结果。下面是一个简单的例子，该例子使用reduce函数将同一用户的订单总额求和：

```sql
var reduceFunction = function(key, values) {
    var sum = 0;
    values.forEach(function(doc) {
        sum += doc.order_total;
    });
    return { order_total: sum };
};
```

### 执行MapReduce操作

> 有了map和reduce函数后，我们可以使用MongoDB的mapReduce()函数执行MapReduce操作。下面是一个简单的例子，该例子使用上面定义的map和reduce函数，将订单数据按照用户ID进行分组，然后计算每个用户的订单总额：

```sql
db.orders.mapReduce(
    mapFunction,
    reduceFunction,
    { out: "order_totals" }
)
```

> 这里的mapFunction和reduceFunction分别是之前定义的map和reduce函数，{ out: "order_totals" }表示将聚合结果保存到名为order_totals的集合中。

### 查询结果

> 执行完上述代码后，MongoDB会自动执行MapReduce操作，将结果存储在order_totals集合中。我们可以使用find()函数查询该集合，查看MapReduce操作的结果：

```sql
db.order_totals.find()
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202304221715349.png" alt="image-20230422171502270" style="zoom:80%;" />

> 这样就可以使用MongoDB的MapReduce功能对数据进行聚合处理和分析了。需要注意的是，由于MapReduce操作需要对数据进行分片处理，因此对于大型数据集，可能需要使用sharding等技术来优化性能。

# 原子操作

> mongodb不支持事务，所以，在你的项目中应用时，要注意这点。无论什么设计，都不要要求mongodb保证数据的完整性。但是mongodb提供了许多原子操作，比如文档的保存，修改，删除等，都是原子操作。所谓原子操作就是要么这个文档保存到Mongodb，要么没有保存到Mongodb，不会出现查询到的文档没有保存完整的情况。

## 数据准备

考虑下面的例子，图书馆的书籍及结账信息。

实例说明了在一个相同的文档中如何确保嵌入字段关联原子操作（update：更新）的字段是同步的。

```sql
db.book.insertOne({
  _id: 123456789,
  title: "MongoDB: The Definitive Guide",
  author: [ "Kristina Chodorow", "Mike Dirolf" ],
  published_date: ISODate("2010-09-24"),
  pages: 216,
  language: "English",
  publisher_id: "oreilly",
  available: 3,
  checkout: [ { by: "joe", date: ISODate("2012-10-15") } ]
})
```

## 常用命令

### $set

用来指定一个键并更新键值，若键不存在并创建。我们可以使用 $set 操作符修改 name 字段的值：

```sql
db.book.update({_id: 123456789}, {$set: {published_date: new Date()}})
```

### $unset

原子地删除一个字段

```sql
db.book.update({_id: 123456789}, {$unset: {published_date: 1}})
```

### $inc

$inc可以对文档的某个值为数字型（只能为满足要求的数字）的键进行增减的操作。

```sql
db.book.update({_id: 123456789}, {$inc: {available: 1}})
db.book.update({_id: 123456789}, {$inc: {available: 2}})
db.book.update({_id: 123456789}, {$inc: {available: 3}})
```

### $push

把value追加到field里面去，field一定要是数组类型才行，**如果field不存在，会新增一个数组类型加进去**。

```sql
db.book.update({_id: 123456789}, {$push: {published_date: new Date()}})
```

### $pull

从数组field内删除一个等于value值。

```sql
db.book.update({_id: 123456789}, {$pull: {author: "Mike Dirolf"}})
```

### $addToSet

增加一个值到数组内，而且只有当这个值不在数组内才增加。

```sql
db.book.update({_id: 123456789}, {$addToSet: {author: "renshuo"}})
```

### $pop

删除数组的第一个或最后一个元素

```sql
db.book.update({_id: 123456789}, {$pop: {author: 1}})
```

### $rename

修改字段名称，旧字段名:新字段名

```sql
db.book.update({_id: 123456789}, {$rename: {author: 'author1'}})
```

































