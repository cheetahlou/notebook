

# 为什么要分库分表

## 前言

> 分库分表是为了解决由于库、表数据量过大，而导致数据库性能持续下降的问题。按照一定的规则，将原本数据量大的数据库拆分成多个单独的数据库，将原本数据量大的表拆分成若干个数据表，使得单一的库、表性能达到最优的效果`（响应速度快），以此提升整体数据库性能。

> 分库分表的核心理念就是对数据进行切分（Sharding），以及切分后如何对数据的快速定位与查询结果整合。而分库与分表都可以从：垂直（纵向）和 水平（横向）两种纬度进行切分。
>

## 为什么要分库分表

> 首先要明确一个问题，单一的数据库是否能够满足公司目前的线上业务需求，比如用户表，可能有几千万，甚至上亿的数据，只是说可能，如果有这么多用户，那必然是大公司了，那么这个时候，如果不分表也不分库的话，那么数据了上来的时候，MySQL单机磁盘容量会撑爆，但是如果拆成多个数据库，磁盘使用率大大降低。
>

> 这样就把磁盘使用率降低，这是通过硬件的形式解决问题，如果数据量是巨大的，这时候，SQL 如果没有命中索引，那么就会导致一个情况，查这个表的SQL语句直接把数据库给干崩了。
>

> 即使SQL命中了索引，如果表的数据量 超过一千万的话， 查询也是会明显变慢的。这是因为索引一般是B+树结构，数据千万级别的话，B+树的高度会增高，查询自然就变慢了，当然，这是题外话了。
>

> 1.IO瓶颈：热点数据太多，数据库缓存不足，产生大量磁盘IO，效率较低。 请求数据太多，带宽不够，网络IO
>
> 2.CPU瓶颈：排序、分组、连接查询、聚合统计等SQL会耗费大量的CPU资源，请求数太多，CPU出现瓶颈。

## 1 垂直方向

`垂直方向`主要针对的是`业务`，下面聊聊业务的发展跟分库分表有什么关系。

### 1 单库

在系统初期，业务功能相对来说比较简单，系统模块较少。为了快速满足迭代需求，减少一些不必要的依赖。更重要的是减少系统的复杂度，保证开发速度，我们通常会使用`单库`来保存数据。系统初期的数据库架构如下：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211758820.png" alt="image-20220421175848694" style="zoom: 50%;" />

此时，使用的数据库方案是：`一个数据库`包含`多张业务表`。用户读数据请求和写数据请求，都操作的同一个数据库。

### 2 分表

**概念：**以字段为依据，按照字段的活跃性，将表中字段拆到不同的表（主表和扩展表）中。系统上线之后，随着业务的发展，不断的添加新功能。导致单表中的字段越来越多，开始变得有点不太好维护了。一个用户表就包含了几十甚至上百个字段，管理起来有点混乱。这时候该怎么办呢？

答：分表。将`用户表`拆分为：`用户基本信息表` 和 `用户扩展表`。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211759113.png" alt="image-20220421175926995" style="zoom:67%;" />

用户基本信息表中存的是用户最主要的信息，比如：用户名、密码、别名、手机号、邮箱、年龄、性别等核心数据。

这些信息跟用户息息相关，查询的频次非常高。

而用户扩展表中存的是用户的扩展信息，比如：所属单位、户口所在地、所在城市等等，非核心数据。

这些信息只有在特定的业务场景才需要查询，而绝大数业务场景是不需要的。

所以通过分表把核心数据和非核心数据分开，让表的结构更清晰，职责更单一，更便于维护。

除了按实际业务分表之外，我们还有一个常用的分表原则是：把调用频次高的放在一张表，调用频次低的放另一张表

有个非常经典的例子就是：订单表和订单详情表。

**结果：**

- 每个表的结构都不一样；
- 每个表的数据也不一样，一般来说，每个表的字段至少有一列交集，一般是主键，用于关联数据；
- 所有表的并集是全量数据；

**场景：**系统绝对并发量并没有上来，表的记录并不多，但是字段多，并且热点数据和非热点数据在一起，单行数据所需的存储空间较大。以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈。

**分析：**可以用列表页和详情页来帮助理解。垂直分表的拆分原则是将热点数据（可能会冗余经常一起查询的数据）放在一起作为主表，非热点数据放在一起作为扩展表。这样更多的热点数据就能被缓存下来，进而减少了随机读IO。拆了之后，要想获得全部数据就需要关联两个表来取数据。

但记住，千万别用join，因为join不仅会增加CPU负担并且会讲两个表耦合在一起（必须在一个数据库实例上）。关联数据，应该在业务Service层做文章，分别获取主表和扩展表数据然后用关联字段关联得到全部数据。

### 3 分库

**概念：**以表为依据，按照业务归属不同，将不同的表拆分到不同的库中。

不知不觉，系统已经上线了一年多的时间了。经历了N个迭代的需求开发，功能已经非常完善。

系统功能完善，意味着系统各种关联关系，错综复杂。

此时，如果不赶快梳理业务逻辑，后面会带来很多隐藏问题，会把自己坑死。

这就需要按业务功能，划分不同领域了。把相同领域的表放到同一个数据库，不同领域的表，放在另外的数据库。

具体拆分过程如下：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211800522.png" alt="image-20220421180032389" style="zoom:67%;" />

将用户、产品、物流、订单相关的表，从原来一个数据库中，拆分成单独的用户库、产品库、物流库和订单库，一共四个数据库。

> 在这里为了看起来更直观，每个库我只画了一张表，实际场景可能有多张表。

这样按领域拆分之后，每个领域只用关注自己相关的表，职责更单一了，一下子变得更好维护了。

**结果：**

- 每个库的结构都不一样；
- 每个库的数据也不一样，没有交集；
- 所有库的并集是全量数据；

**场景：**系统绝对并发量上来了，并且可以抽象出单独的业务模块。

**分析：**到这一步，基本上就可以服务化了。例如，随着业务的发展一些公用的配置表、字典表等越来越多，这时可以将这些表拆到单独的库中，甚至可以服务化。再有，随着业务的发展孵化出了一套业务模式，这时可以将相关的表拆到单独的库中，甚至可以服务化。

### 4 分库分表

有时候按业务，只分库，或者只分表是不够的。比如：有些财务系统，需要按月份和年份汇总，所有用户的资金。

这就需要做：`分库分表`了。

每年都有个单独的数据库，每个数据库中，都有12张表，每张表存储一个月的用户资金数据。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211802172.png" alt="image-20220421180227044" style="zoom:50%;" />

这样分库分表之后，就能非常高效的查询出某个用户每个月，或者每年的资金了。

此外，还有些比较特殊的需求，比如需要按照地域分库，比如：华中、华北、华南等区，每个区都有一个单独的数据库。甚至有些游戏平台，按接入的游戏厂商来做分库分表。

### 5 垂直切分优缺点

**垂直切分的优点**：

> - 业务间数据解耦，不同业务的数据进行独立的维护、监控、扩展。
> - 在高并发场景下，一定程度上缓解了数据库的压力。

**垂直切分的缺点**：

> - 提升了开发的复杂度，由于业务的隔离性，很多表无法直接访问，必须通过接口方式聚合数据。
> - 分布式事务管理难度增加。
> - 数据库还是存在单表数据量过大的问题，并未根本上解决，需要配合水平切分。



## 2 水平方向

`水分方向`主要针对的是`数据`，下面聊聊数据跟分库分表又有什么关系。

### 1 单库

在系统初期，由于用户非常少，所以系统并发量很小。并且存在表中的数据量也非常少。

这时的数据库架构如下：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211802839.png" alt="image-20220421180257701" style="zoom:67%;" />

此时，使用的数据库方案同样是：`一个master数据库`包含`多张业务表`。

用户读数据请求和写数据请求，都是操作的同一个数据库，该方案比较适合于并发量很低的业务场景。

### 2 主从读写分离

系统上线一段时间后，用户数量增加了。此时，你会发现用户的请求当中，读数据的请求占据了大部分，真正写数据的请求占比很少。众所周知，`数据库连接是有限的`，它是非常宝贵的资源。而每次数据库的读或写请求，都需要占用至少一个数据库连接。

如果写数据请求需要的数据库连接，被读数据请求占用完了，不就写不了数据了？这样问题就严重了。

为了解决该问题，我们需要把`读库`和`写库`分开。于是，就出现了主从读写分离架构：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211803444.png" alt="image-20220421180338319" style="zoom:67%;" />

考虑刚开始用户量还没那么大，选择的是`一主一从`的架构，也就是常说的一个master一个slave。

所有的写数据请求，都指向主库。一旦主库写完数据之后，立马异步同步给从库。这样所有的读数据请求，就能及时从从库中获取到数据了（除非网络有延迟）。

读写分离方案可以解决上面提到的单节点问题，相对于单库的方案，能够更好的保证系统的稳定性。

因为如果主库挂了，可以升级从库为主库，将所有读写请求都指向新主库，系统又能正常运行了。

> 读写分离方案其实也是分库的一种，它相对于为数据做了备份，它已经成为了系统初期的首先方案。

但这里有个问题就是：如果用户量确实有些大，如果master挂了，升级slave为master，将所有读写请求都指向新master。

但此时，如果这个新master根本扛不住所有的读写请求，该怎么办？

这就需要`一主多从`的架构了：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211804749.png" alt="image-20220421180433612" style="zoom:67%;" />

上图中我列的是`一主两从`，如果master挂了，可以选择从库1或从库2中的一个，升级为新master。假如我们在这里升级从库1为新master，则原来的从库2就变成了新master的的slave了。

调整之后的架构图如下：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211805792.png" alt="image-20220421180515645" style="zoom: 50%;" />

### 3 分库

**概念：**以字段为依据，按照一定策略（hash、range等），将一个库中的数据拆分到多个库中。

上面的读写分离方案确实可以解决读请求大于写请求时，导致master节点扛不住的问题。但如果某个领域，比如：用户库。如果注册用户的请求量非常大，即写请求本身的请求量就很大，一个master库根本无法承受住这么大的压力。

这时该怎么办呢？答：建立多个用户库。用户库的拆分过程如下：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211805116.png" alt="image-20220421180552987" style="zoom:67%;" />

**结果：**

- 每个库的结构都一样；
- 每个库的数据都不一样，没有交集；
- 所有库的并集是全量数据；

**场景：**系统绝对并发量上来了，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库。

**分析：**库多了，io和cpu的压力自然可以成倍缓解。

### 4 分表

**概念：**以字段为依据，按照一定策略（hash、range等），将一个表中的数据拆分到多个表中

用户请求量上来了，带来的势必是数据量的成本上升。即使做了分库，但有可能单个库，比如：用户库，出现了5000万的数据。

根据经验值，单表的数据量应该尽量控制在1000万以内，性能是最佳的。如果有几千万级的数据量，用单表来存，性能会变得很差。

如果数据量太大了，需要建立的索引也会很大，从小到大检索一次数据，会非常耗时，而且非常消耗cpu资源。

这时该怎么办呢？

答：`分表`，这样可以控制每张表的数据量，和索引大小。

表拆分过程如下：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211806143.png" alt="image-20220421180641011" style="zoom:50%;" />

我在这里将用户库中的用户表，拆分成了四张表（真实场景不一定是这样的），每张表的表结构是一模一样的，只是存储的数据不一样。

如果以后用户数据量越来越大，只需再多分几张用户表即可。

**结果：**

- 每个表的结构都一样；
- 每个表的数据都不一样，没有交集；
- 所有表的并集是全量数据；

**场景：**系统绝对并发量并没有上来，只是单表的数据量太多，影响了SQL效率，加重了CPU负担，以至于成为瓶颈。推荐：[一次SQL查询优化原理分析](http://mp.weixin.qq.com/s?__biz=MzI4Njc5NjM1NQ==&mid=2247491313&idx=2&sn=01d82309c459c7385a2ccf0018bb0d8a&chksm=ebd621dddca1a8cb16f33497c45aeb44e95ca0d5289afee263e047a868cb67f34ada644418b0&scene=21#wechat_redirect)

**分析：**表的数据量少了，单次SQL执行效率高，自然减轻了CPU的负担。

### 5 分库分表

当系统发展到一定的阶段，用户并发量大，而且需要存储的数据量也很多。这时该怎么办呢？

答：需要做`分库分表`。如下图所示：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211807831.png" alt="image-20220421180714692" style="zoom:67%;" />

图中将用户库拆分成了三个库，每个库都包含了四张用户表。

如果有用户请求过来的时候，先根据用户id路由到其中一个用户库，然后再定位到某张表。

路由的算法挺多的：

- `根据id取模`，比如：id=7，有4张表，则7%4=3，模为3，路由到用户表3。
- `给id指定一个区间范围`，比如：id的值是0-10万，则数据存在用户表0，id的值是10-20万，则数据存在用户表1。
- `一致性hash算法`

这篇文章就不过多介绍了，后面会有文章专门介绍这些路由算法的。

## 3 真实案例

接下来，废话不多说，给大家分享三个我参与过的分库分表项目经历，给有需要的朋友一个参考。

### 1 分库

我之前待过一家公司，我们团队是做游戏运营的，我们公司提供平台，游戏厂商接入我们平台，推广他们的游戏。

游戏玩家通过我们平台登录，成功之后跳转到游戏厂商的指定游戏页面，该玩家就能正常玩游戏了，还可以充值游戏币。

这就需要建立我们的账号体系和游戏厂商的账号的映射关系，游戏玩家通过登录我们平台的游戏账号，成功之后转换成游戏厂商自己平台的账号。

这里有两个问题：

1. 每个游戏厂商的接入方式可能都不一样，账号体系映射关系也有差异。
2. 用户都从我们平台登录，成功之后跳转到游戏厂商的游戏页面。当时有N个游戏厂商接入了，活跃的游戏玩家比较多，登录接口的并发量不容小觑。

为了解决这两个问题，我们当时采用的方案是：`分库`。即针对每一个游戏都单独建一个数据库，数据库中的表结构允许存在差异。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211807434.png" alt="image-20220421180745318" style="zoom: 67%;" />

### 2 分表

还是在那家游戏平台公司，我们还有另外一个业务就是：`金钻会员`。说白了就是打造了一套跟游戏相关的会员体系，为了保持用户的活跃度，开通会员有很多福利，比如：送游戏币、充值有折扣、积分兑换、抽奖、专属客服等等。

在这套会员体系当中，有个非常重要的功能就是：`积分`。用户有很多种途径可以获取积分，比如：签到、充值、玩游戏、抽奖、推广、参加活动等等。积分用什么用途呢？

1. 退换实物礼物
2. 兑换游戏币
3. 抽奖

说了这么多，其实就是想说，一个用户一天当中，获取积分或消费积分都可能有很多次，那么，一个用户一天就可能会产生几十条记录。如果用户多了的话，积分相关的数据量其实挺惊人的。我们当时考虑了，水平方向的数据量可能会很大，但是用户并发量并不大，不像登录接口那样。所以采用的方案是：`分表`。

当时使用一个积分数据库就够了，但是分了128张表。然后根据用户id，进行hash除以128取模。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211808217.png" alt="image-20220421180818087" style="zoom:67%;" />

> 需要特别注意的是，分表的数量最好是2的幂次方，方便以后扩容。

### 3 分库分表

后来我去了一家从事餐饮软件开发的公司。这个公司有个特点是在每天的中午和晚上的就餐高峰期，用户的并发量很大。用户吃饭前需要通过我们系统点餐，然后下单，然后结账。当时点餐和下单的并发量挺大的。

餐厅可能会有很多人，每个人都可能下多个订单。这样就会导致用户的并发量高，并且数据量也很大。

所以，综合考虑了一下，当时我们采用的技术方案是：`分库分表`。

经过调研之后，觉得使用了当当网开源的基于jdbc的中间件框架：`sharding-jdbc`。

当时分了4个库，每个库有32张表。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211808597.png" alt="image-20220421180845461" style="zoom:67%;" />

## 4 一定规则是什么

我们上边提到过很多次 `一定规则` ，这个规则其实是一种路由算法，就是决定一条数据具体应该存在哪个数据库的哪张表里。常见的有 `取模算法` 和 `范围限定算法`

### 1、取模算法

按字段取模（对hash结果取余数 (hash() mod N)，N为数据库实例数或子表数量）是最为常见的一种切分方式。

还拿 `order` 订单表举例，先对数据库从 0 到 N-1进行编号，对 `order` 订单表中 `work_no` 订单编号字段进行取模，得到余数 `i`，`i=0`存第一个库，`i=1`存第二个库，`i=2`存第三个库....以此类推。

这样同一笔订单的数据都会存在同一个库、表里，查询时用相同的规则，用 `work_no` 订单编号作为查询条件，就能快速的定位到数据。

**优点：**

数据分片相对比较均匀，不易出现请求都打到一个库上的情况。

**缺点：**

这种算法存在一些问题，当某一台机器宕机，本应该落在该数据库的请求就无法得到正确的处理，这时宕掉的实例会被踢出集群，此时算法变成hash(userId) mod N-1，用户信息可能就不再在同一个库中了。

### 2、范围限定算法

按照 `时间区间` 或 `ID区间` 来切分，比如：我们切分的是用户表，可以定义每个库的 `User` 表里只存10000条数据，第一个库只存 `userId` 从1 ~ 9999的数据，第二个库存 `userId` 为10000 ~ 20000，第三个库存 `userId` 为 20001~ 30000......以此类推，按时间范围也是同理。

**优点：**

- 单表数据量是可控的
- 水平扩展简单只需增加节点即可，无需对其他分片的数据进行迁移
- 能快速定位要查询的数据在哪个库

**缺点：**

- 由于连续分片可能存在数据热点，比如按时间字段分片，可能某一段时间内订单骤增，可能会被频繁的读写，而有些分片存储的历史数据，则很少被查询。

## 5 分库分表的难点

### 1、分布式事务

由于表分布在不同库中，不可避免会带来跨库事务问题。一般可使用 "`三阶段提交` "和 "`两阶段提交`" 处理，但是这种方式性能较差，代码开发量也比较大。通常做法是做到最终一致性的方案，如果不苛求系统的实时一致性，只要在允许的时间段内达到最终一致性即可，采用事务补偿的方式。

这里我应用阿里的分布式事务框架`Seata` 来做分布式事务的管理，后边会结合实际案例。

### 2、分页、排序、跨库联合查询

分页、排序、联合查询是开发中使用频率非常高的功能，但在分库分表后，这些看似普通的操作却是让人非常头疼的问题。将分散在不同库中表的数据查询出来，再将所有结果进行汇总整理后提供给用户。

### 3、分布式主键

分库分表后数据库的自增主键意义就不大了，因为我们不能依靠单个数据库实例上的自增主键来实现不同数据库之间的全局唯一主键，此时一个能够生成全局唯一ID的系统是非常必要的，那么这个全局唯一ID就叫 `分布式ID`。

### 4、读写分离

不难发现大部分主流的关系型数据库都提供了主从架构的高可用方案，而我们需要实现 `读写分离` + `分库分表`，读库与写库都要做分库分表处理，后边会有具体实战案例。

### 5、数据脱敏

数据脱敏，是指对某些敏感信息通过脱敏规则进行数据转换，从而实现敏感隐私数据的可靠保护，如身份证号、手机号、卡号、账号密码等个人信息，一般这些都需要进行做脱敏处理。

## 6 分库分表工具

我还是那句话，尽量不要自己造轮子，因为自己造的轮子可能不那么圆，业界已经有了很多比较成熟的分库分表中间件，我们根据自身的业务需求挑选，将更多的精力放在业务实现上。

- `sharding-jdbc`（当当）
- `TSharding`（蘑菇街）
- `Atlas`（奇虎360）
- `Cobar`（阿里巴巴）
- `MyCAT`（基于Cobar）
- `Oceanus`（58同城）
- `Vitess`（谷歌）

## 7 为什么选 sharding-jdbc

sharding-jdbc是一款轻量级 Java框架，以 jar包形式提供服务，是属于客户端产品不需要额外部署，它相当于是个增强版的 `JDBC` 驱动；相比之下像 `Mycat` 这类需要单独的部署服务的服务端产品，就稍显复杂了。况且我想把更多精力放在实现业务，不想做额外的运维工作。

- `sharding-jdbc`的兼容性也非常强大，适用于任何基于 `JDBC` 的 `ORM` 框架，如：`JPA`， `Hibernate`，`Mybatis`，`Spring JDBC Template` 或直接使用的 `JDBC`。
- 完美兼容任何第三方的数据库连接池，如：`DBCP`， `C3P0`， `BoneCP`，`Druid`， `HikariCP` 等，几乎对所有关系型数据库都支持。

不难发现确实是比较强大的一款工具，而且它对项目的侵入性很小，几乎不用做任何代码层的修改，也无需修改 `SQL` 语句，只需配置待分库分表的数据表即可。



## 8 总结

上面主要从：垂直和水平，两个方向介绍了我们的系统为什么要分库分表。

说实话垂直方向（即业务方向）更简单。

在水平方向（即数据方向）上，`分库`和`分表`的作用，其实是有区别的，不能混为一谈。

- `分库`：是为了解决数据库连接资源不足问题，和磁盘IO的性能瓶颈问题。
- `分表`：是为了解决单表数据量太大，sql语句查询数据时，即使走了索引也非常耗时问题。此外还可以解决消耗cpu资源问题。
- `分库分表`：可以解决 数据库连接资源不足、磁盘IO的性能瓶颈、检索数据耗时 和 消耗cpu资源等问题。

如果在有些业务场景中，用户并发量很大，但是需要保存的数据量很少，这时可以只分库，不分表。

如果在有些业务场景中，用户并发量不大，但是需要保存的数量很多，这时可以只分表，不分库。

如果在有些业务场景中，用户并发量大，并且需要保存的数量也很多时，可以分库分表。



# 分库分表原理

[SpringBoot + Sharding JDBC，一文搞定分库分表、读写分离 (qq.com)](https://mp.weixin.qq.com/s?__biz=Mzg4MjU0OTM1OA==&mid=2247505881&idx=1&sn=eb411129decbe83d1c0c00853206235c&chksm=cf5660d8f821e9ce5ddbd59704fb9f3da1ddd3dc0c1f6b5bbcf8988a3ad15e4f17e0265bd44f&mpshare=1&scene=23&srcid=10214LTIkcjmunw4ejFsj5Yo&sharer_sharetime=1666366564298&sharer_shareid=29b8a04db1dbd975e3bf4e9f47e7ac67#rd)

## MySQL分库分表原理

### 分库分表

`水平拆分：`同一个表的数据拆到不同的库不同的表中。可以根据时间、地区或某个业务键维度，也可以通过hash进行拆分，最后通过路由访问到具体的数据。拆分后的每个表结构保持一致

`垂直拆分：`就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，可以根据业务维度进行拆分，如订单表可以拆分为订单、订单支持、订单地址、订单商品、订单扩展等表；也可以，根据数据冷热程度拆分，20%的热点字段拆到一个表，80%的冷字段拆到另外一个表

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210251621882.png" alt="image-20221025162115810" style="zoom: 67%;" />

### 不停机分库分表数据迁移

一般数据库的拆分也是有一个过程的，一开始是单表，后面慢慢拆成多表。那么就看下如何平滑的从MySQL单表过度到MySQL的分库分表架构

- 利用MySQL+Canal做增量数据同步，利用分库分表中间件，将数据路由到对应的新表中
- 利用分库分表中间件，全量数据导入到对应的新表中
- 通过单表数据和分库分表数据两两比较，更新不匹配的数据到新表中
- 数据稳定后，将单表的配置切换到分库分表配置上

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210251622926.png" alt="image-20221025162217816" style="zoom: 67%;" />

## 分库分表方案

分库分表方案，不外乎就两种，一种是垂直切分，一种是水平切分。

但是总有做开发的小伙伴不知道这垂直切分和水平切分到底是什么样的，为什么垂直切分，为什么水平切分，什么时候应该选择垂直切分，什么时候应该选择水平切分。

有人是这么说的，垂直切分是根据业务来拆分数据库，同一类业务的数据表拆分到一个独立的数据库，另一类的数据表拆分到其他数据库。

有些人不理解这个，实际上垂直切分也是有划分的，上面描述的是垂直切分数据库，可能容易让很多人不太理解，但是如果是垂直切分表，那么肯定百分之90的人都能理解。

### 垂直切分

有一张Order表，表中有诸多记录，比如设计这么一张简单的表。

| **id** | **order_id**                     | **order_date**      | **order_type** | **order_state** |
| :----- | :------------------------------- | :------------------ | :------------- | :-------------- |
| 1      | cd96cff0356e483caae6b2ff4e878fd6 | 2022-06-18 13:57:11 | 支付宝         | 1               |
| 2      | e2496f9e22ce4391806b18480440526a | 2022-06-18 14:22:33 | 微信           | 2               |
| 3      | 9e7ab5a1915c4570a9eaaaa3c01f79c1 | 2022-06-18 15:21:44 | 现金           | 2               |

以上是简化版Order表，如果想要垂直切分，那么应该怎么处理？

直接拆分成2个表，这时候就直接就一分为2 ，咔的一下拆分成两个表

Order1

| **id** | **order_id**                     | **order_date**      |
| :----- | :------------------------------- | :------------------ |
| 1      | cd96cff0356e483caae6b2ff4e878fd6 | 2022-06-18 13:57:11 |
| 2      | e2496f9e22ce4391806b18480440526a | 2022-06-18 14:22:33 |
| 3      | 9e7ab5a1915c4570a9eaaaa3c01f79c1 | 2022-06-18 15:21:44 |

Order2

| **id** | **order_type** | **order_state** |
| :----- | :------------- | :-------------- |
| 1      | 支付宝         | 1               |
| 2      | 微信           | 2               |
| 3      | 现金           | 2               |

这时候主键ID保持的时一致的，而这个操作，就是垂直拆分，分表的操作。

既然说了垂直拆分，那么必然就有水平拆分，

什么是水平拆分呢？实际上水平拆分的话，那真的是只有一句话。

### 水平切分

#### 按照数据来拆分

`水平拆分数据库：`将一张表的数据 ( 按照数据行) 分到多个不同的数据库。每个库的表结构相同，每个库都只有这张表的部分数据，当单表的数据量过大，如果继续使用水平分库，那么数据库的实例 就会不断增加，不利于系统的运维。这时候就要采用水平分表。

`水平拆分表：`将一张表的数据 ( 按照数据行) ，分配到同一个数据库的多张表中，每个表都只有一部分数据。

来看看Order表进行水平拆分的话，是什么样子的。

Order1

| **id** | **order_id**                     | **order_date**      | **order_type** | **order_state** |
| :----- | :------------------------------- | :------------------ | :------------- | :-------------- |
| 1      | cd96cff0356e483caae6b2ff4e878fd6 | 2022-06-18 13:57:11 | 支付宝         | 1               |
| 2      | e2496f9e22ce4391806b18480440526a | 2022-06-18 14:22:33 | 微信           | 2               |

Order2

| **id** | **order_id**                     | **order_date**      | **order_type** | **order_state** |
| :----- | :------------------------------- | :------------------ | :------------- | :-------------- |
| 3      | 9e7ab5a1915c4570a9eaaaa3c01f79c1 | 2022-06-18 15:21:44 | 现金           | 2               |

实际上就是水平的把表数据给分成了2份，这么看起来是不是就很好理解了。

## 分库分表带来的问题

### 事务问题

首先，分库分表最大的隐患就是，事务的一致性， 当需要更新的内容同时分布在不同的库时，不可避免的会产生跨库的事务问题。

原来在一个数据库操 作，本地事务就可以进行控制，分库之后 一个请求可能要访问多个数据库，如何保证事务的一致性，目前还没有简单的解决方案。

### 无法联表的问题

还有一个就是，没有办法进行联表查询了，因为，原来在一个库中的一些表，被分散到多个库，并且这些数据库可能还不在一台服务器，无法关联查询，所以相对应的业务代码可能就比较多了。

### 分页问题

分库并行查询时，如果用到了分页，每个库返回的结果集本身是无序的，只有将多个库中的数据先查出来，然后再根据排序字段在内存中进行排序，如果查询结果过大也是十分消耗资源的。

### 分库分表的技术

目前比较流行的就两种，一种是MyCat，另外一种则是Sharding-Jdbc，都是可以进行分库的，

MyCat是一个数据库中间件，Sharding-Jdbc是以 jar 包提供服务的jdbc框架。

Mycat和Sharding-jdbc 实现原理也是不同：

Mycat的原理中最重要的一个动词是“拦截”，它拦截了用户发送过来的SQL语句，首先对SQL语句做了一些特定的分析：如分库分表分析、路由分析、读写分离分析、缓存分析等，然后将此SQL发往后端的真实数据库，并将返回的结果做适当的处理，最终再返回给用户。

而Sharding-JDBC的原理是接受到一条SQL语句时，会陆续执行SQL解析 => 查询优化 => SQL路由 => SQL改写 => SQL执行 => 结果归并 ，最终返回执行结果。

### 小结

**垂直分表**：将一张宽表(字段很多的表)，按照字段的访问频次进行拆分，就是按照表单结构进行 拆。

**垂直分库**：根据不同的业务，将表进行分类，拆分到不同的数据库。这些库可以部署在不同的服 务器，分摊访问压力。

**水平分库**：将一张表的数据 ( 按照数据行) 分到多个不同的数据库。每个库的表结构相同

**水平分表**：将一张表的数据 ( 按照数据行) ，分配到同一个数据库的多张表中，每个表都只有一部 分数据。



# ShardingSphere

[SpringBoot + Sharding JDBC，一文搞定分库分表、读写分离 (qq.com)](https://mp.weixin.qq.com/s?__biz=Mzg4MjU0OTM1OA==&mid=2247505881&idx=1&sn=eb411129decbe83d1c0c00853206235c&chksm=cf5660d8f821e9ce5ddbd59704fb9f3da1ddd3dc0c1f6b5bbcf8988a3ad15e4f17e0265bd44f&mpshare=1&scene=23&srcid=10214LTIkcjmunw4ejFsj5Yo&sharer_sharetime=1666366564298&sharer_shareid=29b8a04db1dbd975e3bf4e9f47e7ac67#rd)

## 1、简介

官网：https://shardingsphere.apache.org/index_zh.html

文档：https://shardingsphere.apache.org/document/5.1.1/cn/overview/

> Apache ShardingSphere 是一套开源的分布式数据库解决方案组成的生态圈，它由 JDBC、Proxy 和 Sidecar（规划中）这 3 款既能够独立部署，又支持混合部署配合使用的产品组成。 它们均提供标准化的数据水平扩展、分布式事务和分布式治理等功能，可适用于如 Java 同构、异构语言、云原生等各种多样化的应用场景。

> Apache ShardingSphere 旨在充分合理地在分布式的场景下利用关系型数据库的计算和存储能力，而并非实现一个全新的关系型数据库。 关系型数据库当今依然占有巨大市场份额，是企业核心系统的基石，未来也难于撼动，我们更加注重在原有基础上提供增量，而非颠覆。

> Apache ShardingSphere 5.x 版本开始致力于可插拔架构，项目的功能组件能够灵活的以可插拔的方式进行扩展。 目前，数据分片、读写分离、数据加密、影子库压测等功能，以及 MySQL、PostgreSQL、SQLServer、Oracle 等 SQL 与协议的支持，均通过插件的方式织入项目。 开发者能够像使用积木一样定制属于自己的独特系统。Apache ShardingSphere 目前已提供数十个 SPI 作为系统的扩展点，仍在不断增加中。ShardingSphere 已于2020年4月16日成为 Apache 软件基金会的顶级项目。

## 2、ShardingSphere-JDBC

**程序代码封装**

定位为轻量级 Java 框架，`在 Java 的 JDBC 层提供的额外服务`。 它使用客户端直连数据库，`以 jar 包形式提供服务`，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。

适用于任何基于 JDBC 的 ORM 框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template 或直接使用 JDBC。
支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP 等。
支持任意实现 JDBC 规范的数据库，目前支持 MySQL，Oracle，SQLServer，PostgreSQL 以及任何遵循 SQL92 标准的数据库。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171530038.png" alt="image-20220804195402870" style="zoom:80%;" />

## 3、ShardingSphere-Proxy⭐

**中间件封装**

定位为透明化的`数据库代理端`，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。 目前提供 MySQL 和 PostgreSQL版本，它可以使用任何兼容 MySQL/PostgreSQL 协议的访问客户端（如：MySQL Command Client, MySQL Workbench, Navicat 等）操作数据，对 DBA 更加友好。

向应用程序完全透明，可直接当做 MySQL/PostgreSQL 使用。

适用于任何兼容 MySQL/PostgreSQL 协议的的客户端。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171531841.png" alt="image-20220917153125771" style="zoom:80%;" />



## 4、核心概念⭐

在开始 `Sharding-JDBC`分库分表具体实战之前，我们有必要先了解分库分表的一些核心概念。

### 1、分库分表

> 分库，显而易见，就是一个数据库分成多个数据库，部署到不同机器。
>
> 分表，就是一个数据库表分成多个表。

### 2、分片

一般我们在提到分库分表的时候，大多是以水平切分模式（水平分库、分表）为基础来说的，数据分片将原本一张数据量较大的表 `t_order` 拆分生成数个表结构完全一致的小数据量表 `t_order_0`、`t_order_1`、···、`t_order_n`，每张表只存储原大表中的一部分数据，当执行一条`SQL`时会通过 `分库策略`、`分片策略` 将数据分散到不同的数据库、表内。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.11/202206051559869.png" alt="image-20220605155934811" style="zoom:67%;" />

### 3、数据节点

数据节点是分库分表中一个不可再分的最小数据单元（表），它由数据源名称和数据表组成，例如上图中 `order_db_1.t_order_0`、`order_db_2.t_order_1` 就表示一个数据节点。

### 4、逻辑表

逻辑表是指一组具有相同逻辑和数据结构表的总称。比如我们将订单表`t_order` 拆分成 `t_order_0` ···  `t_order_9` 等 10张表。此时我们会发现分库分表以后数据库中已不在有 `t_order` 这张表，取而代之的是 `t_order_n`，但我们在代码中写 `SQL` 依然按 `t_order` 来写。此时 `t_order` 就是这些拆分表的`逻辑表`。

### 5、真实表

真实表也就是上边提到的 `t_order_n` 数据库中真实存在的物理表。

### 6、分片键

用于分片的数据库字段。我们将 `t_order` 表分片以后，当执行一条SQL时，通过对字段 `order_id` 取模的方式来决定，这条数据该在哪个数据库中的哪个表中执行，此时 `order_id` 字段就是 `t_order` 表的分片健。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.11/202206051600597.png" alt="image-20220605160002550" style="zoom:67%;" />

这样以来同一个订单的相关数据就会存在同一个数据库表中，大幅提升数据检索的性能，不仅如此 `sharding-jdbc` 还支持根据多个字段作为分片健进行分片。

## 5、分片算法

上边我们提到可以用分片健取模的规则分片，但这只是比较简单的一种，在实际开发中我们还希望用 `>=`、`<=`、`>`、`<`、`BETWEEN` 和 `IN` 等条件作为分片规则，自定义分片逻辑，这时就需要用到分片策略与分片算法。

从执行 SQL 的角度来看，分库分表可以看作是一种路由机制，把 SQL 语句路由到我们期望的数据库或数据表中并获取数据，分片算法可以理解成一种路由规则。

咱们先捋一下它们之间的关系，分片策略只是抽象出的概念，它是由分片算法和分片健组合而成，分片算法做具体的数据分片逻辑。

> 分库、分表的分片策略配置是相对独立的，可以各自使用不同的策略与算法，每种策略中可以是多个分片算法的组合，每个分片算法可以对多个分片健做逻辑判断。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.11/202206051600013.png" alt="image-20220605160018974" style="zoom:80%;" />

> **注意**：sharding-jdbc 并没有直接提供分片算法的实现，需要开发者根据业务自行实现。

`sharding-jdbc` 提供了4种分片算法：

### 1、精确分片算法

精确分片算法（PreciseShardingAlgorithm）用于单个字段作为分片键，SQL中有 `=` 与 `IN` 等条件的分片，需要在标准分片策略（`StandardShardingStrategy` ）下使用。

### 2、范围分片算法

范围分片算法（RangeShardingAlgorithm）用于单个字段作为分片键，SQL中有 `BETWEEN AND`、`>`、`<`、`>=`、`<=` 等条件的分片，需要在标准分片策略（`StandardShardingStrategy` ）下使用。

### 3、复合分片算法

复合分片算法（ComplexKeysShardingAlgorithm）用于多个字段作为分片键的分片操作，同时获取到多个分片健的值，根据多个字段处理业务逻辑。需要在复合分片策略（`ComplexShardingStrategy` ）下使用。

### 4、Hint分片算法

Hint分片算法（HintShardingAlgorithm）稍有不同，上边的算法中我们都是解析`SQL` 语句提取分片键，并设置分片策略进行分片。但有些时候我们并没有使用任何的分片键和分片策略，可还想将 SQL 路由到目标数据库和表，就需要通过手动干预指定SQL的目标数据库和表信息，这也叫强制路由。

## 6、分片策略

上边讲分片算法的时候已经说过，分片策略是一种抽象的概念，实际分片操作的是由分片算法和分片健来完成的。

### 1、标准分片策略

标准分片策略适用于单分片键，此策略支持 `PreciseShardingAlgorithm` 和 `RangeShardingAlgorithm` 两个分片算法。

其中 `PreciseShardingAlgorithm` 是必选的，用于处理 `=` 和 `IN` 的分片。`RangeShardingAlgorithm` 是可选的，用于处理`BETWEEN AND`， `>`， `<`，`>=`，`<=` 条件分片，如果不配置`RangeShardingAlgorithm`，SQL中的条件等将按照全库路由处理。

### 2、复合分片策略

复合分片策略，同样支持对 SQL语句中的 `=`，`>`， `<`， `>=`， `<=`，`IN`和 `BETWEEN AND` 的分片操作。不同的是它支持多分片键，具体分配片细节完全由应用开发者实现。

### 3、行表达式分片策略

行表达式分片策略，支持对 SQL语句中的 `=` 和 `IN` 的分片操作，但只支持单分片键。这种策略通常用于简单的分片，不需要自定义分片算法，可以直接在配置文件中接着写规则。

`t_order_$->{t_order_id % 4}` 代表 `t_order` 对其字段 `t_order_id`取模，拆分成4张表，而表名分别是`t_order_0` 到 `t_order_3`。

### 4、Hint分片策略

Hint分片策略，对应上边的Hint分片算法，通过指定分片健而非从 `SQL`中提取分片健的方式进行分片的策略。

## 7、分布式主键

数据分⽚后，不同数据节点⽣成全局唯⼀主键是⾮常棘⼿的问题，同⼀个逻辑表（`t_order`）内的不同真实表（`t_order_n`）之间的⾃增键由于⽆法互相感知而产⽣重复主键。

尽管可通过设置⾃增主键 `初始值` 和 `步⻓` 的⽅式避免ID碰撞，但这样会使维护成本加大，乏完整性和可扩展性。如果后去需要增加分片表的数量，要逐一修改分片表的步长，运维成本非常高，所以不建议这种方式。

实现分布式主键⽣成器的方式很多，可以参考我之前写的《[9种分布式ID生成方式](https://mp.weixin.qq.com/s?__biz=MjM5NTY1MjY0MQ==&mid=2650762665&idx=5&sn=783b170510733f03d72d8ce83d24517a&token=490072482&lang=zh_CN&scene=21#wechat_redirect)》。

为了让上手更加简单，ApacheShardingSphere 内置了`UUID`、`SNOWFLAKE` 两种分布式主键⽣成器，默认使⽤雪花算法（`snowflake`）⽣成64bit的⻓整型数据。不仅如此它还抽离出分布式主键⽣成器的接口，⽅便我们实现⾃定义的⾃增主键⽣成算法。

## 8、广播表

广播表：存在于所有的分片数据源中的表，表结构和表中的数据在每个数据库中均完全一致。一般是为字典表或者配置表 `t_config`，某个表一旦被配置为广播表，只要修改某个数据库的广播表，所有数据源中广播表的数据都会跟着同步。

## 9、绑定表

绑定表：那些分片规则一致的主表和子表。比如：`t_order` 订单表和 `t_order_item` 订单服务项目表，都是按 `order_id` 字段分片，因此两张表互为绑定表关系。

那绑定表存在的意义是啥呢？

通常在我们的业务中都会使用 `t_order` 和 `t_order_item` 等表进行多表联合查询，但由于分库分表以后这些表被拆分成N多个子表。如果不配置绑定表关系，会出现笛卡尔积关联查询，将产生如下四条`SQL`。

```sql
SELECT * FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id=i.order_id 
SELECT * FROM t_order_0 o JOIN t_order_item_1 i ON o.order_id=i.order_id 
SELECT * FROM t_order_1 o JOIN t_order_item_0 i ON o.order_id=i.order_id 
SELECT * FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id=i.order_id 
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.11/202206051600349.png" alt="image-20220605160041310" style="zoom:80%;" />

而配置绑定表关系后再进行关联查询时，只要对应表分片规则一致产生的数据就会落到同一个库中，那么只需 `t_order_0` 和 `t_order_item_0` 表关联即可。

```sql
SELECT * FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id=i.order_id 
SELECT * FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id=i.order_id 
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.11/202206051600958.png" alt="image-20220605160052921" style="zoom:80%;" />

> **注意**：在关联查询时 `t_order` 它作为整个联合查询的主表。所有相关的路由计算都只使用主表的策略，`t_order_item` 表的分片相关的计算也会使用 `t_order` 的条件，所以要保证绑定表之间的分片键要完全相同。



# 数据异构方案

何谓数据异构，上周交易部门商品的同事过来做分享，又看到这个词，他的PPT里面是 数据库异构。其实我们以前做的事情，也是可以称之为数据异构。比如我们将DB里面的数据持久化到Redis里面去，就是一种数据异构的方式。

如果要下个定义的话：**把数据按需（数据结构、存取方式、存取形式）异地构建存储。**

## 常见应用场景

分库分表中有一个最为常见的场景，为了提升数据库的查询能力，我们都会对数据库做分库分表操作。比如订单库，开始的时候我们是按照订单ID维度去分库分表，那么后来的业务需求想按照商家维度去查询，比如我想查询某一个商家下的所有订单，就非常麻烦。

这个时候通过数据异构就能很好的解决此问题，如下图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210292033284.png" alt="image-20221029203317187" style="zoom:80%;" />

**数据异构总结起来大概有以下几种场景**

1. 数据库镜像
2. 数据库实时备份
3. 多级索引
4. search build（比如分库分表后的多维度数据查询）
5. 业务cache刷新
6. 价格、库存变化等重要业务消息

## 数据异构方向

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210292051803.png" alt="image-20221029205142744" style="zoom:80%;" />

在日常业务开发中大致可以分为以上几种数据去向，**DB-DB**这种方式，一般常见于分库分表后，聚合查询的时候，比如我们按照订单ID去分库分表，那么这个时候我们要按照用户ID去查询，查询这个用户下面的订单就非常不方便了，当然可以使用统一加到内存中去，但这样不太好。

所以我们就可以用数据库异构的方式，重新按照用户ID的维度来分一个表，像在上面常见应用场景中介绍的那样。把数据异构到redis、elasticserach、slor中去要解决的问题跟按照多维度来查询的需求差不多。这些存储天生都有聚合的功能。当然同时也可以提高查询性能，应对大访问量，比如redis这种抗量银弹。

## 数据异构的常用方法

### 1. 完整克隆

这个很简单就是将数据库A，全部拷贝一份到数据库B，这样的使用场景是离线统计跑任务脚本的时候可以。缺点也很突出，不适用于持续增长的数据。

### 2. 标记同步

这个是业务场景比较简单的时候，理想情况下数据不会发生改变，比如日志数据，这个时候可以去标记，比如时间戳，这样当发生故障的时候还可以回溯到上一次同步点，开始重新同步数据。

### 3. binlog方式

通过实时的订阅MySQL的binlog日志，消费到这些日志后，重新构建数据结构插入一个新的数据库或者是其他存储比如es、slor等等。订阅binlog日志可以比较好的能保证数据的一致性。

### 4. MQ方式

业务数据写入DB的同时，也发送MQ一份，也就是业务里面实现双写。这种方式比较简单，但也很难保证数据一致性，对简单的业务场景可以采用这种方式。

## binlog方式

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210292054713.png" alt="image-20221029205423641" style="zoom:80%;" />

binglog是数据的日志记录方式，每次对数据的操作都会有binlog日志。现在开源的订阅binlog日志的组件，比如使用比较广泛的canal，它是阿里开源的基于mysql数据库binlog的增量订阅和消费组件。

由于cannal服务器目前读取的binlog事件只保存在内存中，并且只有一个canal客户端可以进行消费。所以如果需要多个消费客户端，可以引入activemq或者kafka。如上图绿色虚线框部分。

我们还需要确保全量对比来保证数据的一致性（canal+mq的重试机制基本可以保证写入异构库之后的数据一致性），这个时候可以有一个全量同步WORKER程序来保证，如上图深绿色部分。

### canal的工作原理

先来看下mysql主备（主从）复制原理如下图，在此原理基础之上我们再来理解canal的实现原理就一眼能明白了。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210292054721.png" alt="image-20221029205458647" style="zoom:80%;" />

**mysql主备（主从）复制原理，从上层来看，复制分成三步：**

1. master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events，可以通过show binlog events进行查看）；
2. slave将master的binary log events拷贝到它的中继日志(relay log)；
3. slave重做中继日志中的事件，将改变反映它自己的数据。

再来看下canal的原理，如下图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210292055007.png" alt="image-20221029205516928" style="zoom:80%;" />

**cannal实现原理相对比较简单（参照上面的mysql主备复制实现原理）：**

1. canal模拟mysql slave的交互协议，伪装自己为mysql slave，向mysql master发送dump协议
2. mysql master收到dump请求，开始推送binary log给slave(也就是canal)
3. canal解析binary log对象(原始为byte流)

我们在部署canal server的时候要部署多台，来保证高可用。但是canal的原理，是只有一台服务器在跑处理，其它的服务器作为热备。canal server的高可用是通过zookeeper来维护的。

**有关canal更具体的使用和详细原理请参照：https://github.com/alibaba/canal**

### 注意点

- 确认MySQL开启binlog，使用**show variables like 'log_bin';** 查看ON为已开启
- 确认目标库可以产生binlog，**show master status** 注意Binlog_Do_DB，Binlog_Ignore_DB参数
- 确认binlog格式为ROW，使用**show variables like 'binlog_format';** 非ROW模式登录MySQL执行 **set global binlog_format=ROW; flush logs;** 或者通过更改MySQL配置文件并重启MySQL生效。
- 为保证binlake服务可以获取Binlog，需添加授权，执行 **GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON \*.\* TO 'admin'@'%' identified by 'admin'; FLUSH PRIVILEGES;**

## MQ方式

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210292055583.png" alt="image-20221029205536514" style="zoom:80%;" />

mq的方式，就相对简单，实际上是在业务逻辑中写DB的同时去写一次MQ，但是这种方式不能够保证数据一致性，就是不能保证跨资源的事务。**注：调用第三方远程RPC的操作一定不要放到事务中。**

## 总结

本文主要叙述了数据异构的使用场景，方法。这里面涉及到的activemq以及canal并没有深入分析，关于这块的内容可以直接参考相关具体文档，文中已给了链接地址。

根据数据异构的定义，将数据异地构建存储，我们可以应用的地方就非常多，文中说的分库分表之后按照其它维度来查询的时候，我们想脱离DB直接用缓存比如redis来抗量的时候。数据异构这种方式都能够很好的帮助我们来解决诸如此类的问题。



# 查询分离方案

在之前一篇文章中提到过对于业务主表读写缓慢的解决方案：**冷热分离**。冷热分离固然是一个性价比高的解决方案，但也并不是银弹，仍然有诸多限制，比如：

1. 查询冷数据慢
2. 业务无法修改冷数据
3. 冷数据多到一定程度系统依旧扛不住

此时如果需要解决以上问题，可以采用另外一种方案：使用 **查询分离** 优化业务主表数据大查询缓慢的问题

本篇文章介绍了表数据量大查询缓慢的一种解决方案：查询分离，但这也不是银弹，仍然是存在一些不足，比如表数据量大，写入缓慢怎么办？当然查询分离还有一个重要的问题：历史数据如何迁移？这个处理也是非常简单，但是也有许多需要考虑的点。

## 什么是查询分离？

查询分离从字面上来说非常容易理解，其实就是在写数据时保存一个备份数据到另外的存储系统，在查询时直接从另外的存储系统中获取数据，如下图：

> Tip：查询分离和读写分离是有区别的，读写分离数据库类型是相同的，比如都是MySQL库。读写分离是通过数据库的主从复制的方式来同步数据，通过让主数据库负责事务性的增删改，而从数据库负责非事务性的查询操作来提升数据库的并发负载能力。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210291918603.png" alt="image-20221029191821504" style="zoom: 50%;" />

以上只是简单的架构图，其中有些细节还是需要深究，如下：

1. 什么时候触发查询分离？
2. 如何实现查询分离？
3. 查询数据的存储系统选型？
4. 查询数据如何使用？

## 查询分离的适用场景

当你在实际业务中遇到以下情形，则可以考虑使用查询分离解决方案。

- 数据量大，单表数据达到千万；
- 查询速度慢，即使加了索引也是很慢；
- 存在复杂的表关联查询；
- 所有写数据的请求效率尚可；
- 查询数据的请求效率很低；
- 所有的数据任何时候都可能被修改；
- 业务希望我们优化查询数据的功能。

曾做过 SaaS 客服系统的架构优化，系统里有一个工单查询功能，工单表中存放了几千万条数据，且查询工单表数据时需要关联十几个子表，每个子表的数据也是超亿条。

面对如此庞大的数据量，跟前面的冷热分离一样，每次客户查询数据时几十秒才能返回结果，即便我们使用了索引、SQL 等数据库优化技巧，效果依然不明显。

工单表中有些数据是几年前的，客户说这些数据涉及诉讼问题，需要继续保持更新，因此我们无法将这些旧数据封存到别的地方，也就没法通过前面的冷热分离方案来解决。

最终我们采用了查询分离的解决方案，才得以将这个问题顺利解决：将更新的数据放在一个数据库里，而查询的数据放在另外一个系统里。因为数据的更新都是单表更新，不需要关联也没有外键，所以更新速度立马得到提升，每次客户查询数据时，500ms 内就可得到返回结果。

## 什么时候触发查询分离

简单的来说就是什么时候应该保存一份数据到查询数据库中，其实也就是数据异构的过程，详细文章可以看我前面一篇文章：[数据异构就该这样做，yyds~](https://mp.weixin.qq.com/s?__biz=MzU3MDAzNDg1MA==&mid=2247518212&idx=1&sn=5311d102f977427ca4919dedbb37f94b&chksm=fcf757c9cb80dedf0ce7c0ccdc7d177c22614e0d27a85ee1bd782523552e15b1dfc846934fd8&token=238513143&lang=zh_CN&scene=21#wechat_redirect)

这里介绍三种方式，如下：

1. 同步建立
2. 异步建立
3. binlog方式

### 1、 同步建立

修改业务代码：在写入常规数据后，同步建立查询数据。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210291921151.png" alt="image-20221029192145040" style="zoom:50%;" />

该种方案优缺点也非常明显：

**优点**：查询数据的一致性和实时性得到了保证

**缺点**：业务代码侵入比较强；减缓写操作的效率

### 2、 异步建立

修改业务代码：写入数据后，异步建立查询数据

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210291922361.png" alt="image-20221029192207247" style="zoom:50%;" />

该种方案的优缺点如下：

**优点**：不影响主流程

**缺点**：数据一致性存在问题

### 3、 binlog的方式

该种方案也是业界常用的一种方案，对于代码是无侵入的，通过监听数据库日志的方式建立查询数据，如下：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202210291922038.png" alt="image-20221029192252921" style="zoom:50%;" />

该种方案的优缺点如下：

**优点**：不影响主流程；代码侵入为0

**缺点**：数据一致性存在问题；架构相对复杂

## 如何实现查询分离

对于上述三种方案都算是比较常见的方案，对于第一种同步的方式比较简单，这里不再介绍；对于第三种binlog的方式在数据异构的文章中介绍过，详情见：[数据异构就该这样做，yyds~](https://mp.weixin.qq.com/s?__biz=MzU3MDAzNDg1MA==&mid=2247518212&idx=1&sn=5311d102f977427ca4919dedbb37f94b&chksm=fcf757c9cb80dedf0ce7c0ccdc7d177c22614e0d27a85ee1bd782523552e15b1dfc846934fd8&token=238513143&lang=zh_CN&scene=21#wechat_redirect)

这篇文章来介绍一下异步的方式，异步的方式有很多，可以放在内存中进行操作，但是这有些弊端：

1. 数据过多，内存有限
2. 服务重启，内存数据将会丢失

因此最终我们可以选择**MQ**的方式，那么此时就涉及到了MQ的技术选型，这里给两个建议：

1. 如果你的公司已经用了MQ，那么直接接着用即可
2. 如果公司目前未引入MQ，则需要架构组考量选型了

当然一旦引入了MQ还需要考虑的问题很多，如下：

### 1、 MQ突然宕机了怎么办？

MQ宕机意味着查询数据不能继续建立了，我们可以在写入数据的同时给该条数据加一个标志字段（已搬运、未搬运），当MQ启动后，查询所有未搬运的数据，继续建立查询数据

> 这里的方案很多，按照业务实际情况考量

### 2、消息的幂等消费

消息的幂等消费一定要保证，避免数据重复建立，比如：主数据的订单 A 更新后，我们在查询数据中插入了 A，可是此时系统出问题了，系统误以为查询数据没更新，又把订单 A 插入更新了一次。

### 3、消息的时序性问题

比如某个订单 A 更新了 1 次数据变成 A1，线程甲将 A1 的数据搬到查询数据中。不一会儿，后台订单 A 又更新了 1 次数据变成 A2，线程乙也启动工作，将 A2 的数据搬到查询数据中。

所谓的时序性就是如果线程甲启动比乙早，但搬运数据动作比线程乙还晚完成，就有可能出现查询数据最终变成过期的 A1

## 查询数据的存储系统选型？

既然为了解决表数据量大查询缓慢的问题，肯定是不能选用关系型数据库了，那么还有其他选择吗？

内存数据库虽然性能非常高，比如Redis，但是不适合海量数据，太费钱了

那么这里比较适用的有如下三种：

1. MongoDB
2. HBase
3. Elasticsearch

这里选型还是要根据自己公司业务选择，如果已经有在用的，则直接用即可；另外就是选择自己熟悉的，比如当初我们设计架构方案时，为什么选择用 Elasticsearch，除 ES 对查询的扩展性支持外，最关键的一点是我们团队对 Elasticsearch 很熟悉。

## 查询数据如何使用？

查询数据很简单，每个数据库都有对应的API，直接调用查询

但是，这里有一个问题：**数据查询更新完前，查询数据不一致怎么办？**，给出两种方案：

在查询数据更新到最新前，不允许用户查询。（我们没用过这种设计，但我确实见过市面上有这样的设计。）

给用户提示：您目前查询到的数据可能是 1 秒前的数据，如果发现数据不准确，可以尝试刷新一下，这种提示用户一般比较容易接受。



# 高性能架构模式⭐⭐

互联网业务兴起之后，海量用户加上海量数据的特点，单个数据库服务器已经难以满足业务需要，必须考虑数据库集群的方式来提升性能。高性能数据库集群的`第一种方式是“读写分离”`，`第二种方式是“数据库分片”`。

## 1、读写分离架构

**读写分离原理：**读写分离的基本原理是将数据库读写操作分散到不同的节点上，下面是其基本架构图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804217.jpg" alt="img" style="zoom:80%;" />

**读写分离的基本实现**

> -  主库负责处理事务性的增删改操作，从库负责处理查询操作，能够有效的避免由数据更新导致的行锁，使得整个系统的查询性能得到极大的改善。
> -  读写分离是根据 SQL 语义的分析，将读操作和写操作分别路由至主库与从库。
> -  通过`一主多从`的配置方式，可以将查询请求均匀的分散到多个数据副本，能够进一步的提升系统的处理能力。 
> -  使用`多主多从`的方式，不但能够提升系统的吞吐量，还能够提升系统的可用性，可以达到在任何一个数据库宕机，甚至磁盘物理损坏的情况下仍然不影响系统的正常运行。

**下图展示了根据业务需要，将用户表的写操作和读操路由到不同的数据库的方案：**

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804223.png" alt="image-20220804223138651" style="zoom:80%;" />

## 2、CAP 理论

CAP 定理（CAP theorem）又被称作布鲁尔定理（Brewer's theorem），是加州大学伯克利分校的计算机科学家埃里克·布鲁尔（Eric Brewer）在 2000 年的 ACM PODC 上提出的一个猜想。对于设计分布式系统的架构师来说，CAP 是必须掌握的理论。

### 1 CAP分析

在一个`分布式系统中`，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三者中的两个，另外一个必须被牺牲。

> - C 一致性（Consistency）：对某个指定的客户端来说，读操作保证能够返回最新的写操作结果
> - A 可用性（Availability）：非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）
> - P 分区容忍性（Partition Tolerance）：当出现网络分区后（可能是丢包，也可能是连接中断，还可能是拥塞），系统能够继续“履行职责”

### 2 CAP特点

> 在实际设计过程中，每个系统不可能只处理一种数据，而是包含多种类型的数据，有的数据必须选择 CP，有的数据必须选择 AP，分布式系统理论上不可能选择 CA 架构。

> CP：如下图所示，为了保证一致性，当发生分区现象后，N1 节点上的数据已经更新到 y，但由于 N1 和 N2 之间的复制通道中断，数据 y 无法同步到 N2，N2 节点上的数据还是 x。这时客户端 C 访问 N2 时，N2 需要返回 Error，提示客户端 C“系统现在发生了错误”，这种处理方式`违背了可用性（Availability）的要求，因此 CAP 三者只能满足 CP。

> AP：如下图所示，为了保证可用性，当发生分区现象后，N1 节点上的数据已经更新到 y，但由于 N1 和 N2 之间的复制通道中断，数据 y 无法同步到 N2，N2 节点上的数据还是 x。这时客户端 C 访问 N2 时，N2 将当前自己拥有的数据 x 返回给客户端 C 了，而实际上当前最新的数据已经是 y 了，这就不满足一致性（Consistency）的要求了，因此 CAP 三者只能满足 AP。

> 注意：这里 N2 节点返回 x，虽然不是一个“正确”的结果，但是一个“合理”的结果，因为 x 是旧的数据，并不是一个错乱的值，只是不是最新的数据而已。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251611935.png" alt="img" style="zoom:80%;" />



CAP 理论中的 `C 在实践中是不可能完美实现的`，在数据复制的过程中，节点N1 和节点 N2 的数据并不一致（强一致性）。即使无法做到`强一致性`，但应用可以采用适合的方式达到`最终一致性`。具有如下特点：

> - 基本可用（Basically Available）：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。
> - 软状态（Soft State）：允许系统存在中间状态，而该中间状态不会影响系统整体可用性。这里的中间状态就是  CAP 理论中的数据不一致。
> - 最终一致性（Eventual Consistency）：系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

## 3、数据库分片架构

**读写分离的问题**

> 读写分离分散了数据库读写操作的压力，但没有分散存储压力，为了满足业务数据存储的需求，就需要将存储分散到多台数据库服务器上。
>

**数据分片**

> 将存放在单一数据库中的数据分散地存放至多个数据库或表中，以达到提升性能瓶颈以及可用性的效果。 数据分片的有效手段是对关系型数据库进行分库和分表。数据分片的拆分方式又分为垂直分片和水平分片。
>

### 1、垂直分片

**垂直分库**

> 按照业务拆分的方式称为垂直分片，又称为纵向拆分，它的核心理念是**专库专用**。 在拆分之前，一个数据库由多个数据表构成，每个表对应着不同的业务。而拆分之后，则是按照业务将表进行归类，分布到不同的数据库中，从而将压力分散至不同的数据库。 
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804228.jpg" alt="img" style="zoom: 25%;" />

下图展示了根据业务需要，将用户表和订单表垂直分片到不同的数据库的方案：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804233.png" alt="image-20220804221855449" style="zoom: 80%;" />

> 垂直拆分可以缓解数据量和访问量带来的问题，但无法根治。如果垂直拆分之后，表中的数据量依然超过单节点所能承载的阈值，则需要水平分片来进一步处理。
>

**垂直分表**

垂直分表适合将表中某些不常用的列，或者是占了大量空间的列拆分出去。

假设我们是一个婚恋网站，用户在筛选其他用户的时候，主要是用 age 和 sex 两个字段进行查询，而 nickname 和 description 两个字段主要用于展示，一般不会在业务查询中用到。description 本身又比较长，因此我们可以将这两个字段独立到另外一张表中，这样在查询 age 和 sex 时，就能带来一定的性能提升。

垂直分表引入的复杂性主要体现在表操作的数量要增加。例如，原来只要一次查询就可以获取 name、age、sex、nickname、description，现在需要两次查询，一次查询获取 name、age、sex，另外一次查询获取 nickname、description。

![img](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804225.jpg)

水平分表适合表行数特别大的表，水平分表属于水平分片。

### 2、水平分片

> 水平分片又称为横向拆分。相对于垂直分片，它不再将数据根据业务逻辑分类，而是通过某个字段（或某几个字段），根据某种规则将数据分散至多个库或表中，每个分片仅包含数据的一部分。 例如：根据主键分片，偶数主键的记录放入 0 库（或表），奇数主键的记录放入 1 库（或表），如下图所示。
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804240.png" alt="image-20220804222212087" style="zoom:80%;" />

单表进行切分后，是否将多个表分散在不同的数据库服务器中，可以根据实际的切分效果来确定。

> - **水平分表：**单表切分为多表后，新的表即使在同一个数据库服务器中，也可能带来可观的性能提升，如果性能能够满足业务要求，可以不拆分到多台数据库服务器，毕竟业务分库也会引入很多复杂性；
>
> - **水平分库：**如果单表拆分为多表后，单台服务器依然无法满足性能要求，那就需要将多个表分散在不同的数据库服务器中。

> **阿里巴巴Java开发手册**
>
> 【推荐】单表行数超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表。
>
> 说明：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。

## 4、读写分离和数据分片架构

 下图展现了将数据分片与读写分离一同使用时，应用程序与数据库集群之间的复杂拓扑关系。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804257.png" alt="image-20220804223321167" style="zoom:80%;" />

## 5、实现方式

读写分离和数据分片具体的实现方式一般有两种：  `程序代码封装`和`中间件封装`。

### 1、程序代码封装

程序代码封装指在代码中抽象一个`数据访问层（或中间层封装）`，实现读写操作分离和数据库服务器连接的管理。

**其基本架构是：**以读写分离为例

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804350.jpg" alt="img" style="zoom: 67%;" />

### 2、中间件封装

中间件封装指的是`独立一套系统出来`，实现读写操作分离和数据库服务器连接的管理。对于业务服务器来说，访问中间件和访问数据库没有区别，在业务服务器看来，中间件就是一个数据库服务器。

**基本架构是：**以读写分离为例

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804997.jpg" alt="img" style="zoom:80%;" />

### 3、常用解决方案

> Apache ShardingSphere（程序级别和中间件级别）
>
> MyCat（数据库中间件） 



# ShardingSphere

## 1、简介

官网：https://shardingsphere.apache.org/index_zh.html

文档：https://shardingsphere.apache.org/document/5.1.1/cn/overview/

Apache ShardingSphere 由 JDBC、Proxy 和 Sidecar（规划中）这 3 款既能够独立部署，又支持混合部署配合使用

## 2、ShardingSphere-JDBC

**程序代码封装**

> 定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。 它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804017.png" alt="image-20220804195402870" style="zoom:80%;" />

## 3、ShardingSphere-Proxy

**中间件封装**

定位为透明化的`数据库代理端`，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。 目前提供 MySQL 和 PostgreSQL版本，它可以使用任何兼容 MySQL/PostgreSQL 协议的访问客户端（如：MySQL Command Client, MySQL Workbench, Navicat 等）操作数据，对 DBA 更加友好。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804029.png" alt="image-20220804195432673" style="zoom:80%;" />

# MySQL主从同步⭐

## 1、MySQL主从同步原理

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804041.png" alt="img" style="zoom:80%;" />

**基本原理：**slave会从master读取binlog来进行数据同步

**具体步骤：**

- `step1：`master将数据改变记录到`二进制日志（binary log）`中。
- `step2：` 当slave上执行 `start slave` 命令之后，slave会创建一个 `IO 线程`用来连接master，请求master中的binlog。
- `step3：`当slave连接master时，master会创建一个 `log dump 线程`，用于发送 binlog 的内容。在读取 binlog 的内容的操作中，会对主节点上的 binlog 加锁，当读取完成并发送给从服务器后解锁。
- `step4：`IO 线程接收主节点 binlog dump 进程发来的更新之后，保存到 `中继日志（relay log）` 中。
- `step5：`slave的`SQL线程`，读取relay log日志，并解析成具体操作，从而实现主从操作一致，最终数据一致。

## 2、一主多从配置

服务器规划：使用`docker`方式创建，`主从服务器IP一致，端口号不一致`

![image-20220807183231101](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804054.png)

- 主服务器：容器名`atguigu-mysql-master`，端口`3307`
- 从服务器：容器名`atguigu-mysql-slave1`，端口`3308`
- 从服务器：容器名`atguigu-mysql-slave2`，端口`3309`

**注意：**如果此时防火墙是开启的，`则先关闭防火墙，并重启docker`，否则后续安装的MySQL无法启动

```shell
#关闭docker
systemctl stop docker
#关闭防火墙
systemctl stop firewalld
#启动docker
systemctl start docker
```



### 1、准备主服务器

**step1：在docker中创建并启动MySQL主服务器：**`端口3307`

```shell
docker run -d \
-p 3307:3306 \
-v /atguigu/mysql/master/conf:/etc/mysql/conf.d \
-v /atguigu/mysql/master/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=123456 \
--name atguigu-mysql-master \
mysql:8.0.29
```

**step2：创建MySQL主服务器配置文件：** 

默认情况下MySQL的binlog日志是自动开启的，可以通过如下配置定义一些可选配置

```shell
vim /atguigu/mysql/master/conf/my.cnf
```

配置如下内容

```sh
[mysqld]
# 服务器唯一id，默认值1
server-id=1
# 设置日志格式，默认值ROW
binlog_format=STATEMENT
# 二进制日志名，默认binlog
# log-bin=binlog
# 设置需要复制的数据库，默认复制全部数据库
#binlog-do-db=mytestdb
# 设置不需要复制的数据库
#binlog-ignore-db=mysql
#binlog-ignore-db=infomation_schema
```

重启MySQL容器

```shell
docker restart atguigu-mysql-master
```

binlog格式说明

- binlog_format=STATEMENT：日志记录的是主机数据库的`写指令`，性能高，但是now()之类的函数以及获取系统参数的操作会出现主从数据不同步的问题。
- binlog_format=ROW（默认）：日志记录的是主机数据库的`写后的数据`，批量操作时性能较差，解决now()或者  user()或者  @@hostname 等操作在主从机器上不一致的问题。
- binlog_format=MIXED：是以上两种level的混合使用，有函数用ROW，没函数用STATEMENT，但是无法识别系统变量

`binlog-ignore-db和binlog-do-db的优先级问题：`

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804954.png" alt="img" style="zoom:80%;" />



**step3：使用命令行登录MySQL主服务器**

```shell
#进入容器：env LANG=C.UTF-8 避免容器中显示中文乱码(可以在命令行输入和显示中文)
docker exec -it atguigu-mysql-master env LANG=C.UTF-8 /bin/bash
#进入容器内的mysql命令行
mysql -uroot -p
#修改默认密码校验方式
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456';
```

**step4：主机中创建slave用户**

```sql
-- 创建slave用户
CREATE USER 'atguigu_slave'@'%';
-- 设置密码
ALTER USER 'atguigu_slave'@'%' IDENTIFIED WITH mysql_native_password BY '123456';
-- 授予复制权限
GRANT REPLICATION SLAVE ON *.* TO 'atguigu_slave'@'%';
-- 刷新权限
FLUSH PRIVILEGES;
```

**step5：主机中查询master状态：**

执行完此步骤后`不要再操作主服务器MYSQL`，防止主服务器状态值变化

```sql
SHOW MASTER STATUS;
```

记下`File`和`Position`的值。执行完此步骤后不要再操作主服务器MYSQL，防止主服务器状态值变化。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251655718.png" alt="image-20221125165510607" style="zoom:80%;" />

### 2、准备从服务器

可以配置多台从机slave1、slave2...，这里以配置slave1为例

**step1：在docker中创建并启动MySQL从服务器：**`端口3308，3309`

第一台从服务器

```shell
docker run -d \
-p 3308:3306 \
-v /atguigu/mysql/slave1/conf:/etc/mysql/conf.d \
-v /atguigu/mysql/slave1/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=123456 \
--name atguigu-mysql-slave1 \
mysql:8.0.29
```

第二台从服务器

```sh
docker run -d \
-p 3309:3306 \
-v /atguigu/mysql/slave2/conf:/etc/mysql/conf.d \
-v /atguigu/mysql/slave2/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=123456 \
--name atguigu-mysql-slave2 \
mysql:8.0.29
```

**step2：创建MySQL从服务器配置文件：** 

```shell
vim /atguigu/mysql/slave1/conf/my.cnf
```

配置如下内容：

```sh
[mysqld]
# 服务器唯一id，每台服务器的id必须不同，如果配置其他从机，注意修改id
server-id=2
# 中继日志名，默认xxxxxxxxxxxx-relay-bin
#relay-log=relay-bin
```

第二台服务器也是一样

```sh
vim /atguigu/mysql/slave2/conf/my.cnf
```

```sh
[mysqld]
# 服务器唯一id，每台服务器的id必须不同，如果配置其他从机，注意修改id
server-id=3
# 中继日志名，默认xxxxxxxxxxxx-relay-bin
#relay-log=relay-bin
```

重启MySQL容器

```shell
docker restart atguigu-mysql-slave1
docker restart atguigu-mysql-slave2
```

**step3：使用命令行登录MySQL从服务器：**

登录第一台服务器

```shell
#进入容器：
docker exec -it atguigu-mysql-slave1 env LANG=C.UTF-8 /bin/bash
#进入容器内的mysql命令行
mysql -uroot -p
#修改默认密码校验方式
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456';
```

登录第二台服务器

```sh
#进入容器：
docker exec -it atguigu-mysql-slave2 env LANG=C.UTF-8 /bin/bash
#进入容器内的mysql命令行
mysql -uroot -p
#修改默认密码校验方式
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456';
```



**step4：在从机上配置主从关系：**

在**所有从机**上执行以下SQL操作

```sql
CHANGE MASTER TO MASTER_HOST='192.168.22.150', 
MASTER_USER='atguigu_slave',MASTER_PASSWORD='123456', MASTER_PORT=3307,
MASTER_LOG_FILE='binlog.000003',MASTER_LOG_POS=1357; 
```

### 3、启动主从同步

启动从机的复制功能，执行SQL：

```sql
START SLAVE;
-- 查看状态（不需要分号）
SHOW SLAVE STATUS\G
```

**两个关键进程：**下面两个参数都是Yes，则说明主从配置成功！

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251707023.png" alt="image-20221125170701946" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251707653.png" alt="image-20221125170730540" style="zoom:80%;" />

### 4、实现主从同步

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251708793.png" alt="image-20221125170801714" style="zoom:80%;" />

在主机中执行以下SQL，在从机中查看数据库、表和数据是否已经被同步

```sql
CREATE DATABASE db_user;
USE db_user;
CREATE TABLE t_user (
 id BIGINT AUTO_INCREMENT,
 uname VARCHAR(30),
 PRIMARY KEY (id)
);
INSERT INTO t_user(uname) VALUES('zhang3');
INSERT INTO t_user(uname) VALUES(@@hostname);
```

从数据库1：发现数据已经成功同步

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251710005.png" alt="image-20221125171027868" style="zoom:80%;" />

从数据库2：发现数据已经成功同步

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251710247.png" alt="image-20221125171042125" style="zoom:80%;" />

### 5、停止和重置

需要的时候，可以使用如下SQL语句

```sql
-- 在从机上执行。功能说明：停止I/O 线程和SQL线程的操作。
stop slave; 

-- 在从机上执行。功能说明：用于删除SLAVE数据库的relaylog日志文件，并重新启用新的relaylog文件。
reset slave;

-- 在主机上执行。功能说明：删除所有的binglog日志文件，并将日志索引文件清空，重新开始所有新的日志文件。
-- 用于第一次进行搭建主从库时，进行主库binlog初始化工作；
reset master;
```

### 6、常见问题

#### 问题1

启动主从同步后，常见错误是`Slave_IO_Running： No 或者 Connecting` 的情况，此时查看下方的 `Last_IO_ERROR`错误日志，根据日志中显示的错误信息在网上搜索解决方案即可

![img](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804989.png)

**典型的错误例如：**`Last_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'Client requested master to start replication from position > file size'`

**解决方案**

```sql
-- 在从机停止slave
SLAVE STOP;
-- 在主机查看mater状态
SHOW MASTER STATUS;
-- 在主机刷新日志
FLUSH LOGS;
-- 再次在主机查看mater状态（会发现File和Position发生了变化）
SHOW MASTER STATUS;
-- 修改从机连接主机的SQL，并重新连接即可
```

#### 问题2

启动docker容器后提示 `WARNING: IPv4 forwarding is disabled. Networking will not work.`

![img](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804005.png)

此错误，虽然不影响主从同步的搭建，但是如果想从远程客户端通过以下方式连接docker中的MySQL则没法连接

```shell
C:\Users\administrator>mysql -h 192.168.100.201 -P 3306 -u root -p
```

**解决方案：**

```shell
#修改配置文件：
vim /usr/lib/sysctl.d/00-system.conf
#追加
net.ipv4.ip_forward=1
#接着重启网络
systemctl restart network
```



# ShardingSphere-JDBC读写分离

## 1、创建SpringBoot程序

### 1、创建项目

> 项目类型：Spring Initializr
>
> SpringBoot脚手架：http://start.aliyun.com
>
> 项目名：sharding-jdbc-demo
>
> SpringBoot版本：2.4.1

### 2、添加依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <dependency>
        <groupId>org.apache.shardingsphere</groupId>
        <artifactId>shardingsphere-jdbc-core-spring-boot-starter</artifactId>
        <version>5.1.1</version>
    </dependency>

    <dependency>
        <groupId>mysql</groupId>
        <artifactId>mysql-connector-java</artifactId>
        <scope>runtime</scope>
    </dependency>

    <dependency>
        <groupId>com.baomidou</groupId>
        <artifactId>mybatis-plus-boot-starter</artifactId>
        <version>3.3.1</version>
    </dependency>

    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <optional>true</optional>
    </dependency>
    
</dependencies>
```

### 3、创建实体类

```java
@TableName("t_user")
@Data
public class User {
    @TableId(type = IdType.AUTO) // 使用数据库的主键自增策略
    private Long id;
    private String uname;
}
```

### 4、创建Mapper

```java
@Mapper
public interface UserMapper extends BaseMapper<User> {
}
```

### 5、配置读写分离

application.properties：

```properties
# 应用名称
spring.application.name=sharingTest
# 应用服务 WEB 访问端口
server.port=8080

# 开发环境设置
spring.profiles.active=dev
# 内存模式，单机模式，集群模式，上线要使用集群模式
spring.shardingsphere.mode.type=Memory

# 配置真实数据源
spring.shardingsphere.datasource.names=master,slave1,slave2

# 配置第 1 个数据源
spring.shardingsphere.datasource.master.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.master.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.master.jdbc-url=jdbc:mysql://192.168.22.150:3307/db_user
spring.shardingsphere.datasource.master.username=root
spring.shardingsphere.datasource.master.password=123456

# 配置第 2 个数据源
spring.shardingsphere.datasource.slave1.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.slave1.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.slave1.jdbc-url=jdbc:mysql://192.168.22.150:3308/db_user
spring.shardingsphere.datasource.slave1.username=root
spring.shardingsphere.datasource.slave1.password=123456

# 配置第 3 个数据源
spring.shardingsphere.datasource.slave2.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.slave2.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.slave2.jdbc-url=jdbc:mysql://192.168.22.150:3309/db_user
spring.shardingsphere.datasource.slave2.username=root
spring.shardingsphere.datasource.slave2.password=123456

# 读写分离类型，如: Static，Dynamic，myds是名字，自己起的
spring.shardingsphere.rules.readwrite-splitting.data-sources.myds.type=Static
# 写数据源名称
spring.shardingsphere.rules.readwrite-splitting.data-sources.myds.props.write-data-source-name=master
# 读数据源名称，多个从数据源用逗号分隔
spring.shardingsphere.rules.readwrite-splitting.data-sources.myds.props.read-data-source-names=slave1,slave2

# 负载均衡算法名称
spring.shardingsphere.rules.readwrite-splitting.data-sources.myds.load-balancer-name=alg_round

# 负载均衡算法配置
# 负载均衡算法类型
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_round.type=ROUND_ROBIN
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_random.type=RANDOM
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_weight.type=WEIGHT
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_weight.props.slave1=1
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_weight.props.slave2=2

# 打印SQL
spring.shardingsphere.props.sql-show=true
```



## 2、测试

### 1、读写分离测试

```java
@SpringBootTest
class ReadwriteTest {
    
    @Autowired
    private UserMapper userMapper;

    // 写入数据的测试
    @Test
    public void testInsert(){
        User user = new User();
        user.setUname("张三丰");
        userMapper.insert(user);
    }
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251745246.png" alt="image-20221125174524160" style="zoom:80%;" />

### 2、事务测试

为了保证主从库间的事务一致性，避免跨服务的分布式事务，ShardingSphere-JDBC的主从模型中，事务中的数据读写均用主库

>  * 不添加@Transactional：insert对主库操作，select对从库操作
>  * 添加@Transactional：则insert和select均对主库操作
>  * **注意：**在JUnit环境下的@Transactional注解，默认情况下就会对事务进行回滚（即使在没加注解@Rollback，也会对事务回滚）

```java
// 事务测试
@Transactional//开启事务
@Test
public void testTrans(){
    User user = new User();
    user.setUname("铁锤");
    userMapper.insert(user);
    List<User> users = userMapper.selectList(null);
    System.out.println(users);
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251935746.png" alt="image-20221125193529652" style="zoom:80%;" />

### 3、负载均衡测试

```java
// 读数据测试
@Test
public void testSelectAll(){
    List<User> users = userMapper.selectList(null);
    List<User> users1 = userMapper.selectList(null);//执行第二次测试负载均衡
    users.forEach(System.out::println);
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251938891.png" alt="image-20221125193825804" style="zoom:80%;" />

也可以在web请求中测试负载均衡

```java
package com.atguigu.shardingjdbcdemo.controller;

@RestController
@RequestMapping("/user")
public class UserController {

    @Autowired
    private UserMapper userMapper;

    /**
     * 测试负载均衡策略
     */
    @GetMapping("selectAll")
    public User selectAll(){
       return userMapper.selectList(null);
    }
}
```

## 3、负载均衡算法

```properties
# 负载均衡算法名称(配置要选择的负载均衡算法)
spring.shardingsphere.rules.readwrite-splitting.data-sources.myds
.load-balancer-name=alg_round

# 负载均衡算法类型
# 轮询
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_round.type=ROUND_ROBIN
# 随机
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_random.type=RANDOM
# 权重
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_weight.type=WEIGHT
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_weight.props.slave1=1
spring.shardingsphere.rules.readwrite-splitting.load-balancers.alg_weight.props.slave2=2
```



# ShardingSphere-JDBC垂直分片

## 1、准备服务器

服务器规划：使用`docker`方式创建如下容器

![image-20220807232456342](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211251956697.png)

- 服务器：容器名`server-user`，端口`3301`

- 服务器：容器名`server-order`，端口`3302`


### 1、创建server-user容器

**step1：创建容器**

```shell
docker run -d \
-p 3301:3306 \
-v /atguigu/server/user/conf:/etc/mysql/conf.d \
-v /atguigu/server/user/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=123456 \
--name server-user \
mysql:8.0.29
```

**step2：登录MySQL服务器**

```shell
#进入容器：
docker exec -it server-user env LANG=C.UTF-8 /bin/bash
#进入容器内的mysql命令行
mysql -uroot -p
#修改默认密码插件
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456';
```

**step3：创建数据库**

```sql
CREATE DATABASE db_user;
USE db_user;
CREATE TABLE t_user (
 id BIGINT AUTO_INCREMENT,
 uname VARCHAR(30),
 PRIMARY KEY (id)
);
```

### 2、创建server-order容器

**step1：创建容器**

```shell
docker run -d \
-p 3302:3306 \
-v /atguigu/server/order/conf:/etc/mysql/conf.d \
-v /atguigu/server/order/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=123456 \
--name server-order \
mysql:8.0.29
```

**step2：登录MySQL服务器**

```shell
#进入容器：
docker exec -it server-order env LANG=C.UTF-8 /bin/bash
#进入容器内的mysql命令行
mysql -uroot -p
#修改默认密码插件
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456';
```

**step3：创建数据库**

```sql
CREATE DATABASE db_order;
USE db_order;
CREATE TABLE t_order (
  id BIGINT AUTO_INCREMENT,
  order_no VARCHAR(30),
  user_id BIGINT,
  amount DECIMAL(10,2),
  PRIMARY KEY(id) 
);
```



## 2、程序实现

### 1、创建实体类

```java
@TableName("t_order")
@Data
public class Order {
    @TableId(type = IdType.AUTO)
    private Long id;
    private String orderNo;
    private Long userId;
    private BigDecimal amount;
}
```

```java
@TableName("t_user")
@Data
public class User {
    @TableId(type = IdType.AUTO)
    private Long id;
    private String uname;
}
```

### 2、创建Mapper

```java
import com.baomidou.mybatisplus.core.mapper.BaseMapper;
import com.it.entity.User;
import org.apache.ibatis.annotations.Mapper;

@Mapper
public interface OrderMapper extends BaseMapper<Order> {
}
```

```java
import com.baomidou.mybatisplus.core.mapper.BaseMapper;
import com.it.entity.User;
import org.apache.ibatis.annotations.Mapper;

@Mapper
public interface UserMapper extends BaseMapper<User> {
}
```

### 3、配置垂直分片

```properties
# 应用名称
spring.application.name=sharingTest
# 应用服务 WEB 访问端口
server.port=8080

# 环境设置
spring.profiles.active=dev

# 配置真实数据源
spring.shardingsphere.datasource.names=server-user,server-order

# 配置第 1 个数据源
spring.shardingsphere.datasource.server-user.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.server-user.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.server-user.jdbc-url=jdbc:mysql://192.168.22.150:3301/db_user
spring.shardingsphere.datasource.server-user.username=root
spring.shardingsphere.datasource.server-user.password=123456

# 配置第 2 个数据源
spring.shardingsphere.datasource.server-order.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.server-order.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.server-order.jdbc-url=jdbc:mysql://192.168.22.150:3302/db_order
spring.shardingsphere.datasource.server-order.username=root
spring.shardingsphere.datasource.server-order.password=123456

# 标准分片表配置（数据节点）
# spring.shardingsphere.rules.sharding.tables.<table-name>.actual-data-nodes=值
# 值由数据源名 + 表名组成，以小数点分隔。
# <table-name>：逻辑表名
spring.shardingsphere.rules.sharding.tables.t_user.actual-data-nodes=server-user.t_user
spring.shardingsphere.rules.sharding.tables.t_order.actual-data-nodes=server-order.t_order

# 打印SQL
spring.shardingsphere.props.sql-show=true
```



## 3、测试垂直分片

```java
@SpringBootTest
public class ShardingTest {


    @Autowired
    private UserMapper userMapper;

    @Autowired
    private OrderMapper orderMapper;

    /**
     * 垂直分片：插入数据测试
     */
    @Test
    public void testInsertOrderAndUser(){
        // 往User表插入数据
        User user = new User();
        user.setUname("强哥");
        userMapper.insert(user);
        // 往Order表插入数据
        Order order = new Order();
        order.setOrderNo("ATGUIGU001");
        order.setUserId(user.getId());
        order.setAmount(new BigDecimal(100));
        orderMapper.insert(order);
    }

    // 垂直分片：查询数据测试
    @Test
    public void testSelectFromOrderAndUser(){
        User user = userMapper.selectById(1L);
        Order order = orderMapper.selectById(1L);
        System.out.println(user);
        System.out.println(order);
    }
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181102065.png" alt="image-20220918110239947" style="zoom:80%;" />

# ShardingSphere-JDBC水平分片

## 1、准备服务器

服务器规划：使用`docker`方式创建如下容器

![image-20220808033239206](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211252010465.png)

- 服务器：容器名`server-order0`，端口`3310`

- 服务器：容器名`server-order1`，端口`3311`


### 1、创建server-order0容器

**step1：创建容器：**

```shell
docker run -d \
-p 3310:3306 \
-v /atguigu/server/order0/conf:/etc/mysql/conf.d \
-v /atguigu/server/order0/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=123456 \
--name server-order0 \
mysql:8.0.29
```

**step2：登录MySQL服务器：**

```shell
#进入容器：
docker exec -it server-order0 env LANG=C.UTF-8 /bin/bash
#进入容器内的mysql命令行
mysql -uroot -p
#修改默认密码插件
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456';
```

**step3：创建数据库：**

`注意：`水平分片的id需要在业务层实现，`不能依赖数据库的主键自增`

```sql
CREATE DATABASE db_order;
USE db_order;
CREATE TABLE t_order0 (
  id BIGINT,
  order_no VARCHAR(30),
  user_id BIGINT,
  amount DECIMAL(10,2),
  PRIMARY KEY(id) 
);
CREATE TABLE t_order1 (
  id BIGINT,
  order_no VARCHAR(30),
  user_id BIGINT,
  amount DECIMAL(10,2),
  PRIMARY KEY(id) 
);
```

### 2、创建server-order1容器

**step1：创建容器：**

```shell
docker run -d \
-p 3311:3306 \
-v /atguigu/server/order1/conf:/etc/mysql/conf.d \
-v /atguigu/server/order1/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=123456 \
--name server-order1 \
mysql:8.0.29
```

**step2：登录MySQL服务器：**

```shell
#进入容器：
docker exec -it server-order1 env LANG=C.UTF-8 /bin/bash
#进入容器内的mysql命令行
mysql -uroot -p
#修改默认密码插件
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '123456';
```

**step3：创建数据库：**和server-order0相同

`注意：`水平分片的id需要在业务层实现，不能依赖数据库的主键自增

```sql
CREATE DATABASE db_order;
USE db_order;
CREATE TABLE t_order0 (
  id BIGINT,
  order_no VARCHAR(30),
  user_id BIGINT,
  amount DECIMAL(10,2),
  PRIMARY KEY(id) 
);
CREATE TABLE t_order1 (
  id BIGINT,
  order_no VARCHAR(30),
  user_id BIGINT,
  amount DECIMAL(10,2),
  PRIMARY KEY(id) 
);
```



## 2、水平分片分析

### 1、基本配置

```properties
#========================基本配置
# 应用名称
spring.application.name=sharging-jdbc-demo
# 开发环境设置
spring.profiles.active=dev
# 内存模式
spring.shardingsphere.mode.type=Memory
# 打印SQL
spring.shardingsphere.props.sql-show=true
```

### 2、数据源配置

```properties
#========================数据源配置
# 配置真实数据源
spring.shardingsphere.datasource.names=server-order0,server-order1

# 配置第 2 个数据源
spring.shardingsphere.datasource.server-order0.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.server-order0.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.server-order0.jdbc-url=jdbc:mysql://192.168.22.150:3310/db_order
spring.shardingsphere.datasource.server-order0.username=root
spring.shardingsphere.datasource.server-order0.password=123456

# 配置第 3 个数据源
spring.shardingsphere.datasource.server-order1.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.server-order1.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.server-order1.jdbc-url=jdbc:mysql://192.168.22.150:3311/db_order
spring.shardingsphere.datasource.server-order1.username=root
spring.shardingsphere.datasource.server-order1.password=123456

spring.shardingsphere.rules.sharding.tables.t_order.actual-data-nodes=server-order0.t_order0,server-order0.t_order1,server-order1.t_order0,server-order1.t_order1
```

### 3、标椎分片表配置

```properties
#========================标准分片表配置（数据节点配置）
# spring.shardingsphere.rules.sharding.tables.<table-name>.actual-data-nodes=值
# 值由数据源名 + 表名组成，以小数点分隔。多个表以逗号分隔，支持 inline 表达式。
# <table-name>：逻辑表名
# spring.shardingsphere.rules.sharding.tables.t_user.actual-data-nodes=server-user.t_user
spring.shardingsphere.rules.sharding.tables.t_order.actual-data-nodes=server-order0.t_order0,server-order0.t_order1,server-order1.t_order0,server-order1.t_order1
```

修改Order实体类的主键策略：

```java
//@TableId(type = IdType.AUTO)//依赖数据库的主键自增策略
@TableId(type = IdType.ASSIGN_ID)//分布式id
```

测试：保留上面配置中的一个分片表节点分别进行测试，检查每个分片节点是否可用

```java
// 水平分片：插入数据测试
@Test
public void testInsertOrder(){
    Order order = new Order();
    order.setOrderNo("ATGUIGU001");
    order.setUserId(1L);
    order.setAmount(new BigDecimal(100));
    orderMapper.insert(order);
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202211252034792.png" alt="image-20221125203431688" style="zoom:80%;" />

### 4、行表达式(优化)

优化上一步的分片表配置

https://shardingsphere.apache.org/document/5.1.1/cn/features/sharding/concept/inline-expression/

```properties
#========================标准分片表配置（数据节点配置）
# spring.shardingsphere.rules.sharding.tables.<table-name>.actual-data-nodes=值
# 值由数据源名 + 表名组成，以小数点分隔。多个表以逗号分隔，支持 inline 表达式。
# <table-name>：逻辑表名
# spring.shardingsphere.rules.sharding.tables.t_user.actual-data-nodes=server-user.t_user
# order$->{0..1}连续，order$->[0..1]枚举，都是表示order0和order1表
spring.shardingsphere.rules.sharding.tables.t_order.actual-data-nodes=server-order$->{0..1}.t_order$->{0..1}
```

### 5、分片算法配置

**水平分库**

分片规则：order表中`user_id`为偶数时，数据插入`server-order0服务器`，`user_id`为奇数时，数据插入`server-order1服务器`。这样分片的好处是，同一个用户的订单数据，一定会被插入到同一台服务器上，查询一个用户的订单时效率较高。

```properties
#------------------------分库策略
# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order.database-strategy.standard.sharding-column=user_id
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order.database-strategy.standard.sharding-algorithm-name=alg_inline_userid

#------------------------分片算法配置
# 行表达式分片算法
# 分片算法类型
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_inline_userid.type=INLINE
# 分片算法属性配置
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_inline_userid.props.algorithm-expression=server-order$->{user_id % 2}

# 取模分片算法
# 分片算法类型
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_mod.type=MOD
# 分片算法属性配置
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_mod.props.sharding-count=2
```

为了方便测试，先设置只在 `t_order0`表上进行测试

```properties
xxx.actual-data-nodes=server-order$->{0..1}.t_order0
```

测试：可以分别测试行表达式分片算法和取模分片算法

```java
// 水平分片：分库插入数据测试
@Test
public void testInsertOrderDatabaseStrategy(){
    for (long i = 0; i < 4; i++) {
        Order order = new Order();
        order.setOrderNo("ATGUIGU001");
        order.setUserId(i + 1);
        order.setAmount(new BigDecimal(100));
        orderMapper.insert(order);
    }
}
```



**水平分表**

分片规则：order表中`order_no的哈希值为偶数时`，数据插入对应服务器的`t_order0表`，`order_no的哈希值为奇数时`，数据插入对应服务器的`t_order1表`。因为order_no是字符串形式，因此不能直接取模。

```properties
#------------------------分表策略
# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order.table-strategy.standard.sharding-column=order_no
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order.table-strategy.standard.sharding-algorithm-name=alg_hash_mod


#------------------------分片算法配置
# 哈希取模分片算法
# 分片算法类型
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_hash_mod.type=HASH_MOD
# 分片算法属性配置
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_hash_mod.props.sharding-count=2
```

测试前不要忘记将如下节点改回原来的状态

```properties
xxx.actual-data-nodes=server-order$->{0..1}.t_order$->{0..1}
```

### 6、分布式序列算法

**雪花算法：**

https://shardingsphere.apache.org/document/5.1.1/cn/features/sharding/concept/key-generator/

水平分片需要关注全局序列，因为不能简单的使用基于数据库的主键自增。

这里有两种方案：一种是基于MyBatisPlus的id策略；一种是ShardingSphere-JDBC的全局序列配置。

`基于MyBatisPlus的id策略：`将Order类的id设置成如下形式

```java
@TableId(type = IdType.ASSIGN_ID)
private Long id;
```

`基于ShardingSphere-JDBC的全局序列配置`：和前面的MyBatisPlus的策略二选一

```properties
#------------------------分布式序列策略配置
# 分布式序列列名称
spring.shardingsphere.rules.sharding.tables.t_order.key-generate-strategy.column=id
# 分布式序列算法名称
spring.shardingsphere.rules.sharding.tables.t_order.key-generate-strategy.key-generator-name=alg_snowflake

# 分布式序列算法配置
# 分布式序列算法类型
spring.shardingsphere.rules.sharding.key-generators.alg_snowflake.type=SNOWFLAKE
# 分布式序列算法属性配置
#spring.shardingsphere.rules.sharding.key-generators.alg_snowflake.props.xxx=
```

此时，需要将实体类中的id策略修改成以下形式：

```java
//当配置了shardingsphere-jdbc的分布式序列时，自动使用shardingsphere-jdbc的分布式序列
//当没有配置shardingsphere-jdbc的分布式序列时，自动依赖数据库的主键自增策略
@TableId(type = IdType.AUTO)
```



## 3、水平分片完整实现⭐

### 1、Order

```java
@TableName("t_order")
@Data
public class Order {
    //@TableId(type = IdType.AUTO)//依赖数据库的主键自增策略
    @TableId(type = IdType.ASSIGN_ID)//分布式id
    private Long id;
    private String orderNo;
    private Long userId;
    private BigDecimal amount;
}
```

### 2、OrderMapper

```java
import com.baomidou.mybatisplus.core.mapper.BaseMapper;
import com.it.entity.Order;
import org.apache.ibatis.annotations.Mapper;

@Mapper
public interface OrderMapper extends BaseMapper<Order> {
}
```

### 3、application.properties⭐

```properties
# 内存模式
spring.shardingsphere.mode.type=Memory
# 打印SQl
spring.shardingsphere.props.sql-show=true
#========================数据源配置
# 配置真实数据源
spring.shardingsphere.datasource.names=server-order0,server-order1

## 配置第 1 个数据源
#spring.shardingsphere.datasource.server-user.type=com.zaxxer.hikari.HikariDataSource
#spring.shardingsphere.datasource.server-user.driver-class-name=com.mysql.jdbc.Driver
#spring.shardingsphere.datasource.server-user.jdbc-url=jdbc:mysql://192.168.0.155:3307/db_user
#spring.shardingsphere.datasource.server-user.username=root
#spring.shardingsphere.datasource.server-user.password=123456

# 配置第 2 个数据源
spring.shardingsphere.datasource.server-order0.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.server-order0.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.server-order0.jdbc-url=jdbc:mysql://192.168.0.155:3307/db_order
spring.shardingsphere.datasource.server-order0.username=root
spring.shardingsphere.datasource.server-order0.password=123456

# 配置第 3 个数据源
spring.shardingsphere.datasource.server-order1.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.server-order1.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.server-order1.jdbc-url=jdbc:mysql://192.168.22.145:3306/db_order
spring.shardingsphere.datasource.server-order1.username=root
spring.shardingsphere.datasource.server-order1.password=123456

#========================标准分片表配置（数据节点配置）
# spring.shardingsphere.rules.sharding.tables.<table-name>.actual-data-nodes=值
# 值由数据源名 + 表名组成，以小数点分隔。多个表以逗号分隔，支持 inline 表达式。
# <table-name>：逻辑表名
#spring.shardingsphere.rules.sharding.tables.t_user.actual-data-nodes=server-user.t_user
# order$->{0..1}连续，order$->[0..1]枚举，都是表示order0和order1表
spring.shardingsphere.rules.sharding.tables.t_order.actual-data-nodes=server-order$->{0..1}.t_order$->{0..1}

#------------------------分库策略
# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order.database-strategy.standard.sharding-column=user_id
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order.database-strategy.standard.sharding-algorithm-name=alg_inline_userid

#------------------------分片算法配置
# 行表达式分片算法
# 分片算法类型
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_inline_userid.type=INLINE
# 分片算法属性配置
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_inline_userid.props.algorithm-expression=server-order$->{user_id % 2}

# 取模分片算法
# 分片算法类型
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_mod.type=MOD
# 分片算法属性配置
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_mod.props.sharding-count=2

#------------------------分表策略
# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order.table-strategy.standard.sharding-column=order_no
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order.table-strategy.standard.sharding-algorithm-name=alg_hash_mod

#------------------------分片算法配置
# 哈希取模分片算法
# 分片算法类型
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_hash_mod.type=HASH_MOD
# 分片算法属性配置
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_hash_mod.props.sharding-count=2
```

### 4、测试插入

> 可以发现，数据被分到4张表中了

```java
@Test
public void testInsertOrderTableStrategy(){
    for (long i = 1; i < 5; i++) {
        Order order = new Order();
        order.setOrderNo("ATGUIGU" + i);
        order.setUserId(1L);
        order.setAmount(new BigDecimal(100));
        orderMapper.insert(order);
    }

    for (long i = 5; i < 9; i++) {
        Order order = new Order();
        order.setOrderNo("ATGUIGU" + i);
        order.setUserId(2L);
        order.setAmount(new BigDecimal(100));
        orderMapper.insert(order);
    }
}
```

```java
/**
  * 测试哈希取模
*/
@Test
public void testHash(){
    //注意hash取模的结果是整个字符串hash后再取模，和数值后缀是奇数还是偶数无关
    System.out.println("ATGUIGU001".hashCode() % 2);
    System.out.println("ATGUIGU0011".hashCode() % 2);
}
```

### 5、测试查询

> 可以查询到4张表的所有数据

```java
/**
 * 水平分片：查询所有记录
 * 查询了两个数据源，每个数据源中使用UNION ALL连接两个表
 */
@Test
public void testShardingSelectAll(){
    List<Order> orders = orderMapper.selectList(null);
    orders.forEach(System.out::println);
}

/**
 * 水平分片：根据user_id查询记录
 * 查询了一个数据源，每个数据源中使用UNION ALL连接两个表
 */
@Test
public void testShardingSelectByUserId(){
    QueryWrapper<Order> orderQueryWrapper = new QueryWrapper<>();
    orderQueryWrapper.eq("user_id", 1L);
    List<Order> orders = orderMapper.selectList(orderQueryWrapper);
    orders.forEach(System.out::println);
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181142139.png" alt="image-20220918114233974" style="zoom:80%;" />



## 4、多表关联插入

### 1、创建关联表

在`server-order0、server-order1`服务器中分别创建两张订单详情表`t_order_item0、t_order_item1`

我们希望`同一个用户的订单表和订单详情表中的数据都在同一个数据源中，避免跨库关联`，因此这两张表我们使用相同的分片策略。

那么在`t_order_item`中我们也需要创建`order_no`和`user_id`这两个分片键

```sql
CREATE TABLE t_order_item0(
    id BIGINT,
    order_no VARCHAR(30),
    user_id BIGINT,
    price DECIMAL(10,2),
    `count` INT,
    PRIMARY KEY(id)
);

CREATE TABLE t_order_item1(
    id BIGINT,
    order_no VARCHAR(30),
    user_id BIGINT,
    price DECIMAL(10,2),
    `count` INT,
    PRIMARY KEY(id)
);
```

### 2、创建实体类

```java
@TableName("t_order_item")
@Data
public class OrderItem {
    //当配置了shardingsphere-jdbc的分布式序列时，自动使用shardingsphere-jdbc的分布式序列
    @TableId(type = IdType.AUTO)
    private Long id;
    private String orderNo;
    private Long userId;
    private BigDecimal price;
    private Integer count;
}
```

### 3、创建Mapper

```java
@Mapper
public interface OrderItemMapper extends BaseMapper<OrderItem> {

}
```

### 4、配置关联表

t_order_item的分片表、分片策略、分布式序列策略和t_order一致

```properties
#------------------------标准分片表配置（数据节点配置）
spring.shardingsphere.rules.sharding.tables.t_order_item.actual-data-nodes=server-order$->{0..1}.t_order_item$->{0..1}

#------------------------分库策略
# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order_item.database-strategy.standard.sharding-column=user_id
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order_item.database-strategy.standard.sharding-algorithm-name=alg_mod

#------------------------分表策略
# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order_item.table-strategy.standard.sharding-column=order_no
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order_item.table-strategy.standard.sharding-algorithm-name=alg_hash_mod

#------------------------分布式序列策略配置
# 分布式序列列名称
spring.shardingsphere.rules.sharding.tables.t_order_item.key-generate-strategy.column=id
# 分布式序列算法
spring.shardingsphere.rules.sharding.tables.t_order_item.key-generate-strategy.key-generator-name=alg_snowflake
# 分布式序列算法配置
# 分布式序列算法类型
spring.shardingsphere.rules.sharding.key-generators.alg_snowflake.type=SNOWFLAKE
```



### 5、测试插入数据

同一个用户的订单表和订单详情表中的数据都在同一个数据源中，避免跨库关联

```java
/**
     * 测试关联表插入
     */
@Test
public void testInsertOrderAndOrderItem(){

    for (long i = 1; i < 3; i++) {
        Order order = new Order();
        order.setOrderNo("ATGUIGU" + i);
        order.setUserId(1L);
        orderMapper.insert(order);
        for (long j = 1; j < 3; j++) {
            OrderItem orderItem = new OrderItem();
            orderItem.setOrderNo("ATGUIGU" + i);
            orderItem.setUserId(1L);
            orderItem.setPrice(new BigDecimal(10));
            orderItem.setCount(2);
            orderItemMapper.insert(orderItem);
        }
    }

    for (long i = 5; i < 7; i++) {
        Order order = new Order();
        order.setOrderNo("ATGUIGU" + i);
        order.setUserId(2L);
        orderMapper.insert(order);
        for (long j = 1; j < 3; j++) {
            OrderItem orderItem = new OrderItem();
            orderItem.setOrderNo("ATGUIGU" + i);
            orderItem.setUserId(2L);
            orderItem.setPrice(new BigDecimal(1));
            orderItem.setCount(3);
            orderItemMapper.insert(orderItem);
        }
    }
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181202585.png" alt="image-20220918120250465" style="zoom:80%;" />



## 5、多表关联查询

> **需求：**查询每个订单的订单号和总订单金额

### 1、创建VO对象

```java
@Data
public class OrderVo {
    private String orderNo;
    private BigDecimal amount;
}
```

### 2、添加Mapper方法

```java

@Mapper
public interface OrderMapper extends BaseMapper<Order> {

    @Select({"SELECT o.order_no, SUM(i.price * i.count) AS amount",
            "FROM t_order o JOIN t_order_item i ON o.order_no = i.order_no",
            "GROUP BY o.order_no"})
    List<OrderVo> getOrderAmount();
}
```

### 3、测试关联查询

```java
// 测试关联表查询
@Test
public void testGetOrderAmount(){
    List<OrderVo> orderAmountList = orderMapper.getOrderAmount();
    orderAmountList.forEach(System.out::println);
}
```

### 4、配置绑定表

在原来水平分片配置的基础上添加如下配置：

```properties
#---绑定表，多个绑定表就数组0，1，2.。顺序写就行，报黄标不用管
spring.shardingsphere.rules.sharding.binding-tables[0]=t_order,t_order_item
```

配置完绑定表后再次进行关联查询的测试：不是结果8个，结果正常，只是执行了8次真实表查询

- **如果不配置绑定表：测试的结果为8个SQL。**多表关联查询会出现笛卡尔积关联。

- **如果配置绑定表：测试的结果为4个SQL。** 多表关联查询不会出现笛卡尔积关联，关联查询效率将大大提升。

`绑定表：`指分片规则一致的一组分片表。 使用绑定表进行多表关联查询时，必须使用分片键进行关联，否则会出现笛卡尔积关联或跨库关联，从而影响查询效率。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181232926.png" alt="image-20220918123257654" style="zoom:80%;" />

### 5、完整配置⭐⭐

```properties
#------------------------基本配置
# 开发环境设置
spring.profiles.active=dev
# 内存模式
spring.shardingsphere.mode.type=Memory
# 打印SQl
spring.shardingsphere.props.sql-show=true

#------------------------数据源配置
# 配置真实数据源
spring.shardingsphere.datasource.names=server-order0,server-order1

# 配置第 2 个数据源
spring.shardingsphere.datasource.server-order0.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.server-order0.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.server-order0.jdbc-url=jdbc:mysql://192.168.22.145:3306/db_order
spring.shardingsphere.datasource.server-order0.username=root
spring.shardingsphere.datasource.server-order0.password=123456

# 配置第 3 个数据源
spring.shardingsphere.datasource.server-order1.type=com.zaxxer.hikari.HikariDataSource
spring.shardingsphere.datasource.server-order1.driver-class-name=com.mysql.jdbc.Driver
spring.shardingsphere.datasource.server-order1.jdbc-url=jdbc:mysql://192.168.0.155:3307/db_order
spring.shardingsphere.datasource.server-order1.username=root
spring.shardingsphere.datasource.server-order1.password=123456

#------------------------标准分片表配置（数据节点配置）
# 由数据源名 + 表名组成，以小数点分隔。多个表以逗号分隔，支持 inline 表达式。
spring.shardingsphere.rules.sharding.tables.t_order.actual-data-nodes=server-order$->{0..1}.t_order$->{0..1}
spring.shardingsphere.rules.sharding.tables.t_order_item.actual-data-nodes=server-order$->{0..1}.t_order_item$->{0..1}

#------------------------分库策略，缺省表示使用默认分库策略，以下的分片策略只能选其一
# 用于单分片键的标准分片场景
# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order.database-strategy.standard.sharding-column=user_id
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order.database-strategy.standard.sharding-algorithm-name=alg_mod


# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order_item.database-strategy.standard.sharding-column=user_id
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order_item.database-strategy.standard.sharding-algorithm-name=alg_mod


#------------------------分表策略
# 用于单分片键的标准分片场景
# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order.table-strategy.standard.sharding-column=order_no
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order.table-strategy.standard.sharding-algorithm-name=alg_hash_mod

# 分片列名称
spring.shardingsphere.rules.sharding.tables.t_order_item.table-strategy.standard.sharding-column=order_no
# 分片算法名称
spring.shardingsphere.rules.sharding.tables.t_order_item.table-strategy.standard.sharding-algorithm-name=alg_hash_mod

#------------------------分片算法配置
# 行表达式分片算法
# 分片算法类型
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_inline_userid.type=INLINE
# 分片算法属性配置
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_inline_userid.props.algorithm-expression=server-order$->{user_id % 2}

# 取模分片算法
# 分片算法类型
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_mod.type=MOD
# 分片算法属性配置
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_mod.props.sharding-count=2

# 哈希取模分片算法
# 分片算法类型
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_hash_mod.type=HASH_MOD
# 分片算法属性配置
spring.shardingsphere.rules.sharding.sharding-algorithms.alg_hash_mod.props.sharding-count=2

#------------------------分布式序列策略配置
# 分布式序列列名称
spring.shardingsphere.rules.sharding.tables.t_order.key-generate-strategy.column=id
# 分布式序列算法名称
spring.shardingsphere.rules.sharding.tables.t_order.key-generate-strategy.key-generator-name=alg_snowflake

# 分布式序列列名称
spring.shardingsphere.rules.sharding.tables.t_order_item.key-generate-strategy.column=id
# 分布式序列算法名称
spring.shardingsphere.rules.sharding.tables.t_order_item.key-generate-strategy.key-generator-name=alg_snowflake


#------------------------分布式序列算法配置
# 分布式序列算法类型
spring.shardingsphere.rules.sharding.key-generators.alg_snowflake.type=SNOWFLAKE
# 分布式序列算法属性配置
#spring.shardingsphere.rules.sharding.key-generators.alg_snowflake.props.xxx=

#------------------------绑定表配置
# 绑定表规则列表
spring.shardingsphere.rules.sharding.binding-tables[0]=t_order,t_order_item
```



## 6、广播表

### 1、什么是广播表

> 指所有的分片数据源中都存在的表，表结构及其数据在每个数据库中均完全一致。 适用于数据量不大且需要与海量数据的表进行关联查询的场景，例如：字典表。
>

广播具有以下特性：

> （1）插入、更新操作会实时在所有节点上执行，保持各个分片的数据一致性
>
> （2）查询操作，只从一个节点获取
>
> （3）可以跟任何一个表进行 JOIN 操作

### 2、创建广播表

在server-order0、server-order1和server-user服务器中分别创建t_dict表，表不是固定的

```sql
CREATE TABLE t_dict(
    id BIGINT,
    dict_type VARCHAR(200),
    PRIMARY KEY(id)
);
```



### 3、程序实现

#### 4.3.1、创建实体类

```java
@TableName("t_dict")
@Data
public class Dict {
    //可以使用MyBatisPlus的雪花算法
    @TableId(type = IdType.ASSIGN_ID)
    private Long id;
    private String dictType;
}
```

#### 4.3.2、创建Mapper

```java
@Mapper
public interface DictMapper extends BaseMapper<Dict> {
}
```

#### 4.3.3、配置广播表

```properties
# 数据节点可不配置，默认情况下，向所有数据源广播
spring.shardingsphere.rules.sharding.tables.t_dict.actual-data-nodes=server-user.t_dict,server-order$->{0..1}.t_dict

# 广播表
spring.shardingsphere.rules.sharding.broadcast-tables[0]=t_dict
```

### 4、测试广播表

```java
@Resource
private DictMapper dictMapper;

/**
 * 广播表：每个服务器中的t_dict同时添加了新数据
*/
@Test
public void testBroadcast(){
    Dict dict = new Dict();
    dict.setDictType("type1");
    dictMapper.insert(dict);
}

/**
  * 查询操作，只从一个节点获取数据
  * 随机负载均衡规则
*/
@Test
public void testSelectBroadcast(){
    List<Dict> dicts = dictMapper.selectList(null);
    dicts.forEach(System.out::println);
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181252493.png" alt="image-20220918125235379" style="zoom:80%;" />



# ShardingSphere-Proxy

## 1、获取

目前 ShardingSphere-Proxy 提供了 3 种获取方式：

- [二进制发布包](https://shardingsphere.apache.org/document/5.1.1/cn/user-manual/shardingsphere-proxy/startup/bin/)
- [Docker](https://shardingsphere.apache.org/document/5.1.1/cn/user-manual/shardingsphere-proxy/startup/docker/)
- [Helm](https://shardingsphere.apache.org/document/5.1.1/cn/user-manual/shardingsphere-proxy/startup/helm/)

## 2、使用二进制发布包安装⭐⭐

二进制包既可以Linux系统运行，又可以在windows系统运行

### step1：解压二进制包

`apache-shardingsphere-5.1.1-shardingsphere-proxy-bin.tar.gz`

windows：使用解压软件解压文件

Linux：将文件上传至/root目录，并解压

```shell
tar -zxvf apache-shardingsphere-5.1.1-shardingsphere-proxy-bin.tar.gz
mv apache-shardingsphere-5.1.1-shardingsphere-proxy-bin shardingsphere-proxy
```

### step2：MySQL驱动

新建`ext-lib`目录，并把`mysql-connector-java-8.0.22.jar`上传至该目录

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172137339.png" alt="image-20220917213744263" style="zoom:80%;" />

### step3：修改配置conf/server.yaml

> 只需要解开注释即可,服务器的核心配置

```yaml
rules:
  - !AUTHORITY
    users:
      # 在任何服务器上都能访问该服务器：用户名root，密码root
      - root@%:root 
    provider:
      # 当前用户拥有所有访问权限
      type: ALL_PRIVILEGES_PERMITTED
# 打印SQL
props:
  sql-show: true
```





**spte4：启动ShardingSphere-Proxy**

Linux 操作系统请运行 `bin/start.sh`

Windows 操作系统请运行 `bin/start.bat` 

指定端口号和配置文件目录：`bin/start.bat ${proxy_port} ${proxy_conf_directory}` 

```apl
bin/start.sh 3321
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172135609.png" alt="image-20220917213542488" style="zoom: 80%;" />

### step5：远程连接ShardingSphere-Proxy

> 密码就是配置文件的：root
>

```shell
mysql -h127.0.0.1 -P3321 -uroot -p
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172134669.png" alt="image-20220917213431527" style="zoom:80%;" />

### step6：访问测试

```sql
show databases;
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172134906.png" alt="image-20220917213402762" style="zoom:80%;" />



## 3、使用Docker安装

**step1：启动Docker容器**

```shell
docker run -d \
-v /atguigu/server/proxy-a/conf:/opt/shardingsphere-proxy/conf \
-v /atguigu/server/proxy-a/ext-lib:/opt/shardingsphere-proxy/ext-lib \
-e ES_JAVA_OPTS="-Xmx256m -Xms256m -Xmn128m" \
-p 3321:3307 \
--name server-proxy-a \
apache/shardingsphere-proxy:5.1.1
```



**step2：上传MySQL驱动**

将MySQl驱动上传至`/atguigu/server/proxy-a/ext-lib`目录

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171950106.png" alt="image-20220917195053975" style="zoom:80%;" />



**spte3：修改配置server.yaml**

> 服务器的核心配置

```yaml
rules:
  - !AUTHORITY
    users:
      # 在任何服务器上都能访问该服务器：用户名root，密码root
      - root@%:root 
    provider:
      # 当前用户拥有所有访问权限
      type: ALL_PRIVILEGES_PERMITTED
# 打印SQL
props:
  sql-show: true
```

将配置文件上传至`/atguigu/server/proxy-a/conf`目录



**spte4：重启容器**

```shell
docker restart server-proxy-a
```



**step5：远程连接ShardingSphere-Proxy**

ShardingSphere-Proxy容器中默认情况下没有mysql命令行客户端的安装，因此需要远程访问

```shell
mysql -h192.168.100.201 -P3321 -uroot -p
```



**step6：访问测试**

```sql
show databases;
```

![image-20220819152009158](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804459.png)



**常见问题：docker容器无法远程连接**

容器可以成功的创建并启动，但是无法远程连接。排除防火墙和网络等问题后，看看是不是因为容器内存不足导致。

`原因：`容器可分配内存不足

`查看办法：`进入容器后查看ShardingSphere-Proxy的日志，如有有`cannot allocate memory`，则说明容器内存不足

```shell
docker exec -it server-proxy-a env LANG=C.UTF-8 /bin/bash
cd /opt/shardingsphere-proxy/logs
tail stdout.log 
```

![image-20220819151154763](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209171804470.png)

`解决方案：`创建容器的时候使用JVM参数

```shell
-e ES_JAVA_OPTS="-Xmx256m -Xms256m -Xmn128m"
```



# ShardingSphere-Proxy读写分离

## 配置演示

### 1、修改配置文件

**修改配置config-readwrite-splitting.yaml**

```sh
cd /root/shardingsphere-proxy
```

```yaml
# 名字任意
schemaName: readwrite_splitting_db

dataSources:
  write_ds:
    url: jdbc:mysql://192.168.22.145:3307/ph4?serverTimezone=UTC&useSSL=false
    username: root
    password: 123456
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
    minPoolSize: 1
  read_ds_0:
    url: jdbc:mysql://192.168.22.145:3308/ph4?serverTimezone=UTC&useSSL=false
    username: root
    password: 123456
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
    minPoolSize: 1
  read_ds_1:
    url: jdbc:mysql://192.168.22.145:3309/ph4?serverTimezone=UTC&useSSL=false
    username: root
    password: 123456
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
    minPoolSize: 1

rules:
- !READWRITE_SPLITTING
  dataSources:
    readwrite_ds:
      type: Static
      props:
        write-data-source-name: write_ds
        read-data-source-names: read_ds_0,read_ds_1
```

将配置文件上传至`/atguigu/server/proxy-a/conf`目录

### 2、重启服务

```shell
bin/stop.sh 3321
bin/start.sh 3321
```

### 3、访问测试

```sql
mysql -h127.0.0.1 -P3321 -uroot -p
show databases;
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172303712.png" alt="image-20220917230327619" style="zoom:80%;" />

```sql
use readwrite_splitting_db;
show tables;
# 多次执行，根据下面日志可以发现是轮询从库执行的
select * from author;
# 插入测试
insert into author(id,name,country) values(null,'wangrui','JPA');
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172304681.png" alt="image-20220917230432536" style="zoom:80%;" />

### 4、实时查看日志

可以通过这种方式查看服务器中输出的SQL语句

```shell
tail -f /root/shardingsphere-proxy/logs/stdout.log
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172305310.png" alt="image-20220917230549188" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172308232.png" alt="image-20220917230827088" style="zoom:80%;" />

## SpringBoot访问Proxy⭐

### 1、添加依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <dependency>
        <groupId>mysql</groupId>
        <artifactId>mysql-connector-java</artifactId>
        <version>8.0.22</version>
    </dependency>
    <dependency>
        <groupId>com.baomidou</groupId>
        <artifactId>mybatis-plus-boot-starter</artifactId>
        <version>3.3.1</version>
    </dependency>
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <optional>true</optional>
    </dependency>
</dependencies>
```

### 2、创建实体类

```java
@Data
@Accessors(chain = true) // 方便链式设置对象
public class author {
    // 遵循数据库自增逻辑插入ID，不再自己插入一串字符了
    @TableId(type = IdType.AUTO)
    private Long id;
    private String name;
    private String country;
}
```

### 3、创建Mapper

```java
@Mapper
public interface authorMapper extends BaseMapper<author> {
}
```

### 4、配置数据源

创建application-dev.yml

```yml
spring:
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://192.168.22.145:3321/readwrite_splitting_db?serverTimezone=GMT%2B8&useSSL=false
    password: root
    username: root
mybatis-plus:
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
```

```properties
spring.profiles.active=dev
```



### 5、测试

```java
@Resource
private authorMapper authorMapper;
// 读数据测试
@Test
public void testSelect() {
    List<author> authors = authorMapper.selectList(null);
    authors.forEach(System.out::println);
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172320977.png" alt="image-20220917232025903" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172316705.png" alt="image-20220917231613610" style="zoom:80%;" />



# ShardingSphere-Proxy垂直分片

## 配置演示

### 1、修改配置config-sharding.yaml

```yml
schemaName: sharding_db1

dataSources:
  ds_0:
    url: jdbc:mysql://192.168.22.145:3306/db_user?serverTimezone=UTC&useSSL=false
    username: root
    password: 123456
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
    minPoolSize: 1
  ds_1:
    url: jdbc:mysql://192.168.0.155:3307/db_order?serverTimezone=UTC&useSSL=false
    username: root
    password: 123456
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
    minPoolSize: 1

rules:
- !SHARDING
  tables:
    t_user:
      actualDataNodes: ds_0.t_user
    t_order:
      actualDataNodes: ds_1.t_order
```

### 2、重启服务

```shell
bin/stop.sh 3321
bin/start.sh 3321
```

### 3、访问测试

```sql
mysql -h127.0.0.1 -P3321 -uroot -p
show databases;
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181306715.png" alt="image-20220918130650589" style="zoom:80%;" />

```sql
mysql> show databases;
mysql> use sharding_db1;
mysql> show tables;
mysql> select * from t_order;
mysql> select * from t_user;
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209172304681.png" alt="image-20220917230432536" style="zoom:80%;" />

### 4、实时查看日志

可以通过这种方式查看服务器中输出的SQL语句

```shell
tail -f /root/shardingsphere-proxy/logs/stdout.log
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181309157.png" alt="image-20220918130900986" style="zoom:80%;" />



# ShardingSphere-Proxy水平分片

## 1、修改配置文件

**修改配置config-sharding.yaml**

```yaml
schemaName: sharding_db2

dataSources:
  ds_order0:
    url: jdbc:mysql://192.168.22.145:3306/db_order?serverTimezone=UTC&useSSL=false
    username: root
    password: 123456
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
    minPoolSize: 1
  ds_order1:
    url: jdbc:mysql://192.168.0.155:3307/db_order?serverTimezone=UTC&useSSL=false
    username: root
    password: 123456
    connectionTimeoutMilliseconds: 30000
    idleTimeoutMilliseconds: 60000
    maxLifetimeMilliseconds: 1800000
    maxPoolSize: 50
    minPoolSize: 1

rules:
- !SHARDING
  tables:
    t_order:
      actualDataNodes: ds_order${0..1}.t_order${0..1}
      databaseStrategy:
        standard:
          shardingColumn: user_id
          shardingAlgorithmName: alg_mod
      tableStrategy:
        standard:
          shardingColumn: order_no
          shardingAlgorithmName: alg_hash_mod
      keyGenerateStrategy:
        column: id
        keyGeneratorName: snowflake
    t_order_item:
      actualDataNodes: ds_order${0..1}.t_order_item${0..1}
      databaseStrategy:
        standard:
          shardingColumn: user_id
          shardingAlgorithmName: alg_mod
      tableStrategy:
        standard:
          shardingColumn: order_no
          shardingAlgorithmName: alg_hash_mod
      keyGenerateStrategy:
        column: id
        keyGeneratorName: snowflake

  bindingTables:
    - t_order,t_order_item


  broadcastTables:
    - t_dict

  shardingAlgorithms:
    alg_inline_userid:
      type: INLINE
      props:
        algorithm-expression: server-order$->{user_id % 2}
    alg_mod:
      type: MOD
      props:
        sharding-count: 2
    alg_hash_mod:
      type: HASH_MOD
      props:
        sharding-count: 2
  
  keyGenerators:
    snowflake:
      type: SNOWFLAKE
```

## 2、重启服务

```shell
bin/stop.sh 3321
bin/start.sh 3321
```

## 3、访问测试

```sql
-- 没进去就再启动，要么就等一会
mysql -h127.0.0.1 -P3321 -uroot -p
show databases;
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181314915.png" alt="image-20220918131452786" style="zoom:80%;" />

```sql
mysql> show databases;
mysql> use sharding_db2;
mysql> show tables;
mysql> select * from t_order; --测试水平分片
mysql> select * from t_dict; --测试广播表
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181315312.png" alt="image-20220918131542185" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181315099.png" alt="image-20220918131556896" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181316345.png" alt="image-20220918131606207" style="zoom:80%;" />

## 4、实时查看日志

可以通过这种方式查看服务器中输出的SQL语句

```shell
tail -f /root/shardingsphere-proxy/logs/stdout.log
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202209181316358.png" alt="image-20220918131622107" style="zoom:80%;" />



























































