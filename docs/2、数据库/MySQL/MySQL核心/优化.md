



# MySQL性能影响因素

## 什么影响了数据库查询速度

### 四个因素

> - SQL查询速度
> - 服务器硬件
> - 网卡流量
> - 磁盘IO

### 风险分析 

**QPS：**Queries Per Second意思是“每秒查询率”，是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。

**TPS：**是*TransactionsPerSecond*的缩写，也就是事务数/秒。它是软件测试结果的测量单位。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。

> Tips：最好不要在主库上数据库备份，大型活动前取消这样的计划。

- **效率低下的sql：**超高的QPS与TPS。
- **大量的并发：**数据连接数被占满（max_connection默认100，一般把连接数设置得大一些）。
  **并发量：**同一时刻数据库服务器处理的请求数量
- **超高的CPU使用率：**CPU资源耗尽出现宕机。
- **磁盘IO：**磁盘IO性能突然下降、大量消耗磁盘性能的计划任务。解决：更快磁盘设备、调整计划任务、做好磁盘维护。

### 网卡流量：如何避免无法连接数据库

> - 减少从服务器的数量（从服务器会从主服务器复制日志）
> - 进行分级缓存（避免前端大量缓存失效）
> - 避免使用select * 进行查询
> - 分离业务网络和服务器网络

## 如何使用查询缓冲区？

`查询缓冲区可以提高查询的速度，但是这种方式只适合查询语句比较多、更新语句比较少 的情况`。

默认情况下查询缓冲区的大小为 0，也就是不可用。可以修改 `queiy_cache_size` 以调整查询缓冲区大小；修改 `query_cache_type` 以调整查询缓冲区的类型。

在 `my.cnf` 中修改 `query_cache_size` 和 `query_cache_type` 的值如下所示：

```c
[mysqld]
query_cache_size= 512M 
query_cache_type= 1
query_cache_type=1
```

表示开启查询缓冲区。

只有在查询语句中包含 `SQL_NO_CACHE` 关键字时，才不会使用查询缓冲区。可以使用 `FLUSH QUERY CACHE` 语句来刷新缓冲区，清理查询缓冲区中的碎片。

## 什么影响了MySQL性能（非常重要）

### 影响性能的几个方面

> - 服务器硬件
>
> - 服务器系统（系统参数优化）
>
> - 存储引擎
>
> - - MyISAM：不支持事务，表级锁
>   - InnoDB: 支持事务，支持行级锁，事务ACID
>
> - 数据库参数配置。
>
> - 数据库结构设计和SQL语句。（重点优化）

### 如何选择正确的存储引擎

> - 事务
> - 备份(Innodb免费在线备份)
> - 崩溃恢复
> - 存储引擎的特有特性

总结:Innodb大法好。

**注意：尽量别使用混合存储引擎，比如回滚会出问题在线热备问题。**



### 内存配置参数

**内存的使用上限不能超过物理内存，否则容易造成内存溢出；**

**对于32位操作系统，MySQL只能使用3G以下的内存。**

```apl
# 定义了每个线程排序缓存区的大小，MySQL在有查询、
# 需要做排序操作时才会为每个缓冲区分配内存（直接分配该参数的全部内存）；
sort_buffer_size 
# 定义了每个线程所使用的连接缓冲区的大小，如果一个查询关联了多张表，
# MySQL会为每张表分配一个连接缓冲，导致一个查询产生了多个连接缓冲；
join_buffer_size 
# 定义了当对一张MyISAM进行全表扫描时所分配读缓冲池大小，
# MySQL有查询需要时会为其分配内存，其必须是4k的倍数；
read_buffer_size 
#索引缓冲区大小，MySQL有查询需要时会为其分配内存，只会分配需要的大小。
read_rnd_buffer_size 
```

注意：以上四个参数是为一个线程分配的，如果有100个连接，那么需要×100。

**如何为缓存池分配内存**

Innodb_buffer_pool_size，定义了Innodb所使用缓存池的大小，对其性能十分重要，必须足够大，但是过大时，使得Innodb 关闭时候需要更多时间把脏页从缓冲池中刷新到磁盘中；

key_buffer_size，定义了MyISAM所使用的缓存池的大小，由于数据是依赖存储操作系统缓存的，所以要为操作系统预留更大的内存空间；

```apl
# 10240
select sum(index_length) from information_schema.tables where engine='myisam';
```

注意：即使开发使用的表全部是Innodb表，也要为MyISAM预留内存，因为MySQL系统使用的表仍然是MyISAM表。

max_connections 控制允许的最大连接数，一般2000更大。

**不要使用外键约束保证数据的完整性。**



### 性能优化顺序从上到下

> - 数据库结构设计和SQL语句
> - 数据库存储引擎的选择和参数配置
> - 系统选择及优化
> - 硬件升级



# 数据库性能优化方案

毫不夸张的说咱们后端工程师，无论在哪家公司，呆在哪个团队，做哪个系统，遇到的第一个让人头疼的问题绝对是数据库性能问题。

如果我们有一套成熟的方法论，能让大家快速、准确的去选择出合适的优化方案，我相信能够快速准备解决咱么日常遇到的 80% 甚至 90% 的性能问题。

从解决问题的角度出发，我们得先了解到问题的原因；其次我们得有一套思考、判断问题的流程方式，让我们合理的站在哪个层面选择方案。

最后从众多的方案里面选择一个适合的方案进行解决问题，找到一个合适的方案的前提是我们自己对各种方案之间的优缺点、场景有足够的了解，没有一个方案是完全可以通吃通用的，软件工程没有银弹。

下文的我工作多年以来，曾经使用过的八大方案，结合了平常自己学习收集的一些资料，以系统、全面的方式整理成了这篇博文，也希望能让一些有需要的同行在工作上、成长上提供一定的帮助。

## 为什么数据库会慢？

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011800746.png" alt="image-20220801180009661" style="zoom: 50%;" />

无论是关系型数据库还是 NoSQL，任何存储系统决定于其查询性能的主要有三种：

- 查找的时间复杂度
- 数据总量
- 高负载

而决定于查找时间复杂度主要有两个因素：

- 查找算法
- 存储数据结构

无论是哪种存储，数据量越少，自然查询性能就越高，随着数据量增多，资源的消耗（CPU、磁盘读写繁忙）、耗时也会越来越高。

从关系型数据库角度出发，索引结构基本固定是 B+Tree，时间复杂度是 O(log n)，存储结构是行式存储。因此咱们对于关系数据库能优化的一般只有数据量。

而高负载造成原因有高并发请求、复杂查询等，导致 CPU、磁盘繁忙等，而服务器资源不足则会导致慢查询等问题。该类型问题一般会选择集群、数据冗余的方式分担压力。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011822693.png" alt="image-20220801182216597" style="zoom:50%;" />



## 应该站在哪个层面思考优化？

### 优化层面

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011823680.png" alt="image-20220801182344579" style="zoom:50%;" />

从上图可见，自顶向下的一共有四层，分别是硬件、存储系统、存储结构、具体实现。

层与层之间是紧密联系的，每一层的上层是该层的载体；因此越往顶层越能决定性能的上限，同时优化的成本也相对会比较高，性价比也随之越低。

以最底层的具体实现为例，那么索引的优化的成本应该是最小的，可以说加了索引后无论是 CPU 消耗还是响应时间都是立竿见影降低。

然而一个简单的语句，无论如何优化加索引也是有局限的，当在具体实现这层没有任何优化空间的时候就得往上一层【存储结构】思考，思考是否从物理表设计的层面出发优化（如分库分表、压缩数据量等），如果是文档型数据库得思考下文档聚合的结果。

如果在存储结构这层优化得没效果，得继续往再上一次进行考虑，是否关系型数据库应该不适合用在现在得业务场景？如果要换存储，那么得换怎样得 NoSQL?

所以咱们优化的思路，出于性价比的优先考虑具体实现，实在没有优化空间了再往上一层考虑。当然如果公司有钱，直接使用钞能力，绕过了前面三层，这也是一种便捷的应急处理方式。

该篇文章不讨论顶与底的两个层面的优化，主要从存储结构、存储系统中间两层的角度出发进行探讨。

### 完整优化思路

SQL优化主要分4个方向：`SQL语句跟索引`、`表结构`、`系统配置`、`硬件`。

总优化思路就是**最大化利用索引**、**尽可能避免全表扫描**、**减少无效数据的查询**：

> 1、减少数据访问：设置`合理的字段类型`，启用压缩，通过索引访问等减少磁盘 IO。
>
> 2、返回更少的数据：只`返回需要`的字段和数据分页处理，减少磁盘 IO 及网络 IO。
>
> 3、减少交互次数：`批量` DML 操作，函数存储等减少数据连接次数。
>
> 4、减少服务器 CPU 开销：**尽量减少数据库排序操作以及全表查询**，减少 CPU 内存占用 。
>
> 5、分表分区：使用`表分区`，可以增加并行操作，更大限度利用 CPU 资源。

**SQL语句优化大致举例**：

> 1、合理建立覆盖索引：可以有效减少回表。
>
> 2、union，or，in都能命中索引，建议使用in 
>
> 3、负向条件(!=、<>、not in、not exists、not like 等) 索引不会使用索引，建议用in。
>
> 4、在列上进行运算或使用函数会使索引失效，从而进行全表扫描 
>
> 5、小心隐式类型转换，原字符串用整型会触发`CAST`函数导致索引失效。原int用字符串则会走索引。
>
> 6、不建议使用%前缀模糊查询。
>
> 7、多表关联查询时，小表在前，大表在后。在 MySQL 中，执行 from 后的表关联查询是从左往右执行的(Oracle 相反)，第一张表会涉及到全表扫描。
>
> 8、调整 Where 字句中的连接顺序，MySQL 采用从左往右，自上而下的顺序解析 where 子句。根据这个原理，应将过滤数据多的条件往前放，最快速度缩小结果集。

**SQL调优大致思路**

1、先用慢查询日志定位具体需要优化的sql 

2、使用 [explain](https://mp.weixin.qq.com/s?__biz=MzI4NjI1OTI4Nw==&mid=2247488546&idx=1&sn=732ca84abf572196ddf76597fe096969&scene=21#wechat_redirect) 执行计划查看索引使用情况 

3、重点关注(一般情况下根据这4列就能找到索引问题)：

> 1、key（查看有没有使用索引） 
>
> 2、key_len（查看索引使用是否充分）
>
> 3、type（查看索引类型） 
>
> 4、Extra（查看附加信息：排序、临时表、where条件为false等）

4、根据上1步找出的索引问题优化sql 5、再回到第2步

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208131655894.png" alt="image-20220813165537763" style="zoom:50%;" />

**表结构优化**：

> 1、尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED 。
>
> 2、VARCHAR的长度只分配真正需要的空间 。
>
> 3、尽量使用TIMESTAMP而非DATETIME 。
>
> 4、单表不要有太多字段，建议在20以内。
>
> 5、避免使用NULL字段，很难查询优化且占用额外索引空间。字符串默认为''。

**读写分离**：

> 只在主服务器上写，只在从服务器上读。对应到数据库集群一般都是一主一从、一主多从。业务服务器把需要写的操作都写到主数据库中，读的操作都去从库查询。主库会同步数据到从库保证数据的一致性。一般 [读写分离](https://mp.weixin.qq.com/s?__biz=MzA5NDIzNzY1OQ==&mid=2735617707&idx=2&sn=6fd038b3385c1175a6efd4ef00543e35&scene=21#wechat_redirect) 的实现方式有两种：`代码封装`跟`数据库中间件`。

**分库分表**：[分库分表](https://mp.weixin.qq.com/s?__biz=MzkzNTEwOTAxMA==&mid=2247484479&idx=1&sn=97358231f0f7086f0056fc5bb4e8afff&scene=21#wechat_redirect)分为垂直和水平两个方式，一般是`先垂直后水平`。

> 1、`垂直分库`：将应用分为若干模块，比如订单模块、用户模块、商品模块、支付模块等等。其实就是微服务的理念。
>
> 2、`垂直分表`：一般将不常用字段跟数据较大的字段做拆分。
>
> 3、`水平分表`：根据场景选择什么字段作分表字段，比如淘宝日订单1000万，用userId作分表字段，数据查询支持到最近6个月的订单，超过6个月的做归档处理，那么6个月的数据量就是18亿，分1024张表，每个表存200W数据，hash(userId)%100找到对应表格。
>
> 4、`ID生成器`：[分布式ID](https://mp.weixin.qq.com/s?__biz=MzI4NjI1OTI4Nw==&mid=2247485459&idx=1&sn=9baf434bdeebe98be60bcde7df702f22&scene=21#wechat_redirect) 需要跨库全局唯一方便查询存储-检索数据，确保唯一性跟数字递增性。

目前主要流行的分库分表工具 就是`Mycat`和`sharding-sphere`。

**TiDB**：开源`分布式`数据库，结合了传统的 RDBMS 和NoSQL 的最佳特性。TiDB 兼容 MySQL，`支持无限的水平扩展`，具备强一致性和高可用性。TiDB 的目标是为 OLTP(Online Transactional Processing) 和 OLAP (Online Analytical Processing) 场景提供一站式的解决方案。TiDB 具备如下核心特点

> 1、支持 MySQL 协议（开发接入成本低）。
>
> 2、100% 支持事务（数据一致性实现简单、可靠）。
>
> 3、无限水平拓展（不必考虑分库分表），不停服务。
>
> 4、TiDB 支持和 MySQL 的互备。
>
> 5、遵循jdbc原则，学习成本低，强关系型，强一致性，不用担心主从配置，不用考虑分库分表，还可以无缝动态扩展。

适合：

> 1、原业务的 MySQL 的业务遇到单机容量或者性能瓶颈时，可以考虑使用 TiDB 无缝替换 MySQL。
>
> 2、大数据量下，MySQL 复杂查询很慢。
>
> 3、大数据量下，数据增长很快，接近单机处理的极限，不想分库分表或者使用数据库中间件等对业务侵入性较大、对业务有约束的 Sharding 方案。
>
> 4、大数据量下，有高并发实时写入、实时查询、实时统计分析的需求。5、有分布式事务、多数据中心的数据 100% 强一致性、auto-failover 的高可用的需求。

不适合：

> 1、单机 MySQL 能满足的场景也用不到 TiDB。
>
> 2、数据条数少于 5000w 的场景下通常用不到 TiDB，TiDB 是为大规模的数据场景设计的。
>
> 3、如果你的应用数据量小（所有数据千万级别行以下），且没有高可用、强一致性或者多数据中心复制等要求，那么就不适合使用 TiDB。

## 八大方案总结

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011824093.png" alt="image-20220801182359980" style="zoom:50%;" />

数据库的优化方案核心本质有三种：减少数据量、用空间换性能、选择合适的存储系统，这也对应了开篇讲解的慢的三个原因：数据总量、高负载、查找的时间复杂度。

这里大概解释下收益类型：短期收益，处理成本低，能紧急应对，久了则会有技术债务；长期收益则跟短期收益相反，短期内处理成本高，但是效果能长久使用，扩展性会更好。

静态数据意思是，相对改动频率比较低的，也无需过多联表的，where 过滤比较少。动态数据与之相反，更新频率高，通过动态条件筛选过滤。

### 减少数据量

减少数据量类型共有四种方案：数据序列化存储、数据归档、中间表生成、分库分表。

就如上面所说的，无论是哪种存储，数据量越少，自然查询性能就越高，随着数据量增多，资源的消耗（CPU、磁盘读写繁忙）、耗时也会越来越高。

目前市面上的 NoSQL 基本上都支持分片存储，所以其天然分布式写的能力从数据量上能得到非常的解决方案。

而关系型数据库，查找算法与存储结构是可以优化的空间比较少，因此咱们一般思考出发点只有从如何减少数据量的这个角度进行选择优化，因此本类型的优化方案主要针对关系型数据库进行处理。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011824247.png" alt="image-20220801182422175" style="zoom:50%;" />

### 数据归档

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011824988.png" alt="image-20220801182432919" style="zoom:67%;" />

注意点：别一次性迁移数量过多，建议低频率多次限量迁移。像 MySQL 由于删除数据后是不会释放空间的，可以执行命令 OPTIMIZE TABLE 释放存储空间，但是会锁表，如果存储空间还满足，可以不执行。

建议优先考虑该方案，主要通过数据库作业把非热点数据迁移到历史表，如果需要查历史数据，可新增业务入口路由到对应的历史表（库）。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011824813.png" alt="image-20220801182451728" style="zoom: 67%;" />

在数据库以序列化存储的方式，对于一些不需要结构化存储的业务来说是一种很好减少数据量的方式，特别是对于一些 M*N 的数据量的业务场景，如果以 M 作为主表优化，那么就可以把数据量维持最多是 M 的量级。

另外像订单的地址信息，这种业务一般是不需要根据里面的字段检索出来，也比较适合。

这种方案我认为属于一种临时性的优化方案，无论是从序列化后丢失了部份字段的查询能力，还是这方案的可优化性都是有限的。

### 中间表（结果表）

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011825007.png" alt="image-20220801182509936" style="zoom:67%;" />

中间表（结果表）其实就是利用调度任务把复杂查询的结果跑出来存储到一张额外的物理表，因为这张物理表存放的是通过跑批汇总后的数据，因此可以理解成根据原有的业务进行了高度的数据压缩。

以报表为例，如果一个月的源数据有数十万，我们通过调度任务以月的维度生成，那么等于把原有的数据压缩了几十万分之一。

接下来的季报和年报可以根据月报*N 来进行统计，以这种方式处理的数据，就算三年、五年甚至十年数据量都可以在接受范围之内，而且可以精确计算得到。那么数据的压缩比率是否越低越好？

下面有一段口诀：

- 字段越多，粒度越细，灵活性越高，可以以中间表进行不同业务联表处理。
- 字段越少，粒度越粗，灵活性越低，一般作为结果表查询出来。

### 数据序列化存储

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011825419.png" alt="image-20220801182529350" style="zoom:67%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011825410.png" alt="image-20220801182546317" style="zoom:67%;" />

### 分库分表

分库分表作为数据库优化的一种非常经典的优化方案，特别是在以前 NoSQL 还不是很成熟的年代，这个方案就如救命草一般的存在。

如今也有不少同行也会选择这种优化方式，但是从我角度来看，分库分表是一种优化成本很大的方案。

这里我有几个建议：

- 分库分表是实在没有办法的办法，应放到最后选择。
- 优先选择 NoSQL 代替，因为 NoSQL 诞生基本上为了扩展性与高性能。
- 究竟分库还是分表？量大则分表，并发高则分库
- 不考虑扩容，一部做到位。因为技术更新太快了，每 3-5 年一大变。

#### 拆分方式如下图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011826486.png" alt="image-20220801182600401" style="zoom:67%;" />

只要涉及到这个拆，那么无论是微服务也好，分库分表也好，拆分的方式主要分两种：垂直拆分、水平拆分。

垂直拆分更多是从业务角度进行拆分，主要是为了降低业务耦合度；此外以 SQL Server 为例，一页是 8KB 存储，如果在一张表里字段越多，一行数据自然占的空间就越大，那么一页数据所存储的行数就自然越少，那么每次查询所需要 IO 则越高因此性能自然也越慢。

因此反之，减少字段也能很好提高性能。之前我听说某些同行的表有 80 个字段，几百万的数据就开始慢了。

水平拆分更多是从技术角度进行拆分，拆分后每张表的结构是一模一样的，简而言之就是把原有一张表的数据，通过技术手段进行分片到多张表存储，从根本上解决了数据量的问题。

#### 路由方式如下图

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011829780.png" alt="image-20220801182930688" style="zoom: 67%;" />

进行水平拆分后，根据分区键（sharding key）原来应该在同一张表的数据拆解写到不同的物理表里，那么查询也得根据分区键进行定位到对应的物理表从而把数据给查询出来。

路由方式一般有三种区间范围、Hash、分片映射表，每种路由方式都有自己的优点和缺点，可以根据对应的业务场景进行选择。

区间范围根据某个元素的区间的进行拆分，以时间为例子，假如有个业务我们希望以月为单位拆分那么表就会拆分像 table_2022-04，这种对于文档型、ElasticSearch 这类型的 NoSQL 也适用，无论是定位查询，还是日后清理维护都是非常的方便的。

那么缺点也明显，会因为业务独特性导致数据不平均，甚至不同区间范围之间的数据量差异很大。

Hash 也是一种常用的路由方式，根据 Hash 算法取模以数据量均匀分别存储在物理表里，缺点是对于带分区键的查询依赖特别强。

如果不带分区键就无法定位到具体的物理表导致相关所有表都查询一次，而且在分库的情况下对于 Join、聚合计算、分页等一些 RDBMS 的特性功能还无法使用。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011829579.png" alt="image-20220801182908474" style="zoom:67%;" />

一般分区键就一个，假如有时候业务场景得用不是分区键的字段进行查询，那么难道就必须得全部扫描一遍？

其实可以使用分片映射表的方式，简单来说就是额外有一张表记录额外字段与分区键的映射关系。

举个例子，有张订单表，原本是以 UserID 作为分区键拆分的，现在希望用 OrderID 进行查询，那么得有额外得一张物理表记录了 OrderID 与 UserID 的映射关系。

因此得先查询一次映射表拿到分区键，再根据分区键的值路由到对应的物理表查询出来。

可能有些朋友会问，那这映射表是否多一个映射关系就多一张表，还是多个映射关系在同一张表。

我优先建议单独处理，如果说映射表字段过多，那跟不进行水平拆分时的状态其实就是一致的，这又跑回去的老问题。

### 用空间换性能

该类型的两个方案都是用来应对高负载的场景，方案有以下两种：`分布式缓存`、`一主多从`。

与其说这个方案叫用空间换性能，我认为用空间换资源更加贴切一些。因此两个方案的本质主要通数据冗余、集群等方式分担负载压力。

对于关系型数据库而言，因为他的 ACID 特性让它天生不支持写的分布式存储，但是它依然天然的支持分布式读。

![图片](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

#### 分布式缓存

缓存层级可以分好几种：客户端缓存、API 服务本地缓存和分布式缓存，咱们这次只聊分布式缓存。

一般我们选择分布式缓存系统都会优先选择 NoSQL 的键值型数据库，例如 Memcached、Redis，如今 Redis 的数据结构多样性，高性能，易扩展性也逐渐占据了分布式缓存的主导地位。

缓存策略也主要有很多种：Cache-Aside、Read/Wirte-Through、Write-Back，咱们用得比较多的方式主要 Cache-Aside。

具体流程可看下图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011828980.png" alt="image-20220801182826906" style="zoom:67%;" />

我相信大家对分布式缓存相对都比较熟悉了，但是我在这里还是有几个注意点希望提醒一下大家：

##### ①避免滥用缓存

缓存应该是按需使用，从 28 法则来看，80% 的性能问题由主要的 20% 的功能引起。滥用缓存的后果会导致维护成本增大，而且有一些数据一致性的问题也不好定位。

特别像一些动态条件的查询或者分页，key 的组装是多样化的，量大又不好用 keys 指令去处理，当然我们可以用额外的一个 key 把记录数据的 key 以集合方式存储，删除时候做两次查询，先查 Key 的集合，然后再遍历 Key 集合把对应的内容删除。

这一顿操作下来无疑是非常废功夫的，谁弄谁知道。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011828690.png" alt="image-20220801182811608" style="zoom:67%;" />

##### **②避免缓存击穿**

当缓存没有数据，就得跑去数据库查询出来，这就是缓存穿透。

假如某个时间临界点数据是空的例如周排行榜，穿透过去的无论查找多少次数据库仍然是空，而且该查询消耗 CPU 相对比较高，并发一进来因为缺少了缓存层的对高并发的应对，这个时候就会因为并发导致数据库资源消耗过高，这就是缓存击穿。数据库资源消耗过高就会导致其他查询超时等问题。

该问题的解决方案也简单，对于查询到数据库的空结果也缓存起来，但是给一个相对快过期的时间。有些同行可能又会问，这样不就会造成了数据不一致了么？

一般有数据同步的方案像分布式缓存、后续会说的一主多从、CQRS，只要存在数据同步这几个字，那就意味着会存在数据一致性的问题，因此如果使用上述方案，对应的业务场景应允许容忍一定的数据不一致。

##### **③不是所有慢查询都适用**

一般来说，慢的查询都意味着比较吃资源的（CPU、磁盘 I/O）。

举个例子，假如某个查询功能需要 3 秒时间，串行查询的时候并没什么问题，我们继续假设这功能每秒大概 QPS 为 100，那么在第一次查询结果返回之前，接下来的所有查询都应该穿透到数据库。

也就意味着这几秒时间有 300 个请求到数据库，如果这个时候数据库 CPU 达到了 100%，那么接下来的所有查询都会超时，也就是无法有第一个查询结果缓存起来，从而还是形成了缓存击穿。

#### 一主多从

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011827283.png" alt="image-20220801182759208" style="zoom:67%;" />

常用的分担数据库压力还有一种常用做法，就是读写分离、一主多从。咱们都是知道关系型数据库天生是不具备分布式分片存储的，也就是不支持分布式写，但是它天然的支持分布式读。

一主多从是部署多台从库只读实例，通过冗余主库的数据来分担读请求的压力，路由算法可有代码实现或者中间件解决，具体可以根据团队的运维能力与代码组件支持视情况选择。

一主多从在还没找到根治方案前是一个非常好的应急解决方案，特别是在现在云服务的年代，扩展从库是一件非常方便的事情，而且一般情况只需要运维或者 DBA 解决就行，无需开发人员接入。

当然这方案也有缺点，因为数据无法分片，所以主从的数据量完全冗余过去，也会导致高的硬件成本。从库也有其上限，从库过多了会主库的多线程同步数据的压力。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011827843.png" alt="image-20220801182743745" style="zoom:67%;" />

### 选择合适的存储系统

NoSQL 主要以下五种类型：键值型、文档型、列型、图型、搜素引擎，不同的存储系统直接决定了查找算法、存储数据结构，也应对了需要解决的不同的业务场景。NoSQL 的出现也解决了关系型数据库之前面临的难题（性能、高并发、扩展性等）。

例如，ElasticSearch 的查找算法是倒排索引，可以用来代替关系型数据库的低性能、高消耗的 Like 搜索（全表扫描）。而 Redis 的 Hash 结构决定了时间复杂度为 O(1)，还有它的内存存储，结合分片集群存储方式以至于可以支撑数十万 QPS。

因此本类型的方案主要有两种：CQRS、替换（选择）存储，这两种方案的最终本质基本是一样的主要使用合适存储来弥补关系型数据库的缺点，只不过切换过渡的方式会有点不一样。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011827631.png" alt="image-20220801182728530" style="zoom:50%;" />

#### CQRS

CQS（命令查询分离）指同一个对象中作为查询或者命令的方法，每个方法或者返回的状态，要么改变状态，但不能两者兼备。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011827399.png" alt="image-20220801182705320" style="zoom:67%;" />

讲解 CQRS 前得了解 CQS，有些小伙伴看了估计还没不是很清晰，我这里用通俗的话解释：某个对象的数据访问的方法里，要么只是查询，要么只是写入（更新）。

而 CQRS（命令查询职责分离）基于 CQS 的基础上，用物理数据库来写入（更新），而用另外的存储系统来查询数据。

因此我们在某些业务场景进行存储架构设计时，可以通过关系型数据库的 ACID 特性进行数据的更新与写入，用 NoSQL 的高性能与扩展性进行数据的查询处理。

这样的好处就是关系型数据库和 NoSQL 的优点都可以兼得，同时对于某些业务不适于一刀切的替换存储的也可以有一个平滑的过渡。

从代码实现角度来看，不同的存储系统只是调用对应的接口 API，因此 CQRS 的难点主要在于如何进行数据同步。

#### 数据同步方式

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011826956.png" alt="image-20220801182651880" style="zoom:67%;" />

一般讨论到数据同步的方式主要是分推和拉：

- 推指的是由数据变更端通过直接或者间接的方式把数据变更的记录发送到接收端，从而进行数据的一致性处理，这种主动的方式优点是实时性高。
- 拉指的是接收端定时的轮询数据库检查是否有数据需要进行同步，这种被动的方式从实现角度来看比推简单，因为推是需要数据变更端支持变更日志的推送的。

而推的方式又分两种：CDC（变更数据捕获）和领域事件。对于一些旧的项目来说，某些业务的数据入口非常多，无法完整清晰的梳理清楚，这个时候 CDC 就是一种非常好的方式，只要从最底层数据库层面把变更记录取到就可。

对于已经服务化的项目来说领域事件是一种比较舒服的方式，因为 CDC 是需要数据库额外开启功能或者部署额外的中间件，而领域事件则不需要，从代码可读性来看会更高，也比较开发人员的维护思维模式。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202208011826445.png" alt="image-20220801182639353" style="zoom:67%;" />

#### 替换（选择）存储系统

因为从本质来看该模式与 CQRS 的核心本质是一样的，主要是要对 NoSQL 的优缺点有一个全面认识，这样才能在对应业务场景选择与判断出一个合适的存储系统。

这里我像大家介绍一本书马丁.福勒《NoSQL精粹》，这本书我重复看了好几遍，也很好全面介绍各种 NoSQL 优缺点和使用场景。

当然替换存储的时候，我这里也有个建议：加入一个中间版本，该版本做好数据同步与业务开关，数据同步要保证全量与增加的处理，随时可以重来，业务开关主要是为了后续版本的更新做的一个临时型的功能，主要避免后续版本更新不顺利或者因为版本更新时导致的数据不一致的情况出现。

在跑了一段时间后，验证了两个不同的存储系统数据是一致的后，接下来就可以把数据访问层的底层调用替换了。如此一来就可以平滑的更新切换。



# MySQL 调优工具

对于正在运行的mysql，性能如何，参数设置的是否合理，账号设置的是否存在安全隐患，你是否了然于胸呢？

俗话说工欲善其事，必先利其器，定期对你的MYSQL数据库进行一个体检，是保证数据库安全运行的重要手段，因为，好的工具是使你的工作效率倍增！

今天和大家分享几个mysql 优化的工具，你可以使用它们对你的mysql进行一个体检，生成awr报告，让你从整体上把握你的数据库的性能情况。

## mysqltuner.pl

是mysql一个常用的数据库性能诊断工具，主要检查参数设置的合理性包括日志文件、存储引擎、安全建议及性能分析。针对潜在的问题，给出改进的建议。是mysql优化的好帮手。

在上一版本中，MySQLTuner支持MySQL / MariaDB / Percona Server的约300个指标。

> 项目地址：https://github.com/major/MySQLTuner-perl

### 1 下载

```apl
wget https://raw.githubusercontent.com/major/MySQLTuner-perl/master/mysqltuner.pl
```

### 2 使用

```apl
./mysqltuner.pl --socket /var/lib/mysql/mysql.sock
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207291749248.png" alt="image-20220729174948043" style="zoom: 67%;" />

### 3 报告分析

1）重要关注[!!]（中括号有叹号的项）例如[!!] Maximum possible memory usage: 4.8G (244.13% of installed RAM)，表示内存已经严重用超了。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207291751443.png" alt="image-20220729175153179" style="zoom:67%;" />

2）关注最后给的建议“Recommendations ”。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207291753467.png" alt="image-20220729175324240" style="zoom:67%;" />



## tuning-primer.sh

mysql的另一个优化工具，针于mysql的整体进行一个体检，对潜在的问题，给出优化的建议。

> 项目地址：https://github.com/BMDan/tuning-primer.sh

目前，支持检测和优化建议的内容如下：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207291753216.png" alt="image-20220729175344136" style="zoom:67%;" />

### 1 下载

```apl
wget https://launchpad.net/mysql-tuning-primer/trunk/1.6-r1/+download/tuning-primer.sh
```

### 2 使用

```apl
chmod 777  tuning-primer.sh
./tuning-primer.sh 
```

#### 2.3 报告分析

重点查看有红色告警的选项，根据建议结合自己系统的实际情况进行修改，例如：

![图片](https://mmbiz.qpic.cn/mmbiz_png/eQPyBffYbufSDbY2ShvbiahwmOibhicu41WJicIu2QBMDx1HdgQtsnyuiazQiawobo9JAUUgNXibD8KA5d7dNqWte4Nvg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

## pt-variable-advisor

pt-variable-advisor 可以分析MySQL变量并就可能出现的问题提出建议。

#### 1 安装

> https://www.percona.com/downloads/percona-toolkit/LATEST/

```apl
[root@localhost ~]#wget https://www.percona.com/downloads/percona-toolkit/3.0.13/binary/redhat/7/x86_64/percona-toolkit-3.0.13-re85ce15-el7-x86_64-bundle.tar
[root@localhost ~]#yum install percona-toolkit-3.0.13-1.el7.x86_64.rpm 
```

#### 2 使用

pt-variable-advisor是pt工具集的一个子工具，主要用来诊断你的参数设置是否合理。

```apl
[root@localhost ~]# pt-variable-advisor localhost --socket /var/lib/mysql/mysql.sock
```

#### 3 报告分析

重点关注有WARN的信息的条目，例如：

![图片](https://mmbiz.qpic.cn/mmbiz_png/eQPyBffYbufSDbY2ShvbiahwmOibhicu41WZLib3M64FM13TW1JliaYgMEicdy3iawJcxfM2mMb62sNSic0iconalMC04vw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

# SQL语句优化建议⭐

[书写高质量SQL的30条建议，这下够用了！ (qq.com)](https://mp.weixin.qq.com/s?__biz=MzU1Nzg4NjgyMw==&mid=2247484646&idx=1&sn=1b013813a8e9d398807a14d89f5b30ac&chksm=fc2fb8eecb5831f8f240ca5ef6ba9dba09c17d22275efb765a283bf49440844e8aacd1087565&mpshare=1&scene=23&srcid=0509DtlfP7DzJwBx9srE5jkB&sharer_sharetime=1652111561209&sharer_shareid=29b8a04db1dbd975e3bf4e9f47e7ac67#rd)

[Sql优化的15个小技巧，这也太实用了叭！ (qq.com)](https://mp.weixin.qq.com/s?__biz=MzU1Nzg4NjgyMw==&mid=2247495707&idx=2&sn=9d5dcad3f77ff9863410b834328e8156&chksm=fc2c4c13cb5bc505e5fdd587f59fcad2284d18ec5923b2e34afdfec1b9ba5e962dbcec3fd29b&mpshare=1&scene=23&srcid=04155SkwZaRosLw9KKkB37W0&sharer_sharetime=1650025089175&sharer_shareid=29b8a04db1dbd975e3bf4e9f47e7ac67#rd)

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211139668.png" alt="image-20220421113912586" style="zoom: 60%;" />

## 1 避免使用select * ⭐

“不要使用`SELECT *`”几乎已经成为了使用MySQL的一条金科玉律，就连《阿里Java开发手册》也明确表示不得使用`*`作为查询的字段列表，更是让这条规则拥有了权威的加持。

很多时候，我们写sql语句时，为了方便，喜欢直接使用`select *`，一次性查出表中所有列的数据。

```sql
select * from user where id=1;
```

在实际业务场景中，可能我们真正需要使用的只有其中一两列。查了很多数据，但是不用，白白浪费了数据库资源，比如：内存或者cpu。此外，多查出来的数据，通过网络IO传输的过程中，也会增加数据传输的时间。

还有一个最重要的问题是：`select *`不会走`覆盖索引`，会出现大量的`回表`操作，而从导致查询sql的性能很低。

- 不必要的磁盘I/O
- 加重网络时延
- 无法使用覆盖索引
- 可能拖慢JOIN连接查询

那么，如何优化呢？

```sql
select name,age from user where id=1;
```

sql语句查询时，只查需要用到的列，多余的列根本无需查出来。

## 2 union all和union

### 1 union

`UNION`在进行表链接后会筛选掉重复的记录，所以在表链接后会对所产生的结果集进行排序运算，删除重复的记录再返回结果。实际大部分应用中是不会产生重复的记录，最常见的是过程表与历史表`UNION`。如：

```sql
select username,tel from user
union
select departmentname from department
```

这个SQL在运行时先取出两个表的结果，再用排序空间进行排序删除重复的记录，最后返回结果集，如果表数据量大的话可能会导致用磁盘进行排序。推荐方案：采用`UNION ALL`操作符替代`UNION`，因为`UNION ALL`操作只是简单的将两个结果合并后就返回。

### 2 使用union all代替union

我们都知道sql语句使用`union`关键字后，可以获取排重后的数据。

而如果使用`union all`关键字，可以获取所有数据，包含重复的数据。

**反例：**

```sql
(select * from boys where id=2)
union
(select * from boys where id=4);
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5/202205081335686.png" alt="image-20220508133525607" style="zoom:80%;" />

排重的过程需要遍历、排序和比较，它更耗时，更消耗cpu资源。

所以如果能用union all的时候，尽量不用union。

**正例：**

```sql
(select * from boys where id=2)
union all
(select * from boys where id=4);
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5/202205081335595.png" alt="image-20220508133559518" style="zoom:80%;" />

除非是有些特殊的场景，比如union all之后，结果集中出现了重复数据，而业务场景中是不允许产生重复数据的，这时可以使用union。



## 3 小表驱动大表

小表驱动大表，也就是说用小表的数据集驱动大表的数据集。

假如有order和user两张表，其中order表有10000条数据，而user表有100条数据。

这时如果想查一下，所有有效的用户下过的订单列表。

可以使用`in`关键字实现：

```sql
select * from order
where user_id in (select id from user where status=1)
```

也可以使用`exists`关键字实现：

```sql
select * from order
where exists (select 1 from user where order.user_id = user.id and status=1)
```

前面提到的这种业务场景，使用in关键字去实现业务需求，更加合适。

为什么呢？

因为`如果sql语句中包含了in关键字，则它会优先执行in里面的子查询语句`，`然后再执行in外面的语句`。如果in里面的数据量很少，作为条件查询速度更快。

而如果sql语句中包含了exists关键字，它优先执行exists左边的语句（即主查询语句）。然后把它作为条件，去跟右边的语句匹配。如果匹配上，则可以查询出数据。如果匹配不上，数据就被过滤掉了。

这个需求中，order表有10000条数据，而user表有100条数据。order表是大表，user表是小表。如果order表在左边，则用in关键字性能更好。

总结一下：

- `in` 适用于左边大表，右边小表。
- `exists` 适用于左边小表，右边大表。

不管是用in，还是exists关键字，其核心思想都是用小表驱动大表。

## 4 批量插入，速度更快

如果你有一批数据经过业务处理之后，需要插入数据，该怎么办？

### 1 反例：多条提交

在循环中逐条插入数据。

```sql
INSERT INTO user (id,username) VALUES(1,'哪吒编程');

INSERT INTO user (id,username) VALUES(2,'妲己');
```

该操作需要多次请求数据库，才能完成这批数据的插入。

但众所周知，我们在代码中，每次远程请求数据库，是会消耗一定性能的。而如果我们的代码需要请求多次数据库，才能完成本次业务功能，势必会消耗更多的性能。

### 2 正例：批量提交

```sql
orderMapper.insertBatch(list):
```

提供一个批量插入数据的方法。

```sql
insert into order(id,code,user_id) values(123,'001',100),(124,'002',100);
```

这样只需要远程请求一次数据库，sql性能会得到提升，数据量越多，提升越大。

批量插入 SQL 执行效率高的主要原因是合并后日志量 MySQL 的 binlog 和 innodb 的事务让日志减少了，降低日志刷盘的数据量和频率，从而提高了效率

但需要注意的是，不建议一次批量操作太多的数据，如果数据太多数据库响应也会很慢。批量操作需要把握一个度，建议每批数据尽量控制在500以内。如果数据多于500，则分多批次处理。



## 5 多用limit

有时候，我们需要查询某些数据中的第一条，比如：查询某个用户下的第一个订单，想看看他第一次的首单时间。

`如果知道查询结果只有一条或者只要最大/最小一条记录，建议用limit 1`

- 加上limit 1后,只要找到了对应的一条记录,就不会继续向下扫描了,效率将会大大提高。
- 当然，如果name是唯一索引的话，是不必要加上limit 1了，因为limit的存在主要就是为了防止全表扫描，从而提高性能,如果一个语句本身可以预知不用全表扫描，有没有limit ，性能的差别并不大。

**反例：**

```sql
select id, create_date 
from order 
where user_id=123 
order by create_date asc;
```

根据用户id查询订单，按下单时间排序，先查出该用户所有的订单数据，得到一个订单集合。然后在代码中，获取第一个元素的数据，即首单的数据，就能获取首单时间。

```sql
List<Order> list = orderMapper.getOrderList();
Order order = list.get(0);
```

虽说这种做法在功能上没有问题，但它的效率非常不高，需要先查询出所有的数据，有点浪费资源。

那么，如何优化呢？

**正例：**

```sql
select id, create_date 
from order 
where user_id=123 
order by create_date asc 
limit 1;
```

使用`limit 1`，只返回该用户下单时间最小的那一条数据即可。

> 此外，在删除或者修改数据时，为了防止误操作，导致删除或修改了不相干的数据，也可以在sql语句最后加上limit。

例如：

```sql
update order set status=0,edit_time=now(3) 
where id>=100 and id<200 limit 100;
```

这样即使误操作，比如把id搞错了，也不会对太多的数据造成影响。

## 6 in中值太多

对于批量查询接口，我们通常会使用`in`关键字过滤出数据。比如：想通过指定的一些id，批量查询出用户信息。

sql语句如下：

```sql
select id,name from category
where id in (1,2,3...100000000);
```

如果我们不做任何限制，该查询语句一次性可能会查询出非常多的数据，很容易导致接口超时。

这时该怎么办呢？

```sql
select id,name from category
where id in (1,2,3...100)
limit 500;
```

可以在sql中对数据用limit做限制。

不过我们更多的是要在业务代码中加限制，伪代码如下：

```sql
public List<Category> getCategory(List<Long> ids) {
   if(CollectionUtils.isEmpty(ids)) {
      return null;
   }
   if(ids.size() > 500) {
      throw new BusinessException("一次最多允许查询500条记录")
   }
   return mapper.getCategoryList(ids);
}
```

还有一个方案就是：如果ids超过500条记录，可以分批用多线程去查询数据。每批只查500条记录，最后把查询到的数据汇总到一起返回。

不过这只是一个临时方案，不适合于ids实在太多的场景。因为ids太多，即使能快速查出数据，但如果返回的数据量太大了，网络传输也是非常消耗性能的，接口性能始终好不到哪里去。

## 7 增量查询

有时候，我们需要通过远程接口查询数据，然后同步到另外一个数据库。

**反例：**

```sql
select * from user;
```

如果直接获取所有的数据，然后同步过去。这样虽说非常方便，但是带来了一个非常大的问题，就是如果数据很多的话，查询性能会非常差。

这时该怎么办呢？

**正例：**

```sql
select * from user 
where id>#{lastId} and create_time >= #{lastCreateTime} 
limit 100;
```

按id和时间升序，每次只同步一批数据，这一批数据只有100条记录。每次同步完成之后，保存这100条数据中最大的id和时间，给同步下一批数据的时候用。

通过这种增量查询的方式，能够提升单次查询的效率。

## 8 高效的分页

有时候，列表页在查询数据时，为了避免一次性返回过多的数据影响接口性能，我们一般会对查询接口做分页处理。

在mysql中分页一般用的`limit`关键字：

```sql
select id,name,age 
from user limit 10,20;
```

如果表中数据量少，用limit关键字做分页，没啥问题。但如果表中数据量很多，用它就会出现性能问题。

比如现在分页参数变成了：

```sql
select id,name,age 
from user limit 1000000,20;
```

mysql会查到1000020条数据，然后丢弃前面的1000000条，只查后面的20条数据，这个是非常浪费资源的。

那么，这种海量数据该怎么分页呢？

优化sql：

```sql
-- 方案一 ：返回上次查询的最大记录(偏移量)
select id,name,age 
from user where id > 1000000 limit 20;
```

```sql
-- 方案二：order by + 索引
select id，name 
from employee 
order by id  
limit 10000，10
```

先找到上次分页最大的id，然后利用id上的索引查询。不过该方案，要求id是连续的，并且有序的。

还能使用`between`优化分页。

```sql
select id,name,age 
from user where id between 1000000 and 1000020;
```

需要注意的是between要在唯一索引上分页，不然会出现每页大小不一致的问题。

理由：

- 当偏移量最大的时候，查询效率就会越低，因为Mysql并非是跳过偏移量直接去取后面的数据，而是先把偏移量+要取的条数，然后再把前面偏移量这一段的数据抛弃掉再返回的。
- 如果使用优化方案一，返回上次最大查询记录（偏移量），这样可以跳过偏移量，效率提升不少。
- 方案二使用order by+索引，也是可以提高查询效率的。



## 9 用连接查询代替子查询

mysql中如果需要`从两张以上的表中查询出数据的话`，一般有两种实现方式：`子查询` 和 `连接查询`。

子查询的例子如下：

```sql
select * from order
where user_id in (select id from user where status=1)
```

子查询语句可以通过`in`关键字实现，一个查询语句的条件落在另一个select语句的查询结果中。程序先运行在嵌套在最内层的语句，再运行外层的语句。

子查询语句的优点是简单，结构化，如果涉及的表数量不多的话。

但缺点是mysql执行子查询时，需要创建临时表，查询完毕后，需要再删除这些临时表，有一些额外的性能消耗。

这时可以改成连接查询。具体例子如下：

```java
select o.* from order o
inner join user u on o.user_id = u.id
where u.status=1
```



## 10 join的表不宜过多

根据阿里巴巴开发者手册的规定，join表的数量不应该超过`3`个。

- 连表越多，编译的时间和开销也就越大。
- 把连接表拆开成较小的几个执行，可读性更高。
- 如果一定需要连接很多表才能得到数据，那么意味着糟糕的设计了。

**反例：**

```sqlite
select a.name,b.name.c.name,d.name
from a 
inner join b on a.id = b.a_id
inner join c on c.b_id = b.id
inner join d on d.c_id = c.id
inner join e on e.d_id = d.id
inner join f on f.e_id = e.id
inner join g on g.f_id = f.id
```

如果join太多，mysql在选择索引的时候会非常复杂，很容易选错索引。

并且如果没有命中中，nested loop join 就是分别从两个表读一行数据进行两两对比，复杂度是 n^2。

所以我们应该尽量控制join表的数量。

**正例：**

```sqlite
select a.name,b.name.c.name,a.d_name 
from a 
inner join b on a.id = b.a_id
inner join c on c.b_id = b.id
```

如果实现业务场景中需要查询出另外几张表中的数据，可以在a、b、c表中`冗余专门的字段`，比如：在表a中冗余d_name字段，保存需要查询出的数据。

不过我之前也见过有些ERP系统，并发量不大，但业务比较复杂，需要join十几张表才能查询出数据。

所以join表的数量要根据系统的实际情况决定，不能一概而论，尽量越少越好。

`当在SQL语句中连接多个表时,请使用表的别名，并把别名前缀于每一列上，这样语义更加清晰`。

反例：

```sql
select  * from A innerjoin B on A.deptId = B.deptId;
```

正例：

```sql
select  memeber.name,deptment.deptName 
from A member innerjoin B deptment 
on member.deptId = deptment.deptId;
```



## 11 join时要注意

我们在涉及到多张表联合查询的时候，一般会使用`join`关键字。

Inner join 、left join、right join，优`先使用Inner join`，如果是left join，左边表结果尽量小

> - Inner join 内连接，在两张表进行连接查询时，只保留两张表中完全匹配的结果集
> - left join 在两张表进行连接查询时，会返回左表所有的行，即使在右表中没有匹配的记录。
> - right join 在两张表进行连接查询时，会返回右表所有的行，即使在左表中没有匹配的记录。

都满足SQL需求的前提下，推荐优先使用Inner join（内连接），如果要使用left join，左边表数据结果尽量小，如果有条件的尽量放到左边处理。

而join使用最多的是left join和inner join。

- `left join`：求两个表的交集外加左表剩下的数据。
- `inner join`：求两个表交集的数据。

使用inner join的示例如下：

```sql
select o.id,o.code,u.name 
from order o 
inner join user u on o.user_id = u.id
where u.status=1;
```

如果两张表使用inner join关联，mysql会自动选择两张表中的小表，去驱动大表，所以性能上不会有太大的问题。

使用left join的示例如下：

```sql
select o.id,o.code,u.name 
from order o 
left join user u on o.user_id = u.id
where u.status=1;
```

如果两张表使用left join关联，mysql会默认用left join关键字左边的表，去驱动它右边的表。如果左边的表数据很多时，就会出现性能问题。

> 要特别注意的是在用left join关联查询时，左边要用小表，右边可以用大表。如果能用inner join的地方，尽量少用left join。



## 12 控制索引的数量

众所周知，`索引能够显著的提升查询sql的性能，但索引数量并非越多越好`。

因为表中新增数据时，需要同时为它创建索引，`而索引是需要额外的存储空间的，而且还会有一定的性能消耗`。

阿里巴巴的开发者手册中规定，单表的索引数量应该尽量控制在`5`个以内，并且单个索引中的字段数不超过`5`个。

mysql使用的B+树的结构来保存索引的，在insert、update和delete操作时，需要更新B+树索引。如果索引过多，会消耗很多额外的性能。

那么，问题来了，如果表中的索引太多，超过了5个该怎么办？

这个问题要辩证的看，`如果你的系统并发量不高，表中的数据量也不多，其实超过5个也可以，只要不要超过太多就行`。

但对于一些高并发的系统，请务必遵守单表索引数量不要超过5的限制。

那么，高并发系统如何优化索引数量？

`能够建联合索引，就别建单个索引，可以删除无用的单个索引`。

将部分查询功能迁移到其他类型的数据库中，比如：Elastic Seach、HBase等，在业务表中只需要建几个关键索引即可。



## 13 选择合理的字段类型

`char`表示固定字符串类型，该类型的字段存储空间的固定的，`会浪费存储空间`。

```sql
alter table order add column code char(20) NOT NULL;
```

`varchar`表示变长字符串类型，该类型的字段存储空间会根据实际数据的长度调整，`不会浪费存储空间`。

```sql
alter table order add column code varchar(20) NOT NULL;
```

如果是长度固定的字段，比如用户手机号，一般都是11位的，可以定义成char类型，长度是11字节。

但如果是企业名称字段，假如定义成char类型，就有问题了。

如果长度定义得太长，比如定义成了200字节，而实际企业长度只有50字节，则会浪费150字节的存储空间。

如果长度定义得太短，比如定义成了50字节，但实际企业名称有100字节，就会存储不下，而抛出异常。

所以建议将企业名称改成varchar类型，变长字段存储空间小，可以节省存储空间，而且对于查询来说，在一个相对较小的字段内搜索效率显然要高些。

我们在选择字段类型时，应该遵循这样的原则：

1. `能用数字类型，就不用字符串`，因为字符的处理往往比数字要慢。
2. `尽可能使用小的类型`，比如：用bit存布尔值，用tinyint存枚举值等。
3. `长度固定的字符串字段，用char类型`。
4. `长度可变的字符串字段，用varchar类型`。
5. `金额字段用decimal，避免精度丢失问题`。

还有很多原则，这里就不一一列举了。



## 14 提升group by的效率

我们有很多业务场景需要使用`group by`关键字，它主要的功能是去重和分组。

通常它会跟`having`一起配合使用，表示分组后再根据一定的条件过滤数据。

**反例：先分组，再过滤**

```sql
select user_id,user_name from order
group by user_id
having user_id <= 200;
```

这种写法性能不好，它先把所有的订单根据用户id分组之后，再去过滤用户id大于等于200的用户。

分组是一个相对耗时的操作，为什么我们不先缩小数据的范围之后，再分组呢？

**正例：先过滤，后分组**

```sql
select user_id,user_name from order
where user_id <= 200
group by user_id
```

理由：使用where条件在分组前，就把多余的数据过滤掉了，这样分组时效率就会更高一些。

> 其实这是一种思路，不仅限于group by的优化。我们的sql语句在做一些耗时的操作之前，应尽可能缩小数据范围，这样能提升sql整体的性能。



## 15 索引优化

sql优化当中，有一个非常重要的内容就是：`索引优化`。

`索引不宜太多，一般5个以内`

- 索引并不是越多越好，索引虽然提高了查询的效率，但是也降低了插入和更新的效率。
- insert或update时有可能会重建索引，所以建索引需要慎重考虑，视具体情况来定。
- 一个表的索引数最好不要超过5个，若太多需要考虑一些索引是否没有存在的必要。

很多时候sql语句，走了索引，和没有走索引，执行效率差别很大。所以索引优化被作为sql优化的首选。

日常开发写SQL的时候，尽量养成一个习惯吧。`用explain分析一下你写的SQL，尤其是走不走索引这一块`。

索引优化的第一步是：检查sql语句有没有走索引。

那么，如何查看sql走了索引没？

可以使用`explain`命令，查看mysql的执行计划。

例如：

```sql
explain select * from `order` where code='002';
```

结果：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5.13/202205200954607.png" alt="image-20220520095423509" style="zoom:80%;" />

如果字段类型是字符串，where时一定用引号括起来，否则索引失效

反例：

```sql
select * from user where userid =123;
```

正例：

```sql
select * from user where userid ='123';
```

理由：

- 为什么第一条语句未加单引号就不走索引了呢？这是因为不加单引号时，是字符串跟数字的比较，它们类型不匹配，MySQL会做隐式的类型转换，把它们转换为浮点数再做比较。

`使用联合索引时，注意索引列的顺序，一般遵循最左匹配原则`

表结构：（有一个联合索引idxuseridage，userId在前，age在后）

```sql
CREATE TABLE `user` (  
    `id` int(11) NOT NULL AUTO_INCREMENT,
    `userId` int(11) NOT NULL,
    `age` int(11) DEFAULT NULL,
    `name` varchar(255) NOT NULL,
    PRIMARY KEY (`id`),
    KEY `idx_userid_age` (`userId`,`age`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
```

反例：

```sql
select * from user where age = 10;
```

正例：

```sql
-- 符合最左匹配原则
select * from user where userid=10 and age =10；
-- 符合最左匹配原则
select * from user where userid =10;
```

理由：

- 当我们创建一个联合索引的时候，如(k1,k2,k3)，相当于创建了（k1）、(k1,k2)和(k1,k2,k3)三个索引，这就是最左匹配原则。
- 联合索引不满足最左原则，索引一般会失效，但是这个还跟Mysql优化器有关的。



`对查询进行优化，应考虑在where及order by涉及的列上建立索引，尽量避免全表扫描`

反例：

```sql
select * from user where address ='深圳' order by age ;
```

正例：

```sql
-- 添加索引
alter table user add index idx_address_age (address,age)
```



`在适当的时候，使用覆盖索引`

覆盖索引能够使得你的SQL语句不需要回表，仅仅访问索引就能够得到所有需要的数据，大大提高了查询效率。

反例：

```sql
-- like模糊查询，不走索引了
select * from user where userid like '%123%'
```

正例：

```sql
-- id为主键，那么为普通索引，即覆盖索引登场了
select id,name from user where userid like '%123%';
```



`索引不适合建在有大量重复数据的字段上，如性别这类型数据库字段`

因为SQL优化器是根据表中数据量来进行查询优化的，如果索引列有大量重复数据，Mysql查询优化器推算发现不走索引的成本更低，很可能就放弃索引了。



通过这几列可以判断索引使用情况，执行计划包含列的含义如下图所示：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211142914.png" alt="image-20220421114259809" style="zoom: 60%;" />

说实话，sql语句没有走索引，排除没有建索引之外，最大的可能性是索引失效了。

下面说说索引失效的常见原因：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022/202204211143266.png" alt="image-20220421114356191" style="zoom:67%;" />

如果不是上面的这些原因，则需要再进一步排查一下其他原因。

此外，你有没有遇到过这样一种情况：明明是同一条sql，只有入参不同而已。有的时候走的索引a，有的时候却走的索引b？

没错，有时候mysql会选错索引。

必要时可以使用`force index`来强制查询sql走某个索引。

至于为什么mysql会选错索引，后面有专门的文章介绍的，这里先留点悬念。



## 16 where使用事项⭐

新建一个user表，它有一个普通索引userId，表结构如下：

```sql
CREATE TABLE `user` (
    `id` int(11) NOT NULL AUTO_INCREMENT,
    `userId` int(11) NOT NULL,
    `age` int(11) NOT NULL,
    `name` varchar(255) NOT NULL,  
    PRIMARY KEY (`id`),
    KEY `idx_userId` (`userId`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

### 1 避免使用 or

假设现在需要查询userid为1或者年龄为18岁的用户，很容易有以下SQL

反例:

```sql
select * from user where userid=1 or age =18
```

正例：

```sql
-- 使用union all
select * from user where userid=1 
union all
select * from user where age = 18
-- 或者分开两条sql写：
select * from user where userid=1
select * from user where age = 18
```

理由：使用or可能会使索引失效，从而全表扫描。

> 对于or+没有索引的age这种情况，假设它走了userId的索引，但是走到age查询条件时，它还得全表扫描，也就是需要三步过程：全表扫描+索引扫描+合并 如果它一开始就走全表扫描，直接一遍扫描就完事。mysql是有优化器的，处于效率与成本考虑，遇到or条件，索引可能失效，看起来也合情合理。

### 2 避免返回多余的行

使用where条件限定要查询的数据，避免返回多余的行

假设业务场景是这样：查询某个用户是否是会员。曾经看过老的实现代码是这样。。。

反例：

```sql
select userId from user where isVip=1;
```

正例：

```sql
select userId from user where userId='userId' and isVip='1';
```

理由：需要什么数据，就去查什么数据，避免返回不必要的数据，节省开销。

### 3 避免表达式操作

应尽量避免在where子句中对字段进行表达式操作，这将导致系统放弃使用索引而进行全表扫描

反例：

```sql
select * from user where age-1 =10；
```

正例：

```sql
select * from user where age =11；
```

理由：虽然age加了索引，但是因为对它进行运算，索引直接迷路了。。。

### 4 避免使用!=或<>操作符

应尽量避免在where子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描。

反例：

```sql
select age,name  from user where age <>18;
```

正例：

```sql
//可以考虑分开两条sql写
select age,name  from user where age <18;
select age,name  from user where age >18;
```

理由：使用!=和<>很可能会让索引失效

### 5 考虑使用默认值代替null

反例：

```sql
select * from user where age is not null;
```

正例：

```sql
-- 设置0为默认值
select * from user where age>0;
```

理由：并不是说使用了is null 或者 is not null 就会不走索引了，这个跟mysql版本以及查询成本都有关。

> 如果mysql优化器发现，走索引比不走索引成本还要高，肯定会放弃索引，这些条件 `！=，>isnull，isnotnull`经常被认为让索引失效，其实是因为一般情况下，查询的成本高，优化器自动放弃索引的。
>
> 如果把null值，换成默认值，很多时候让走索引成为可能，同时，表达意思会相对清晰一点。



## 17 优化你的like语句

日常开发中，如果用到模糊关键字查询，很容易想到like，但是like很可能让你的索引失效。

### 1、反例

```sql
select * from citys where name like '%大连' (不使用索引)
select * from citys where name like '%大连%' (不使用索引)
```

### 2、正例

```sql
select * from citys where name like '大连%' (使用索引) 。
```

### 3、理由

- 首先尽量避免模糊查询，如果必须使用，不采用全模糊查询，也应尽量采用右模糊查询， 即`like ‘…%’`，是会使用索引的；
- 左模糊`like ‘%...’`无法直接使用索引，但可以利用`reverse + function index`的形式，变化成 `like ‘…%’`；
- 全模糊查询是无法优化的，一定要使用的话建议使用搜索引擎。



## 18 避免在索引列上使用内置函数

业务需求：查询最近七天内登陆过的用户(假设loginTime加了索引)

反例：

```sql
select userId,loginTime from loginuser where Date_ADD(loginTime,Interval 7 DAY) >=now();
```

正例：

```sql
explain  
select userId,loginTime from loginuser where  loginTime >= Date_ADD(NOW(),INTERVAL - 7 DAY);
```

理由：

- 索引列上使用mysql的内置函数，索引失效
- 如果索引列不加内置函数，索引还是会走的。 



## 19 慎用distinct关键字

distinct 关键字一般用来过滤重复记录，以返回不重复的记录。在查询一个字段或者很少字段的情况下使用时，给查询带来优化效果。但是在字段很多的时候使用，却会大大降低查询效率。

反例：

```sql
SELECT DISTINCT * from  user;
```

正例：

```sql
select DISTINCT name from user;
```

理由：

- 带distinct的语句cpu时间和占用时间都高于不带distinct的语句。因为当查询很多字段时，如果使用distinct，数据库引擎就会对数据进行比较，过滤掉重复数据，然而这个比较、过滤的过程会占用系统资源，cpu时间。



## 20 删除冗余和重复索引

反例：

```sql
KEY `idx_userId` (`userId`)    
KEY `idx_userId_age` (`userId`,`age`)
```

正例:

```sql
-- 删除userId索引，因为组合索引（A，B）相当于创建了（A）和（A，B）索引  
KEY `idx_userId_age` (`userId`,`age`)
```

理由：

- 重复的索引需要维护，并且优化器在优化查询的时候也需要逐个地进行考虑，这会影响性能的。

## 21. 删改使用limit或分批次

避免同时修改或删除过多数据，因为会造成cpu利用率过高，从而影响别人对数据库的访问。

### 反例

```sql
-- 一次删除10万或者100万+？
delete from user where id <100000;
-- 或者采用单一循环操作，效率低，时间漫长
for（User user：list）{   
	delete from user；
}
```

### 正例

```sql
-- 分批进行删除,如每次500
delete user where id<500
delete product where id>=500 and id<1000；
```

### 理由

#### 1、降低写错SQL的代价

清空表数据可不是小事情，一个手抖全没了，删库跑路？如果加limit，删错也只是丢失部分数据，可以通过binlog日志快速恢复的。

#### 2、SQL效率很可能更高

SQL中加了`limit 1`，如果第一条就命中目标`return`， 没有`limit`的话，还会继续执行扫描表。

#### 3、避免长事务

`delete`执行时,如果`age`加了索引，MySQL会将所有相关的行加写锁和间隙锁，所有执行相关行会被锁住，如果删除数量大，会直接影响相关业务无法使用。

#### 4、数据量大的话，容易把CPU打满

如果你删除数据量很大时，不加 limit限制一下记录数，容易把`cpu`打满，导致越删越慢。

#### 5、锁表

一次性删除太多数据，可能造成锁表，会有lock wait timeout exceed的错误，所以建议分批操作。



## 22 exist&in的合理利用

假设表A表示某企业的员工表，表B表示部门表，查询所有部门的所有员工，很容易有以下SQL:

```sql
select * from A where deptId in (select deptId from B);
```

这样写等价于：

> 先查询部门表B
>
> select deptId from B
>
> 再由部门deptId，查询A的员工
>
> select * from A where A.deptId = B.deptId

可以抽象成这样的一个循环：

```sql
List<> resultSet;    
for(int i=0;i<B.length;i++) {          
	for(int j=0;j<A.length;j++) {          
		if(A[i].id==B[j].id) {             
			resultSet.add(A[i]);             
			break;          
		}       
	}    
}
```

显然，除了使用in，我们也可以用exists实现一样的查询功能，如下：

```sql
select * from A where exists (select 1 from B where A.deptId = B.deptId);
```

因为exists查询的理解就是，先执行主查询，获得数据后，再放到子查询中做条件验证，根据验证结果（true或者false），来决定主查询的数据结果是否得意保留。

那么，这样写就等价于：

> select * from A,先从A表做循环
>
> select * from B where A.deptId = B.deptId,再从B表做循环.

同理，可以抽象成这样一个循环：

```sql
List<> resultSet ;
for(int i=0;i<A.length;i++) {
	for(int j=0;j<B.length;j++) {          
		if(A[i].deptId==B[j].deptId) {             
			resultSet.add(A[i]);            
			break;        
		}      
	}   
}
```

数据库最费劲的就是跟程序链接释放。假设链接了两次，每次做上百万次的数据集查询，查完就走，这样就只做了两次；相反建立了上百万次链接，申请链接释放反复重复，这样系统就受不了了。即mysql优化原则，就是小表驱动大表，小的数据集驱动大的数据集，从而让性能更优。

因此，我们要选择最外层循环小的，也就是，如果**B的数据量小于A，适合使用in，如果B的数据量大于A，即适合选择exist**。



## 23 尽量使用数字型字段

尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型

反例：

```sql
`king_id` varchar（20） NOT NULL COMMENT '守护者Id'
```

正例：

1. 主键（id）：`primary key`优先使用数值类型`int`，`tinyint`
2. 性别（sex）：0代表女，1代表男；数据库没有布尔类型，`mysql`推荐使用`tinyint`

```sql
`king_id` int(11) NOT NULL COMMENT '守护者Id'
```

1. 因为引擎在处理查询和连接时会逐个比较字符串中每一个字符；
2. 而对于数字型而言只需要比较一次就够了；
3. 字符会降低查询和连接的性能，并会增加存储开销；
4. 相对于数字型字段，字符型会降低查询和连接的性能，并会增加存储开销。

## 24 避免向客户端返回过多数据

假设业务需求是，用户请求查看自己最近一年观看过的直播数据。

反例：

```sql
-- 一次性查询所有数据回来
select * from LivingInfo where watchId =useId and watchTime >= Date_sub(now(),Interval 1 Y)
```

正例：

```sql
-- 分页查询
select * from LivingInfo 
where watchId = useId and watchTime>= Date_sub(now(),Interval 1 Y) 
limit offset，pageSize
-- 如果是前端分页，可以先查询前两百条记录，因为一般用户应该也不会往下翻太多页，
select * from LivingInfo 
where watchId =useId and watchTime>= Date_sub(now(),Interval 1 Y) 
limit 200;
```



## 25. update可能出现bug⭐

> 结论：不要使用update语句的影响行数做重要的业务判断!

业务系统中，使用update语句更新数据是再正常不过的场景，我们也经常通过update更新的行数，来做一些业务判断，类似下面的伪代码：(mybatis + mysql 场景)

```java
if (xxxMapper.updateByPrimaryKeySelective(entity)>0){
   //更新成功，做其它业务处理
}
```

但是这里有一个坑，mysql中update影响行数>0是有条件的

```sql
-- 第一次执行：Affected rows: 1
update course set name = "java" where id = 1;

-- 再次执行：Affected rows: 0
update course set name = "java" where id = 1;
```

也就是说，**当待更新的记录与原始记录旧值相同时，mysql其实并不会做任何更新。** 换言之，如果上游传过来的数据，与数据库本身的旧值相等，没有变化时，update语句影响行数为0。这与另一种场景："更新一条并不存在的记录，影响行数返回0" 无法区分。

## 26. 选择表合适存储引擎

- myisam：应用时以读和插入操作为主，只有少量的更新和删除，并且对事务的完整性，并发性要求不是很高的。
- InnoDB：事务处理，以及并发条件下要求数据的一致性。除了插入和查询外，包括很多的更新和删除。（InnoDB 有效地降低删除和更新导致的锁定）。对于支持事务的 InnoDB类 型的表来说，影响速度的主要原因是 AUTOCOMMIT 默认设置是打开的，而且程序没有显式调用 BEGIN 开始事务，导致每插入一条都自动提交，严重影响了速度。可以在执行 SQL 前调用 begin，多条 SQL 形成一个事物（即使 autocommit 打开也可以），将大大提高性能。

## 27. 使用varchar代替char

### 1、反例

```sql
`address` char(100) DEFAULT NULL COMMENT '地址'
```

### 2、正例

```sql
`address` varchar(100) DEFAULT NULL COMMENT '地址'
```

### 3、理由

1. `varchar`变长字段按数据内容实际长度存储，存储空间小，可以节省存储空间；
2. `char`按声明大小存储，不足补空格；
3. 其次对于查询来说，在一个相对较小的字段内搜索，效率更高；

### 4、技术延伸

1、`char`的长度是固定的，而`varchar2`的长度是可以变化的。

比如，存储字符串`“101”`，对于`char(10)`，表示你存储的字符将占10个字节（包括7个空字符），在数据库中它是以空格占位的，而同样的`varchar2(10)`则只占用3个字节的长度，10只是最大值，当你存储的字符小于10时，按实际长度存储。

2、`char`的效率比`varchar2`的效率稍高。

3、何时用`char`，何时用`varchar2`?

`char`和`varchar2`是一对矛盾的统一体，两者是互补的关系，`varchar2`比`char`节省空间，在效率上比`char`会稍微差一点，既想获取效率，就必须牺牲一点空间，这就是我们在数据库设计上常说的“以空间换效率”。

`varchar2`虽然比`char`节省空间，但是假如一个`varchar2`列经常被修改，而且每次被修改的数据的长度不同，这会引起“行迁移”现象，而这造成多余的I/O，是数据库设计中要尽力避免的，这种情况下用`char`代替`varchar2`会更好一些。`char`中还会自动补齐空格，因为你`insert`到一个`char`字段自动补充了空格的,但是`select`后空格没有删除，因此`char`类型查询的时候一定要记得使用`trim`，这是写本文章的原因。

如果开发人员细化使用`rpad()`技巧将绑定变量转换为某种能与`char`字段相比较的类型（当然，与截断`trim`数据库列相比，填充绑定变量的做法更好一些，因为对列应用函数`trim`很容易导致无法使用该列上现有的索引），可能必须考虑到经过一段时间后列长度的变化。如果字段的大小有变化，应用就会受到影响，因为它必须修改字段宽度。

正是因为以上原因，定宽的存储空间可能导致表和相关索引比平常大出许多，还伴随着绑定变量问题，所以无论什么场合都要避免使用char类型。

## 28. 清空表优先使用truncate

```sql
truncate table user;
```

1、`truncate table`在功能上与不带 `where`子句的 `delete`语句相同：二者均删除表中的全部行。但 `truncate table`比 `delete`速度快，且使用的系统和事务日志资源少。

2、`delete`语句每次删除一行，并在事务日志中为所删除的每行记录一项。`truncate table`通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。

3、`truncate table`删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 `drop table`语句。

4、对于由 `foreign key`约束引用的表，不能使用 `truncate table`，而应使用不带  `where`子句的 DELETE 语句。由于 `truncate table`不记录在日志中，所以它不能激活触发器。

5、`truncate table`不能用于参与了索引视图的表。

## 29. 索引问题⭐

### 组合索引

排序时应按照组合索引中各列的顺序进行排序，即使索引中只有一个列是要排序的，否则排序性能会比较差。

```sql
create index IDX_USERNAME_TEL on user(deptid,position,createtime);

select username,tel from user 
where deptid= 1 and position = 'java开发' 
order by deptid,position,createtime desc; 
```

实际上只是查询出符合 `deptid= 1 and position = 'java开发'`条件的记录并按createtime降序排序，但写成order by createtime desc性能较差。

### 复合索引最左特性

1、创建复合索引

```sql
ALTER TABLE employee ADD INDEX idx_name_salary (name,salary)
```

2、满足复合索引的最左特性，哪怕只是部分，复合索引生效

```sql
SELECT * FROM employee WHERE NAME='哪吒编程'
```

3、没有出现左边的字段，则不满足最左特性，索引失效

```sql
SELECT * FROM employee WHERE salary=5000
```

4、复合索引全使用，按左侧顺序出现 name,salary，索引生效

```sql
SELECT * FROM employee WHERE NAME='哪吒编程' AND salary=5000
```

5、虽然违背了最左特性，但MySQL执行SQL时会进行优化，底层进行颠倒优化

```sql
SELECT * FROM employee WHERE salary=5000 AND NAME='哪吒编程'
```

6、理由

复合索引也称为联合索引，当我们创建一个联合索引的时候，如(k1,k2,k3)，相当于创建了（k1）、(k1,k2)和(k1,k2,k3)三个索引，这就是最左匹配原则。

联合索引不满足最左原则，索引一般会失效。



# MySQL使用好习惯⭐

[MySQL中，21个写SQL的好习惯！ (qq.com)](https://mp.weixin.qq.com/s?__biz=MzU1Nzg4NjgyMw==&mid=2247487851&idx=2&sn=71f1903f135a58fd907a61440d93c213&chksm=fc2fad63cb5824756800116db1b0f8cc84990482c0a38ddf062fa7d8a9656e4e8ddf5cc15817&mpshare=1&scene=23&srcid=0510DW7fxxM6ivm9sya799xv&sharer_sharetime=1652115030717&sharer_shareid=29b8a04db1dbd975e3bf4e9f47e7ac67#rd)

## SQL性能优化

### 字段设置为NOT NULL

在可能的情况下，尽量把字段设置为NOT NULL,这样在将来执行查询的时候，数据库不用去比较NULL值。

尽量把所有列定义为NOT NULL

- **「NOT NULL列更节省空间」**，NULL列需要一个额外字节作为判断是否为 NULL 的标志位。
- **「NULL列需要注意空指针问题」**，NULL列在计算和比较的时候，需要注意空指针问题。

> 对于某些文本字段来说，例如“省份”或者“性别”，我们可以将他们定义为ENUM(枚举)类型。因为在MySQL中，ENUM类型被当做数值型数据来处理，而数值型数据被处理起来的速度要比文本类型要快得多。这样我们又可以提高数据库的性能。

### 字段为NULL问题

列中使用`NULL`值容易引发不受控制的事情发生,有时候还会严重托慢系统的性能.

- 对含有NULL值的列进行统计计算,eg. `count()`,`max()`,`min()`,结果并不符合我们的期望值.
- 干扰排序，分组,去重结果.
- 有的时候为了消除`NULL`带来的技术债务,我们需要在SQL中使用`IFNULL()`来确保结果可控,但是这使程序变得复杂.
- `NULL`值并是占用原有的字段空间存储,而是额外申请一个字节去标注,这个字段添加了`NULL`约束.(就像额外的标志位一样)

> 根据以上缺点,我们并不推荐在列中设置NULL作为列的默认值,你可以使用`NOT NULL`消除默认设置,使用`0`或者`''`空字符串来代替`NULL`.



### 查看SQL执行计划

1. 写完SQL先explain查看执行计划（SQL性能优化）

日常开发写SQL的时候，尽量养成这个好习惯呀：写完SQL后，用explain分析一下，尤其注意走不走索引。

```sql
explain select userid,name,age from user 
where userid =10086 or age =18;
```

### 优先使用索引列

2.写完SQL语句，检查where,order by,group by后面的列，`多表关联的列是否已加索引，优先考虑组合索引`。（SQL性能优化）

**「反例：」**

```sql
select * from user 
where address ='深圳' order by age;
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5/202205102154336.png" alt="image-20220510215443243" style="zoom: 67%;" />

**「正例：」**

```sql
-- 添加索引
alter table user add index idx_address_age (address,age)
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5/202205102155858.png" alt="image-20220510215501773" style="zoom: 67%;" />

### 类型隐式转换

10. `where后面的字段，留意其数据类型的隐式转换`（SQL性能优化）

**「反例：」**

```sql
-- userid 是varchar字符串类型
select * from user where userid =123;
```

**「正例：」**

```sql
select * from user where userid ='123';
```

**「理由：」**

- 因为不加单引号时，是字符串跟数字的比较，它们类型不匹配，MySQL会做隐式的类型转换，把它们转换为浮点数再做比较，最后导致索引失效

### 减少字段返回

13.减少不必要的字段返回，如使用select <具体字段> 代替 select * （SQL性能优化）

**「反例：」**

```sql
select * from employee;
```

**「正例：」**

```sql
select id，name from employee;
```

理由：

- 节省资源、减少网络开销。
- 可能用到覆盖索引，减少回表，提高查询效率。

### 使用Innodb存储引擎

14.所有表必须使用Innodb存储引擎（SQL规范优雅）

Innodb **「支持事务，支持行级锁，更好的恢复性」**，高并发下性能更好，所以呢，没有特殊要求（即Innodb无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用Innodb存储引擎

### 使用varchar代替 char

16. 尽量使用varchar代替 char。（SQL性能优化）

**「反例：」**

```sql
  `deptName` char(100) DEFAULT NULL COMMENT '部门名称'
```

**「正例：」**

```sql
`deptName` varchar(100) DEFAULT NULL COMMENT '部门名称'
```

理由：

- 因为首先变长字段存储空间小，可以节省存储空间。

### where后不跟函数计算

20. WHERE从句中不对列进行函数转换和表达式计算

假设loginTime加了索引

**「反例：」**

```sql
select userId,loginTime 
from loginuser
where Date_ADD(loginTime,Interval 7 DAY) >=now();
```

**「正例：」**

```sql
explain  select userId,loginTime 
from loginuser 
where  loginTime >= Date_ADD(NOW(),INTERVAL - 7 DAY);
```

**「理由：」**

- 索引列上使用mysql的内置函数，索引失效

### 批量执行

21.如果修改/更新数据过多，考虑批量进行。

反例：

```sql
delete from account  limit 100000;
```

正例：

```sql
for each(200次)
{
 delete from account  limit 500;
}
```

理由：

- 大批量操作会会造成主从延迟。
- 大批量操作会产生大事务，阻塞。
- 大批量操作，数据量过大，会把cpu打满。



## SQL规范优雅

### 使用外键

> 锁定表的方法可以维护数据的完整性，但是他却不能保证数据的关联性。这个时候我们可以使用外键。例如：外键可以保证每一条销售记录都指向某一个存在的客户。

在这里，外键可以把customerinfo表中的customerid映射到salesinfo表中customerid，任何一条没有办法合法customerid的记录都不会被跟新或插入到salesinfo中.

```sql
CREATE TABLE customerinfo(customerid int primary key) engine = innodb;

CREATE TABLE salesinfo( 
    salesid int not null,
    customerid  int not null, 
    primary key(customerid,salesid),
    foreign key(customerid) references customerinfo(customerid) on delete cascade
)engine = innodb;
```

> 注意例子中的参数“on delete cascade”.`该参数保证当customerinfo表中的一条客户记录也会被自动删除`。如果要在mysql中使用外键，一定要记住在创建表的时候将表的类型定义为事务安全表InnoDB类型。该类型不是mysql表的默认类型。定义的方法是在CREATE TABLE语句中加上engine=innoDB。



### 添加注释

1. 设计表的时候，所有表和字段都添加相应的注释（SQL规范优雅）

这个好习惯一定要养成啦，设计数据库表的时候，所有表和字段都添加相应的注释，后面更容易维护。

**「正例：」**

```sql
CREATE TABLE `account` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键Id',
  `name` varchar(255) DEFAULT NULL COMMENT '账户名',
  `balance` int(11) DEFAULT NULL COMMENT '余额',
  `create_time` datetime NOT NULL COMMENT '创建时间',
  `update_time` datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='账户表';
```

**「反例：」**

```sql
CREATE TABLE `account` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) DEFAULT NULL,
  `balance` int(11) DEFAULT NULL,
  `create_time` datetime NOT NULL ,
  `update_time` datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8;
```



2. SQL书写格式，关键字大小保持一致，使用缩进。（SQL规范优雅）

**「正例：」**

```sql
SELECT stu.name, sum(stu.score)
FROM Student stu
WHERE stu.classNo = '1班'
GROUP BY stu.name
```

**「反例：」**

```sql
SELECT stu.name, sum(stu.score) from Student stu WHERE stu.classNo = '1班' group by stu.name.
```

显然，统一关键字大小写一致，使用缩进对齐，会使你的SQL看起来更优雅~



3. INSERT语句标明对应的字段名称（SQL规范优雅）

**「反例：」**

```sql
insert into Student values ('666','捡田螺的小男孩','100');
```

**「正例：」**

```sql
insert into Student(student_id,name,score) values ('666','捡田螺的小男孩','100');
```

### 表必备字段主键

设计数据库表的时候，加上三个字段：主键，create_time,update_time。（SQL规范优雅）

**「反例：」**

```sql
CREATE TABLE `account` (
  `name` varchar(255) DEFAULT NULL COMMENT '账户名',
  `balance` int(11) DEFAULT NULL COMMENT '余额',
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='账户表';
```

**「正例：」**

```sql
CREATE TABLE `account` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键Id',
  `name` varchar(255) DEFAULT NULL COMMENT '账户名',
  `balance` int(11) DEFAULT NULL COMMENT '余额',
  `create_time` datetime NOT NULL COMMENT '创建时间',
  `update_time` datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='账户表';
```

**「理由：」**

- 主键一般都要加上的，没有主键的表是没有灵魂的
- 创建时间和更新时间的话，还是建议加上吧，详细审计、跟踪记录，都是有用的。



### 字符集utf8mb4

数据库和表的字符集尽量统一使用UTF8（SQL规范优雅）

尽量统一使用UTF8编码

- 可以避免乱码问题
- 可以避免，不同字符集比较转换，导致的索引失效问题

**「如果需要存储表情，那么选择utf8mb4来进行存储，注意它与utf-8编码的区别。」**

### 字段加注释

如果修改字段含义或对字段表示的状态追加时，需要及时更新字段注释。（SQL规范优雅）

这个点，是阿里开发手册中，Mysql的规约。你的字段，尤其是表示枚举状态时，如果含义被修改了，或者状态追加时，为了后面更好维护，需要即时更新字段的注释。

### 索引名规范

索引命名要规范，`主键索引名为 pk_ 字段名；唯一索引名为 uk _字段名 ；普通索引名则为 idx _字段名`。说明：pk\_即primary key；uk\_即unique key；idx_即index 的简称。



# MySQL防误操作

## 1.不要用聊天工具发sql语句

通常开发人员写好sql语句之后，习惯通过聊天工具，比如：qq、钉钉、或者腾讯通等，发给`团队老大`或者`DBA`在线上环境执行。但由于有些聊天工具，对部分`特殊字符`会自动转义，而且有些消息由于`内容太长`，会被自动分成多条消息。

这样会导致团队老大或者DBA复制出来的sql不一定是正确的。

他们需要手动拼接成一条`完整的sql`，有时甚至需要把转义后的字符替换回以前的特殊字符，无形之中会浪费很多额外的时间。即使最终sql拼接好了，真正执行sql的人，心里一定很虚。

所以，强烈建议你把要在线上执行的sql语句用`邮件`发过去，可以避免使用聊天工具的一些弊端，减少一些误操作的机会。而且有个存档，方便今后有问题的时候回溯原因。很多聊天工具只保留最近`7天`的历史记录，邮件会保留更久一些。

**别用聊天工具发sql语句！**

**别用聊天工具发sql语句！**

**别用聊天工具发sql语句！**

重要的事情说三遍，它真的能减少一些误操作。

## 2.把SQL语句压缩成一行

有些时候，开发人员写的sql语句很长，使用了各种`join`和`union`，而且使用美化工具，将一条sql变成了多行。在复制sql的时候，自己都无法确定sql是否完整。（为了装逼，把自己也坑了，哈哈哈）

线上环境有时候需要通过命令行连接数据库，比如：mysql，你把sql语句复制过来后，在命令行界面执行，由于屏幕滚动太快，这时根本无法确定sql是否都执行成功。

针对这类问题，强烈建议把sql语句`压缩成一行`，去掉多余的`换行符`和`空格`，可以有效的减少一些误操作。

`sql压缩工具`推荐使用：https://tool.lu/sql/



## 3.操作数据之前先select一下

需要特别说明的是：本文的操作数据主要指`修改`和`删除`数据。

很多时候，由于我们人为失误，把where条件写错了。但没有怎么仔细检查，就把sql语句直接执行了。影响范围小还好，如果影响几万、几十万，甚至几百万行数据，我们可能要哭了。

针对这种情况，在操作数据之前，把sql先改成`select count(*)`语句，比如：

```sql
update order set status=1 where status=0;
```

改成：

```sql
select count(*) from order where status=0;
```

查一下该sql执行后影响的记录行数，做到自己心中有数。也给自己一次测试sql是否正确，确认是否执行的机会。



## 4.操作数据SQL加limit

即使通过上面的`select`语句确认了sql语句没有问题，执行后影响的记录行数是对的。

也建议你不要立刻执行，建议在正在执行的时候，加上`limit` + `select出的记录行数`。例如：

```sql
update order set status=1 where status=0 limit 1000;
```

假设有一次性更新的数据太多，所有相关记录行都会被锁住，造成长时间的锁等待，而造成用户请求超时。

此外，加`limit`可以避免一次性操作太多数据，对服务器的cpu造成影响。

还有一个最重要的原因：加`limit`后，操作数据的影响范围是完全可控的。

删改加limit

`操作delete或者update语句，加个limit(SQL后悔药）`

在执行删除或者更新语句，尽量加上limit，以下面的这条 SQL 为例吧：

```sql
delete from euser where age > 30 limit 200;
```

因为加了limit 主要有这些好处：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.5/202205102146361.png" alt="image-20220510214613262" style="zoom:80%;" />

- **「降低写错SQL的代价」**, 你在命令行执行这个SQL的时候，如果不加limit，执行的时候一个**「不小心手抖」**，可能数据全删掉了，如果**「删错」**了呢？加了limit 200，就不一样了。删错也只是丢失200条数据，可以通过binlog日志快速恢复的。
- **「SQL效率很可能更高」**，你在SQL行中，加了limit 1，如果第一条就命中目标return， 没有limit的话，还会继续执行扫描表。
- **「避免了长事务」**，delete执行时,如果age加了索引，MySQL会将所有相关的行加写锁和间隙锁，所有执行相关行会被锁住，如果删除数量大，会直接影响相关业务无法使用。
- **「数据量大的话，容易把CPU打满」** ,如果你删除数据量很大时，不加 limit限制一下记录数，容易把cpu打满，导致越删越慢的。



## 5.update时更新修改人和修改时间

很多人写`update`语句时，如果要修改状态，就只更新状态，不管其他的字段。比如：

```sql
update order set status=1 where status=0;
```

这条sql会把`status`等于0的数据，全部更新成1。

后来发现业务逻辑有问题，不应该这么更新，需要把`status`状态回滚。

这时你可能会很自然想到这条sql：

```sql
update order set status=0 where status=1;
```

但仔细想想又有些不对。

这样不是会把有部分以前`status`就是1的数据更新成0？

这回真的要哭了，呜呜呜。

这时，送你一个好习惯：在更新数据的时候，同时更新`修改人`和`修改时间`字段。

```sql
# 这样在恢复数据时就能通过修改人和修改时间字段过滤数据了
update order set status=1,edit_date=now(),edit_user='admin' where status=0;
```

后面需要用到的`修改时间`通过这条sql语句可以轻松找到：

```sql
select edit_user ,edit_date from `order` order by edit_date desc limit 50;
```

> 当然，如果是高并发系统不建议这种批量更新方式，可能会锁表一定时间，造成请求超时。

有些同学可能会问：为什么要同时更新修改人，只更新修改时间不行吗？

主要有如下的原因：

1. 为了标识非正常用户操作，方便后面统计和定位问题。
2. 有些情况下，在执行sql语句的过程中，正常用户产生数据的修改时间跟你的可能一模一样，导致回滚时数据查多了。

## 6.多用逻辑删除，少用物理删除

在业务开发中，删除数据是必不可少的一种业务场景。

有些人开发人员习惯将表设计成`物理删除`，根据主键只用一条`delete`语句就能轻松搞定。

他们给出的理由是：`节省数据库的存储空间`。

想法是好的，但是现实很残酷。

如果有条极重要的数据删错了，想恢复怎么办？

此时只剩八个字：没有数据，恢复不了。（PS：或许通过binlog二进制文件可以恢复）

如果之前设计表的时候用的`逻辑删除`，上面的问题就变得好办了。删除数据时，只需`update`删除状态即可，例如：

```sql
update order set del_status=1,edit_date=now(),edit_user='admin' where id=123;
```

假如出现异常，要恢复数据，把该id的删除状态还原即可，例如：

```sql
update order set del_status=0,edit_date=now(),edit_user='admin' where id=123;
```

## 7.操作数据之前先做备份

如果只是修改了少量的数据，或者只执行了一两条sql语句，通过上面的`修改人`和`修改时间`字段，在需要回滚时，能快速的定位到正确的数据。

但是如果修改的记录行数很多，并且执行了多条sql，产生了很多`修改时间`。这时，你可能就要犯难了，没法一次性找出哪些数据需要回滚。

为了解决这类问题，可以将表做备份。

可以使用如下sql备份：

```sql
create table order_bak_2021031721 like`order`;
insert into order_bak_2021031721 select * from`order`;
```

先创建一张一模一样的表，然后把数据复制到新表中。

也可以简化成一条sql：

```sql
create table order_bak_2021031722 select * from`order`;
-- 创建表的同时复制数据到新表中。
```

> 此外，建议在表名中加上`bak`和`时间`，一方面是为了通过表名快速识别出哪些表是备份表，另一方面是为了备份多次时好做区分。因为有时需要执行多次sql才能把数据修复好，这种情况建议把表备份多次，如果出现异常，把数据回滚到最近的一次备份，可以节省很多重复操作的时间。

恢复数据时，把sql语句改成`select`语句，先在备份库找出相关数据，每条数据对应一条`update`语句，还原到老表中。

## 8.中间结果写入临时表

有时候，我们要先用一条sql查询出要更新的记录的id，然后通过这些id更新数据。

批量更新之后，发现不对，要回滚数据。但由于有些数据已更新，此时使用相同的sql相同的条件，却查不出上次相同的id了。

这时，我们开始慌了。

针对这种情况，我们可以先将第一次查询的id存入一张`临时表`，然后通过`临时表`中的id作为查询条件更新数据。

如果要恢复数据，只用通过`临时`表中的id作为查询条件更新数据即可。

修改完，3天之后，如果没有出现问题，就可以把`临时表`删掉了。

## 9.表名前面一定要带库名

我们在写sql时为了方便，习惯性不带数据库名称。比如：

```sql
update order set status=1,edit_date=now(),edit_user='admin' where status=0;
- 假如有多个数据库中有相同的表order，表结构一模一样，只是数据不一样。
```

由于执行sql语句的人一个小失误，进错数据库了。

```sql
use trade1;
```

然后执行了这条sql语句，结果悲剧了。

有个非常有效的预防这类问题的方法是加`数据库名`：

```sql
update `trade2`.`order` set status=1,edit_date=now(),edit_user='admin' where status=0;
-- 这样即使执行sql语句前进错数据库了，也没什么影响。
```

## 10.字段增删改的限制

很多时候，我们少不了对表字段的操作，比如：新加、修改、删除字段，但每种情况都不一样。

### 新加的字段一定要允许为空

新加的字段一定要允许为空。为什么要这样设计呢？

正常情况下，如果程序新加了字段，一般是先在数据库中加字段，然后再发程序的最新代码。

为什么是这种顺序？

因为如果先发程序，然后在数据库中加字段。在该程序刚部署成功，但数据库新字段还没来得及加的这段时间内，最新程序中，所有使用了新加字段的增删改查sql都会报字段不存在的异常。

好了，就按先在数据库中加字段，再发程序的顺序。

如果数据库中新加的字段非空，最新的程序还没发，线上跑的还是老代码，这时如果有`insert`操作，就会报字段不能为空的异常。因为新加的非空字段，老代码是没法赋值的。

所以说新加的字段一定要允许为空。

除此之外，这种设计更多的考虑是为了程序发布失败时的回滚操作。如果新加的字段允许为空，则可以不用回滚数据库，只需回滚代码即可，是不是很方便？

### 不允许删除字段

删除字段是不允许的，特别是必填字段一定不能删除。

为什么这么说？

假设开发人员已经把程序改成不使用删除字段了，接下来如何部署呢？

1. 如果先把程序部署好了，还没来得及删除数据库相关表字段。当有`insert`请求时，由于数据库中该字段是必填的，会报必填字段不能为空的异常。
2. 如果先把数据库中相关表字段删了，程序还没来得及发。这时所有涉及该删除字段的`增删改查`，都会报字段不存在的异常。

所以，线上环境必填字段一定不能删除的。

### 根据实际情况修改字段

修改字段要分为这三种情况：

#### **1.修改字段名称**

修改字段名称也不允许，跟删除必填字段的问题差不多。

1. 如果把程序部署好了，还没来得及修改数据库中表字段名称。这时所有涉及该字段的`增删改查`，都会报字段不存在的异常。
2. 如果先把数据库中字段名称改了，程序还没来得及发。这时所有涉及该字段的`增删改查`，同样也会报字段不存在的异常。

所以，线上环境字段名称一定不要修改。

#### **2.修改字段类型**

修改字段类型时一定要兼容之前的数据。例如：

1. tinyint改成int可以，但int改成tinyint要仔细衡量一下。
2. varchar改成text可以，但text改成varchar要仔细衡量一下。

#### **3.修改字段长度**

字段长度建议改大，通常情况下，不建议改小。如果一定要改小，要先确认该字段可能会出现的最大长度，避免`insert`操作时出现字段太长的异常。

此外，建议改大也需要设置一个合理的长度，避免数据库资源浪费。



# MySQL配置优化⭐

既然谈到优化，一定想到要从多个维度进行优化。

这里的优化维度有四个：**SQL语句及索引**、**表结构设计**、**系统配置**、**硬件配置**

其中SQL语句相关的优化手段是最为重要的。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img/image-20220314174530735.png" alt="image-20220314174530735" style="zoom:67%;" />

## 1. 连接配置优化

处理连接是MySQL客户端和MySQL服务端亲热的第一步，第一步都迈不好，也就别谈后来的故事了。

正常情况下，客户端与server层如果只有**一条**连接，那么在执行sql查询之后，只能阻塞等待结果返回，如果有大量查询同时并发请求，那么**后面的请求都需要等待前面的请求执行完成**后，才能开始执行。

如果我们能**多建几条连接**，那么请求就可以并发执行，后面的连接就不用等那么久了。

既然连接是双方的事情，我们自然从服务端和客户端两个方面来进行优化喽。

### 1. 服务端配置

服务端需要做的就是尽可能地多接受客户端的连接，或许你遇到过`error 1040: Too many connections`的错误？就是服务端的胸怀不够宽广导致的，格局太小！

我们可以从两个方面解决连接数不够的问题：

1. `增加可用连接数`，修改环境变量`max_connections`，默认情况下服务端的最大连接数为`151`个

```mysql
mysql> show variables like 'max_connections';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| max_connections | 151   |
+-----------------+-------+
1 row in set (0.01 sec)
```

```sql
show global status like 'max_used_connections';
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.11/202206221522452.png" alt="image-20220622152230346" style="zoom:67%;" />

`可以看到服务器响应的最大连接数为1，远远低于MySQL 服务器允许的最大连接数值`

直接在my.ini中新增即可

```apl
# 设置用户连接数
max_connections=500
```

> 对于 MySOL 服务器最大连接数值的设置范围比较理想的是:服务器响应的最大连接数值占服务器上限连接数值的比例值在10%以上，如果在10%以下，说明MySQL 服务器最大连接上限值设置过高
> Max used connections / max connections * 100%= 1/151 *100%~ 0.6%
> 可以看到占比远低于10%。

> 注意:这是本地测试服务器，结果值没有太大的实际意义，将来可以根据实际情况设置连接数的上限值。

及时释放不活动的连接，系统默认的客户端超时时间是28800秒（8小时），我们可以把这个值调小一点

```mysql
mysql> show variables like 'wait_timeout';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| wait_timeout  | 28800 |
+---------------+-------+
1 row in set (0.01 sec)
```

> MySQL有非常多的配置参数，并且大部分参数都提供了默认值，默认值是MySQL作者经过精心设计的，完全可以满足大部分情况的需求，不建议在不清楚参数含义的情况下贸然修改。



### 2. 客户端优化

客户端能做的就是尽量减少和服务端建立连接的次数，已经建立的连接能凑合用就凑合用，别每次执行个SQL语句都创建个新连接，服务端和客户端的资源都吃不消啊。

解决的方案就是使用**连接池**来复用连接。

常见的数据库连接池有`DBCP`、`C3P0`、阿里的`Druid`、`Hikari`，前两者用得很少了，后两者目前如日中天。

但是需要注意的是连接池并不是越大越好，比如`Druid`的默认最大连接池大小是8，`Hikari`默认最大连接池大小是10，盲目地加大连接池的大小，系统执行效率反而有可能降低。为什么？

对于每一个连接，服务端会创建一个单独的线程去处理，连接数越多，服务端创建的线程自然也就越多。而线程数超过CPU个数的情况下，CPU势必要通过分配时间片的方式进行线程的上下文切换，频繁的上下文切换会造成很大的性能开销。

[Hikari官方](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fbrettwooldridge%2FHikariCP%2Fwiki%2FAbout-Pool-Sizing)给出了一个`PostgreSQL`数据库连接池大小的建议值公式，`CPU核心数*2+1`。假设服务器的CPU核心数是4，把连接池设置成9就可以了。这种公式在一定程度上对其他数据库也是适用的，大家面试的时候可以吹一吹。



### 3. 设置超时参数

有时候，由于业务的复杂性，在JVM中拼装一些数据，会造成资源的极大浪费。举个例子，从MySQL中查询出一个List，然后在代码里循环查询数据库，进行一些字段的填充。

这种数据组装方式，除了执行效率的问题，往往会有更多的内存占用，对整个JVM计算节点造成了比较大的压力，有时候甚至造成内存溢出。于是，一些比较**牛X**的开发人员，使用非常复杂的SQL，来把这些耗时的操作，转嫁给数据库。

可怜的数据库，成了最后一道屏障。谁让数据库的配置普遍都比较高呢？活该。

但是可惜的是，数据库完成这些动作，同样要经历耗时的操作。Java线程等的不耐烦了，就会对用户直接返回超时，懵逼的用户会在这种情况下，再次发起重试。

要知道，Java端超时，并不代表发起的请求就结束运行了，这在一些高并发的场景中，可怜的数据库会空跑一些耗时的慢查询，计算着一些无人能知的数据。

对于mysql数据库，有两个可用的参数：

**connectTimeout**

默认值：0，单位：毫秒
配置连接超时时间，通过 Socket 对象的 connect(SocketAddress endpoint, int timeout) 方法来配置

当设置 connectTimeout=1时，在建立数据库连接时即发生了错误。`该值在建立数据库连接时有效`。

**socketTimeout**

默认值：0，单位：ms
配置socket的超时时间，通过 Socket 对象的 setSoTimeout(int timeout) 方法来配置

构造一个慢查，并设置socketTimeout小于慢查的时间，如：socketTimeout=1000&connectTimeout=1000，慢查执行时间100S。

示例：

```apl
jdbc:mysql://xxx.xx.xxx.xxx:3306/database?connectTimeout=60000&socketTimeout=60000
```





## 2. 保证从内存读取

MySQL 会在内存中保存一定的数据，通过 **LRU（最近最少使用）算法**将不常访问的数据保存在硬盘文件中。尽可能的扩大内存中的数据量，将数据保存在内存中，从内存中读取数据，可以提升 MySQL 性能。

LRU 算法针对的是 MySQL 内存中的结构，这里有个区域叫 **Buffer Pool（缓冲池）** 作为数据读写的缓冲区域。把这个区域进行相应的扩大即可提升性能，当然这个参数要针对服务器硬件的实际情况进行调整。

我们在前面的数据库查询流程里，提到了进了innodb之后，会有一层内存buffer pool，用于将磁盘数据页加载到内存页中，只要查询到buffer pool里有，就可以直接返回，否则就要走磁盘IO，那就慢了。

> 也就是说，如果我的buffer pool 越大，那我们能放的数据页就越多，相应的，sql查询时就更可能命中buffer pool，那查询速度自然就更快了。

### 配置buffer_pool大小

通过以下命令可以查看相应的BufferPool的相关参数：

```sql
show global status like 'innodb_buffer_pool_pages_%';
```

<img src="https://mmbiz.qpic.cn/mmbiz_png/mngWTkJEOYKht9kicH7icGHOvAiav2hRhq8UZdzTzGibwsn2C6GpunpkEQ3NS4icv6OhN1SdkURo6c0uySNibwInvP5g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" />

输入以下命令可以查看 BufferPool 的大小：

```sql
show variables like "%innodb_buffer_pool_size%";
```

在这里我们可以修改这个参数的值，如果该服务器是 MySQL 专用的服务器，我们可以 **修改为总内存的 60%~80%** ，当然不能影响系统程序的运行。

这个参数是只读的，可以在 MySQL 的配置文件（my.cnf 或 my.ini）中进行修改。Linux 的配置文件为 **my.cnf**。

```sql
# 修改缓冲池大小为750M
innodb_buffer_pool_size = 750M
```

### 判断buffer_pool是否太小

### 怎么知道buffer pool是不是太小了？

这个我们可以看**buffer pool的缓存命中率**。

```sql
show status like 'Innodb_buffer_pool_%';
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.11/202206071452329.png" alt="image-20220607145201244" style="zoom:80%;" />

通过 `show status like 'Innodb_buffer_pool_%';`可以看到跟buffer pool有关的一些信息。

`Innodb_buffer_pool_read_requests`表示读请求的次数。

`Innodb_buffer_pool_reads` 表示从物理磁盘中读取数据的请求次数。

所以buffer pool的命中率就可以这样得到：

```sql
buffer pool 命中率 = 1 - (Innodb_buffer_pool_reads/Innodb_buffer_pool_read_requests) * 100%
```

比如我上面截图里的就是，1 - (437/3048) = 86.98%。

一般情况下**buffer pool命中率**都在`99%`以上，如果低于这个值，才需要考虑加大innodb buffer pool的大小。

当然，还可以把这个命中率做到**监控**里，这样半夜sql变慢了，早上上班还能定位到原因，就很舒服。



## 3. 降低磁盘的写入次数

增大redo log，减少落盘次数：

redo log是重做日志，用于保证数据的一致，减少落盘相当于减少了系统 IO 操作。

```sql
innodb_log_file_size 设置为 0.25 * innodb_buffer_pool_size
```

通用查询日志、慢查询日志可以不开 ，binlog 可开启

通用查询和慢查询日志也是要落盘的，可以根据实际情况开启，如果不需要使用的话就可以关掉。binlog 用于恢复和主从复制，这个可以开启。

```sql
# 慢查询日志
show variables like 'slow_query_log%';
# 通用查询日志
show variables like '%general%';
# 错误日志
show variables like '%log_error%';
# 二进制日志
show variables like '%binlog%';
```

innodb_flush_log_at_trx_commit 设置为 0 或 2，默认是1

```sql
show variables like '%innodb_flush_log_at_trx_commit%';
```

对于不需要强一致性的业务，可以设置为 0 或 2。

- 0：每隔 1 秒写日志文件和刷盘操作（写日志文件 LogBuffer --> OS cache，刷盘 OS cache --> 磁盘文件），最多丢失 1 秒数据
- 1：事务提交，立刻写日志文件和刷盘，数据不丢失，但是会频繁 IO 操作
- 2：事务提交，立刻写日志文件，每隔 1 秒钟进行刷盘操作

## 其余参数调优

### back_log

back_log值可以指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySQL的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。**可以从默认的50升至500**。

```sql
-- 默认查找是80
show variables like '%back_log%';
```

### wait_timeout

数据库连接闲置时间，闲置连接会占用内存资源。可以从默认的8小时减到半小时。

查询显示28800秒，进行转换就是8小时

```sql
show variables like '%wait_timeout%';
```

### max_user_connection

最大连接数，默认为0无上限，最好设一个合理上限。

```sql
show variables like '%max_user_connection%';
```

### thread_concurrency

并发线程数，设为CPU核数的两倍。

```sql
-- 默认是33
show variables like '%thread_concurrency%';
```

### skip_name_resolve

禁止对外部连接进行DNS解析，消除DNS解析时间，但需要所有远程主机用IP访问。

```sql
-- 默认OFF
show variables like '%skip_name_resolve%';
```

### key_buffer_size

索引块的缓存大小，增加会提升索引处理速度，对MyISAM表性能影响最大。对于内存4G左右，可设为256M或384M，通过查询show status like 'key_read%'，保证key_reads / key_read_requests在0.1%以下最好。

```sql
-- 8388608
show variables like '%key_buffer_size%';
```

### innodb_buffer_pool_size

缓存数据块和索引块，对InnoDB表性能影响最大。保证 (Innodb_buffer_pool_read_requests – Innodb_buffer_pool_reads) / Innodb_buffer_pool_read_requests越高越好。

```sql
-- 8388608
show variables like '%innodb_buffer_pool_size%';
```

### innodb_additional_mem_pool_size

> InnoDB存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，当数据库对象非常多的时候，适当调整该参数的大小以确保所有数据都能存放在内存中提高访问效率，当过小的时候，MySQL会记录Warning信息到数据库的错误日志中，这时就需要该调整这个参数大小。

### innodb_log_buffer_size

InnoDB存储引擎的事务日志所使用的缓冲区，一般来说不建议超过32MB。

```sql
-- 1048576
show variables like '%innodb_log_buffer_size%';
```

### query_cache_size

> 缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集，所以仅仅只能针对select语句。当某个表的数据有任何变化，都会导致所有引用了该表的select语句在Query Cache中的缓存数据失效。所以，当我们数据变化非常频繁的情况下，使用Query Cache可能得不偿失。根据命中率(Qcache_hits/(Qcache_hits+Qcache_inserts)*100))进行调整，一般不建议太大，256MB可能已经差不多了，大型的配置型静态数据可适当调大。可以通过命令show status like 'Qcache_%'查看目前系统Query catch使用大小。

```sql
-- 1048576
show variables like '%query_cache_size%';
```

### read_buffer_size

> MySQL读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySQL会为它分配一段内存缓冲区。如果对表的顺序扫描请求非常频繁，可以通过增加该变量值以及内存缓冲区大小来提高其性能。

```sql
-- 8192
show variables like '%read_buffer_size%';
```

### sort_buffer_size

> MySQL执行排序使用的缓冲大小。如果想要增加ORDER BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。如果不能，可以尝试增加sort_buffer_size变量的大小。

```sql
-- 262144
show variables like '%sort_buffer_size%';
```

### read_rnd_buffer_size

> MySQL的随机读缓冲区大小。当按任意顺序读取行时(例如按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySQL会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySQL会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。

```sql
-- 默认1
show variables like '%read_rnd_buffer_size%';
```

### record_buffer

每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，可能想要增加该值。

### thread_cache_size

> 保存当前没有与连接关联但是准备为后面新的连接服务的线程，可以快速响应连接的线程请求而无需创建新的。

```sql
-- 默认是10
show variables like '%thread_cache_size%';
```



# MySQL表结构优化⭐

## 表设计优化

### 尽量避免使用NULL

NULL在MySQL中不好处理，存储需要额外空间，运算也需要特殊的运算符，含有NULL的列很难进行查询优化

应当指定列为not null，用0、空串或其他特殊的值代替空值，比如定义为int not null default 0

非空字段尽量设置成`NOT NULL`，并提供默认值，或者使用特殊值代替`NULL`。

因为`NULL`类型的存储和优化都会存在性能不佳的问题，具体原因在这里就不展开了。

### 最小数据长度

越小的数据类型长度通常在磁盘、内存和CPU缓存中都需要更少的空间，处理起来更快

MySQL提供了6种整数类型，分别是

- **tinyint**
- **smallint**
- **mediumint**
- **int**
- **integer**
- **bigint**

不同的存储类型的最大存储范围不同，占用的存储的空间自然也不同。

例如，是否被删除的标识，建议选用`tinyint`，而不是`bigint`。

### 使用合适的数据类型

原则：更小通常更好，简单就好，所有字段都得有默认值，尽量避免 NULL。

例如：数据库表设计时候更小的占磁盘空间尽可能使用更小的整数类型。(mediumint 就比 int 更合适)

比如时间字段：datetime 和 timestamp。datetime 占用8个字节，timestamp 占用4个字节，只用了一半。而 timestamp 表示的范围是 1970—2037 适合做更新时间。

MySQL可以很好的支持大数据量的存取，但是一般说来，数据库中的表越小，在它上面执行的查询也就会越快。

因此，在创建表的时候，为了获得更好的性能，我们可以将表中字段的宽度设得尽可能小。

例如：在定义邮政编码这个字段时，如果将其设置为 CHAR(255)，显然给数据库增加了不必要的空间。甚至使用VARCHAR 这种类型也是多余的，因为 CHAR(6) 就可以很好的完成任务了。

同样的，如果可以的话，我们应该使用 MEDIUMINT 而不是 BIGIN 来定义整型字段，应该尽量把字段设置为 NOT NULL，这样在将来执行查询的时候，数据库不用去比较 NULL 值。

对于某些文本字段，例如“省份”或者“性别”，我们可以将它们定义为 ENUM 类型。因为在 MySQL 中，ENUM 类型被当作数值型数据来处理，而数值型数据被处理起来的速度要比文本类型快得多。这样，我们又可以提高数据库的性能。

### 尽量少定义 text 类型

text 类型的查询效率很低，如果必须要使用 text 定义字段，可以把此字段分离成子表，需要查询此字段时使用联合查询，这样可以提高主表的查询效率

### 适当分表、分库策略

分表是指当一张表中的字段更多时，可以尝试将一张大表拆分为多张子表，把使用比较高频的主信息放入主表中，其他的放入子表，这样我们大部分查询只需要查询字段更少的主表就可以完成了，从而有效的提高了查询的效率

分库是指将一个数据库分为多个数据库。比如我们把一个数据库拆分为了多个数据库，一个主数据库用于写入和修改数据，其他的用于同步主数据并提供给客户端查询，这样就把一个库的读和写的压力，分摊给了多个库，从而提高了数据库整体的运行效率

### 字符类型

你是不是直接把所有字符串的字段都设置为`varchar`格式了？甚至怕不够，还会直接设置成`varchar(1024)`的长度？

如果不确定字段的长度，肯定是要选择`varchar`，但是`varchar`需要额外的空间来记录该字段目前占用的长度；因此如果字段的长度是固定的，尽量选用`char`，这会给你节约不少的内存空间。

### 不要用外键、触发器和视图功能

这也是「阿里巴巴开发手册」中提到的原则。原因有三个：

1. 降低了可读性，检查代码的同时还得查看数据库的代码；
2. 把计算的工作交给程序，数据库只做好存储的工作，并把这件事情做好；
3. 数据的完整性校验的工作应该由开发者完成，而不是依赖于外键，一旦用了外键，你会发现测试的时候随便删点垃圾数据都变得异常艰难。

### 图片、音频、视频存储

不要直接存储大文件，而是要存储大文件的访问地址。

### 大字段拆分和数据冗余

**大字段拆分**其实就是前面说过的垂直分表，把不常用的字段或者数据量较大的字段拆分出去，避免列数过多和数据量过大，尤其是习惯编写`SELECT *`的情况下，列数多和数据量大导致的问题会被严重放大！

**字段冗余**原则上不符合数据库设计范式，但是却非常有利于快速检索。比如，合同表中存储客户id的同时可以冗余存储客户姓名，这样查询时就不需要再根据客户id获取用户姓名了。因此针对业务逻辑适当做一定程度的冗余也是一种比较好的优化技巧。

## 常见类型选择

**整数类型宽度设置**

MySQL可以为整数类型指定宽度，例如int(11)，实际上并没有意义，它并不会限制值的范围，对于存储和计算来说，int(1)和int(20)是相同的

**VARCHAR和CHAR类型**

char类型是定长的，而varchar存储可变字符串，比定长更省空间，但是varchar需要额外1或2个字节记录字符串长度，更新时也容易产生碎片

需要结合使用场景来选择：如果字符串列最大长度比平均长度大很多，或者列的更新很少，选择varchar较合适；如果要存很短的字符串，或者字符串值长度都相同，比如MD5值，或者列数据经常变更，选择使用char类型

**DATETIME和TIMESTAMP类型**

datetime的范围更大，能表示从1001到9999年，timestamp只能表示从1970年到2038年。datetime与时区无关，timestamp显示值依赖于时区。在大多数场景下，这两种类型都能良好地工作，但是建议使用timestamp，因为datetime占用8个字节，timestamp只占用了4个字节，timestamp空间效率更高

**BLOB和TEXT类型**

blob和text都是为存储很大数据而设计的字符串数据类型，分别采用二进制和字符方式存储

在实际使用中，要慎用这两种类型，它们的查询效率很低，如果字段必须要使用这两种类型，可以把此字段分离成子表，需要查询此字段时使用联合查询，这样可以提高主表的查询效率



## 设计中间表

设计中间表，一般针对于统计分析功能，或者实时性不高的需求（报表统计，数据分析等系统）

### 设计冗余字段

为减少关联查询，创建合理的冗余字段（创建冗余字段还需要注意数据一致性问题）。这里分库分表时较为常用。

### 拆表

对于字段太多的大表，考虑拆表（比如一个表有100多个字段） 对于表中经常不被使用的字段或者存储数据比较多的字段，考虑拆表。

### 主键优化

每张表建议都要有一个主键（主键索引），而且**主键类型最好是int类型**，建议自增主键（**分布式系统的情况下建议雪花算法**）

### 字段的设计

数据库中的表越小，在它上面执行的查询也就会越快。因此，在创建表的时候，为了获得更好的性能，我们可以将表中字段的宽度设得尽可能小。

- **使用可以存下数据最小的数据类型，合适即可**
- 尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED；
- VARCHAR的长度**只分配真正需要的空间**；
- 对于某些文本字段，比如**"省份"或者"性别"**，**使用枚举或整数代替字符串类型**；在MySQL中， ENUM类型被当作数值型数据来处理，而数值型数据被处理起来的速度要比文本类型快得多
- **尽量使用TIMESTAMP而非DATETIME**；
- 单表不要有太多字段，建议在20以内；
- 尽可能使用not null定义字段，null占用4字节空间，这样在将来执行查询的时候，数据库不用去比较NULL值。
- 用整型来存IP。
- 尽量少用 text 类型，非用不可时最好考虑拆表

# MySQL高性能优化规范建议⭐

## 数据库命令规范

- 所有数据库对象名称必须使用小写字母并用下划线分割
- 所有数据库对象名称禁止使用mysql保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来）
- 数据库对象的命名要能做到见名识意，并且最后不要超过32个字符
- 临时库表必须以tmp_为前缀并以日期为后缀，备份表必须以bak_为前缀并以日期(时间戳)为后缀
- 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低）

------

## 数据库基本设计规范

### 1. 所有表必须使用Innodb存储引擎

没有特殊要求（即Innodb无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用Innodb存储引擎（mysql5.5之前默认使用Myisam，5.6以后默认的为Innodb）。

Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。

### 2. 数据库和表的字符集统一使用UTF8

兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储emoji表情的需要，字符集需要采用utf8mb4字符集。

### 3. 所有表和字段都需要添加注释

使用comment从句添加表和列的备注，从一开始就进行数据字典的维护

### 4. 尽量控制单表数据量的大小，建议控制在500万以内

500万并不是Mysql数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。

可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小

### 5. 谨慎使用MySQL分区表

分区表在物理上表现为多个文件，在逻辑上表现为一个表；

谨慎选择分区键，跨分区查询效率可能更低；

建议采用物理分表的方式管理大数据。

### 6. 尽量做到冷热数据分离，减小表的宽度

> MySQL限制每个表最多存储4096列，并且每一行数据的大小不能超过65535字节。

减少磁盘IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的IO）；

更有效的利用缓存，避免读入无用的冷数据；

经常一起使用的列放到一个表中（避免更多的关联操作）。

### 7. 禁止在表中建立预留字段

预留字段的命名很难做到见名识义。

预留字段无法确认存储的数据类型，所以无法选择合适的类型。

对预留字段类型的修改，会对表进行锁定。

### 8. 禁止在数据库中存储图片，文件等大的二进制数据

通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机IO操作，文件很大时，IO操作很耗时。

通常存储于文件服务器，数据库只存储文件地址信息

### 9. 禁止在线上做数据库压力测试

### 10. 禁止从开发环境测试环境直接连接生产环境数据库

------

## 数据库字段设计规范

### 1. 优先选择符合存储需要的最小的数据类型

**原因：**

列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的IO次数也就越多，索引的性能也就越差。

**方法：**

#### 1、将字符串转换成数字类型存储，如：将IP地址转换成整形数据

mysql提供了两个方法来处理ip地址

- inet_aton 把ip转为无符号整型(4-8位)
- inet_ntoa 把整型的ip转为地址

插入数据前，先用inet_aton把ip地址转为整型，可以节省空间，显示数据时，使用inet_ntoa把整型的ip地址转为地址显示即可。

#### 2、对于非负型的数据（如自增ID、整型IP）来说，要优先使用无符号整型来存储

**原因：**

无符号相对于有符号可以多出一倍的存储空间

```apl
SIGNED INT -2147483648~2147483647
UNSIGNED INT 0~4294967295
```

VARCHAR(N)中的N代表的是字符数，而不是字节数，使用UTF8存储255个汉字 Varchar(255)=765个字节。**过大的长度会消耗更多的内存。**

### 2. 避免使用TEXT、BLOB数据类型，最常见的TEXT类型可以存储64k的数据

##### **1、建议把BLOB或是TEXT列分离到单独的扩展表中**

Mysql内存临时表不支持TEXT、BLOB这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，Mysql还是要进行二次查询，会使sql性能变得很差，但是不是说一定不能使用这样的数据类型。

如果一定要使用，建议把BLOB或是TEXT列分离到单独的扩展表中，查询时一定不要使用select * 而只需要取出必要的列，不需要TEXT列的数据时不要对该列进行查询。

##### **2、TEXT或BLOB类型只能使用前缀索引**

因为[MySQL](http://mp.weixin.qq.com/s?__biz=MzI4Njc5NjM1NQ==&mid=2247487885&idx=1&sn=65b1bf5f7d4505502620179669a9c2df&chksm=ebd62ea1dca1a7b7bf884bcd9d538d78ba064ee03c09436ca8e57873b1d98a55afd6d7884cfc&scene=21#wechat_redirect)对索引字段长度是有限制的，所以TEXT类型只能使用前缀索引，并且TEXT列上是不能有默认值的

### 3. 避免使用ENUM类型

修改ENUM值需要使用ALTER语句

ENUM类型的ORDER BY操作效率低，需要额外操作

禁止使用数值作为ENUM的枚举值

### 4. 尽可能把所有列定义为NOT NULL

**原因：**

索引NULL列需要额外的空间来保存，所以要占用更多的空间

进行比较和计算时要对NULL值做特别的处理

### 5. 使用DATETIME类型（8个字节）存储时间

TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07

TIMESTAMP 占用4字节和INT相同，但比INT可读性高

超出TIMESTAMP取值范围的使用DATETIME类型存储

**经常会有人用字符串存储日期型的数据（不正确的做法）**

- 缺点1：无法用日期函数进行计算和比较
- 缺点2：用字符串存储日期要占用更多的空间

### 6. 同财务相关的金额类数据必须使用decimal类型

- 非精准浮点：float,double
- 精准浮点：decimal

Decimal类型为精准浮点数，在计算时不会丢失精度

占用空间由定义的宽度决定，每4个字节可以存储9位数字，并且小数点要占用一个字节

可用于存储比bigint更大的整型数据

------

## 索引设计规范

### 1. 限制表的索引数量，建议单张表索引不超过5个

索引并不是越多越好！索引可以提高效率同样可以降低效率。

索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。

因为mysql优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加mysql优化器生成执行计划的时间，同样会降低查询性能。

### 2. 禁止给表中的每一列都建立单独的索引

5.6版本之前，一个sql只能使用到一个表中的一个索引，5.6以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。

### 3. 每个Innodb表必须有个主键

Innodb是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。

Innodb是按照主键索引的顺序来组织表的

- 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引）
- 不要使用UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长）
- 主键建议使用自增ID值

------

## 常见索引列建议

- 出现在SELECT、UPDATE、DELETE语句的WHERE从句中的列
- 包含在ORDER BY、GROUP BY、DISTINCT中的字段
- 并不要将符合1和2中的字段的列都建立一个索引， 通常将1、2中的字段建立联合索引效果更好
- 多表join的关联列

------

## 如何选择索引列的顺序

建立[索引](http://mp.weixin.qq.com/s?__biz=MzI4Njc5NjM1NQ==&mid=2247485618&idx=1&sn=ed892f572b81e6aa6a7be5c43c2e7351&chksm=ebd6379edca1be88f7d3703db31115ced7c5cacd814b684731d6bc1f66f547f341b6c6451da4&scene=21#wechat_redirect)的目的是：希望通过索引进行数据查找，减少随机IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。

- 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数）
- 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO性能也就越好）
- 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）

------

## 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间）

- 重复索引示例：primary key(id)、index(id)、unique index(id)
- 冗余索引示例：index(a,b,c)、index(a,b)、index(a)

------

## 对于频繁的查询优先考虑使用覆盖索引

> 覆盖索引：就是包含了所有查询字段(where,select,ordery by,group by包含的字段)的索引

**覆盖索引的好处**

### 避免Innodb表进行索引的二次查询

Innodb是以聚集索引的顺序来存储的，对于Innodb来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。

而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了IO操作，提升了查询效率。

### 可以把随机IO变成顺序IO加快查询效率

由于覆盖索引是按键值的顺序存储的，对于IO密集型的范围查找来说，对比随机从磁盘读取每一行的数据IO要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的IO转变成索引查找的顺序IO。

------

## 索引SET规范

#### 尽量避免使用外键约束

- 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引
- 外键可用于保证数据的参照完整性，但建议在业务端实现
- 外键会影响父表和子表的写操作从而降低性能

------

## 数据库SQL开发规范

### 1. 建议使用预编译语句进行数据库操作

预编译语句可以重复使用这些计划，减少SQL编译所需要的时间，还可以解决动态SQL所带来的SQL注入的问题。

只传参数，比传递SQL语句更高效。

相同语句可以一次解析，多次使用，提高处理效率。

### 2. 避免数据类型的隐式转换

隐式转换会导致索引失效如:

```sql
select name,phone from customer where id = '111';
```

### 3. 充分利用表上已经存在的索引

避免使用双%号的查询条件。如：`a like '%123%'`，（如果无前置%,只有后置%，是可以用到列上的索引的）

一个SQL只能利用到复合索引中的一列进行范围查询。如：有 a,b,c列的联合索引，在查询条件中有a列的范围查询，则在b,c列上的索引将不会被用到。

在定义联合索引时，如果a列要用到范围查找的话，就要把a列放到联合索引的右侧，使用left join 或 not exists 来优化not in 操作，因为not in 也通常会使用索引失效。

### 4. 数据库设计时，应该要对以后扩展进行考虑

### 5. 程序连接不同的数据库使用不同的账号，禁止跨库查询

- 为数据库迁移和分库分表留出余地
- 降低业务耦合度
- 避免权限过大而产生的安全风险

### 6. 禁止使用SELECT * 必须使用SELECT <字段列表> 

**原因：**

- 消耗更多的CPU和IO以网络带宽资源
- 无法使用覆盖索引
- 可减少表结构变更带来的影响

### 7. 禁止使用不含字段列表的INSERT语句

如：

```sql
insert into values ('a','b','c');
```

应使用：

```sql
insert into t(c1,c2,c3) values ('a','b','c');
```

### 8. 避免使用子查询，可以把子查询优化为join操作

通常子查询在in子句中，且子查询中为简单SQL(不包含union、group by、order by、limit从句)时,才可以把子查询转化为关联查询进行优化。

**子查询性能差的原因：**

子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。

由于子查询会产生大量的临时表也没有索引，所以会消耗过多的CPU和IO资源，产生大量的慢查询。

### 9. 避免使用JOIN关联太多的表

对于Mysql来说，是存在关联缓存的，缓存的大小可以由join_buffer_size参数进行设置。

在Mysql中，对于同一个SQL多关联（join）一个表，就会多分配一个关联缓存，如果在一个SQL中关联的表越多，所占用的内存也就越大。

如果程序中大量的使用了多表关联的操作，同时join_buffer_size设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。

同时对于关联操作来说，会产生临时表操作，影响查询效率，Mysql最多允许关联61个表，建议不超过5个。

### 10. 减少同数据库的交互次数

数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。

### 11. 对应同一列进行or判断时，使用in代替or

in 的值不要超过500个，in 操作可以更有效的利用索引，or大多数情况下很少能利用到索引。

### 12. 禁止使用order by rand() 进行随机排序

order by rand()会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的CPU和IO及内存资源。

推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。

### 13. WHERE从句中禁止对列进行函数转换和计算

对列进行函数转换或计算时会导致无法使用索引

**不推荐：**

```sql
where date(create_time)='20190101'
```

**推荐：**

```sql
where create_time >= '20190101' and create_time < '20190102'
```

### 14. 在不会有重复值时使用UNION ALL 而不是UNION

- UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作
- UNION ALL 不会再对结果集进行去重操作

### 15. 拆分复杂的大SQL为多个小SQL

- 大SQL逻辑上比较复杂，需要占用大量CPU进行计算的SQL
- MySQL中，一个SQL只能使用一个CPU进行计算
- SQL拆分后可以通过并行执行来提高处理效率

------

## 数据库操作行为规范

### 超100万行的批量写（UPDATE、DELETE、INSERT）操作，要分批多次进行操作

#### 1、大批量操作可能会造成严重的主从延迟

主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间，
而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况

#### 2、binlog日志为row格式时会产生大量的日志

大批量写操作会产生大量日志，特别是对于row格式二进制数据而言，由于在row格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因

#### 3、避免产生大事务操作

大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对MySQL的性能产生非常大的影响。

特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批

### 对于大表使用pt-online-schema-change修改表结构

- 避免大表修改产生的主从延迟
- 避免在对表字段进行修改时进行锁表

对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。

pt-online-schema-change它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个DDL操作，分解成多个小的批次进行。

### 禁止为程序使用的账号赋予super权限

- 当达到最大连接数限制时，还运行1个有super权限的用户连接
- super权限只能留给DBA处理问题的账号使用

### 对于程序连接数据库账号，遵循权限最小原则

- 程序使用数据库账号只能在一个DB下使用，不准跨库
- 程序使用的账号原则上不准有drop权限



# MySQL莫名其妙的断开连接以及解决方案

## 前言

最近遇到在将本地的项目部署到服务器上之后遇到的一个奇怪问题

在部署完成后，网站当时可以正常工作，但是第二天访问网站的时候却会遇到一个500 Server Error。

**从日志中可以看出是MySQL数据库出现了异常**

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208181628366.png" alt="image-20220818162823267" style="zoom:67%;" />

翻译如下:

> 最后一个数据包在 83827560 ms 之前被成功接收，最后一个数据包在83827560 ms 之前被成功发送。比服务的配置参数`wait_timeout`的值要长。

**日志中给出的建议如下**

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208181628150.png" alt="image-20220818162846044" style="zoom:67%;" />

翻译如下：

> 你应考虑在程序中进行数据库操作之前检验数据库连接的有效性或者将数据库的autoReconnect属性设置为true来避免这个问题

关于`wait_timeout`和autoReconnect下面我们会依次分析介绍！

## 原因分析

我们进入mysql的命令行查询超时时间

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208181629293.png" alt="image-20220818162902141" style="zoom:67%;" />

28800单位是秒转化成小时就是8小时

看出MySQL的默认设置，当一个连接的空闲时间超过8小时后，MySQL就会断开该连接

所以发现问题出在如果超过这个`wait_timeout`时间(默认是8小时)对数据库没有任何操作，那么MySQL会自动关闭数据库连接以节省资源

数据库连接自动断开的问题确实是在第二天发生了，也就是在一个晚上没有对数据库进行操作(显然超过了8小时)的情况下发生的这个问题

**大家用命令show processlist; 可以查看Sleep状态的进程Sleep，同时可以看到每个进程Sleep多久了：**

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208181631417.png" alt="image-20220818163155246" style="zoom:67%;" />

下面介绍下解决和优化办法！

## 解决方法⭐

### 1.autoReconnect

这个参数表示在mysql超时断开连接后会自动重新连接

配置的话，只需要在连接mysql的语句写上autoReconnect=true

```apl
jdbc:mysql://127.0.0.1:3306/stock_tweet?autoReconnect=true 
```

**下面是MySQL官网对autoReconnect的解释:**

![图片](https://mmbiz.qpic.cn/mmbiz_png/hC3oNAJqSRzsflmFTIzdIic3ljut36p4uVyhNqqX3UeH0y1A0icA6a8XbLYb0l1vAblqnrotHHY6KE9WEQbfjx1g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

同时可以看到官网不推荐使用这个参数，因为它有一些副作用

具体介绍下：

- 原有连接上的事务将会被回滚，事务的提交模式将会丢失
- 原有连接持有的表的锁将会全部释放
- 原有连接关联的会话Session将会丢失，重新恢复的连接关联的将会是一个新的会话Session
- 原有连接定义的用户变量将会丢失
- 原有连接定义的预编译SQL将会丢失
- 原有连接失效，新的连接恢复后，MySQL将会使用新的记录行来存储连接中的性能数据

### 2.修改配置

涉及到两个配置参数`interactive_timeout和wait_timeout`

`wait_timeout `指的是mysql在关闭一个非交互的连接之前所要等待的秒数

`interactive_time` 指的是mysql在关闭一个交互的连接之前所要等待的秒数

对于交互和非交互连接，说得直白一点就是，通过mysql客户端连接数据库是交互式连接，通过jdbc连接数据库是非交互式连接。

配置方法：

> 1.会话方式

```
msyql> set global wait_timeout=2880000;
msyql> set global interactive_timeout=2880000;
```

这种方式只对当前会话生效

> 2.修改配置文件方式

修改/etc/my.cnf文件，在 [mysqld] 节中设置：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.8.30/202208181632244.png" alt="image-20220818163258101" style="zoom: 50%;" />

之后再重启下服务器就好了

**注意：**

将`wait_timeout`这个值设置得大了，可能会导致空闲连接过多。

如果你的MySQL Server有大量的闲置连接，他们不仅会白白消耗内存，而且如果连接一直在累加而不断开，最终肯定会达到MySQL Server的连接上限数，这会报'too many connections'的错误。

### 连接池配置

因为连接池的配置也会影响项目和MySQL的连接，所以也需要对数据库连接池的一些配置做一定修改

我们以Spring Boot 2.0默认的数据库连接池HikariCP为例

主要是下面这几个配置

**maximum-pool-size：**

最大连接数，超过这个数，新的数据库访问线程会被阻，缺省值：10。

常见的错误是设置一个太大的值，连接数多反而性能下降。

参考计算公式是：

```apl
# core_count：CPU个数，effective_spindle_count:硬盘个数
connections = ((core_count * 2) + effective_spindle_count)
```

例如：一个4核，1块硬盘的服务器，连接数 = （4 * 2） + 1 = 9，凑个整数，10就可以了。

**minimum-idle：**

最小的连接数目

**max-lifetime:**

最大的连接时间，用来设置一个connection在连接池中的存活时间

缺省：30分钟。强烈建议设置比数据库超时时长少一点（MySQL的`wait_timeout`参数一般为8小时）。

**idle-timeout:**

一个连接idle状态的最长时间，超时则被释放

其他参数详见：https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing





# 优化 count 计数

最近我在公司优化过几个慢查询接口的性能，总结了一些心得体会拿出来跟大家一起分享一下，希望对你会有所帮助。

我们使用的数据库是`Mysql8`，使用的存储引擎是`Innodb`。这次优化除了`优化索引`之外，更多的是在优化`count(*)`。通常情况下，分页接口一般会查询两次数据库，第一次是获取具体数据，第二次是获取总的记录行数，然后把结果整合之后，再返回。查询具体数据的sql，比如是这样的：

```sql
select id,name from user limit 1,20;
```

它没有性能问题。但另外一条使用count(*)查询总记录行数的sql，例如：

```sql
select count(*) from user;
```

却存在性能差的问题。为什么会出现这种情况呢？

## 1 count(*)为什么性能差？

在Mysql中，`count(*)`的作用是统计表中记录的总行数。

而`count(*)`的性能跟存储引擎有直接关系，并非所有的存储引擎，`count(*)`的性能都很差。

在Mysql中使用最多的存储引擎是：`innodb`和`myisam`。

在myisam中会把总行数保存到磁盘上，使用count(*)时，只需要返回那个数据即可，无需额外的计算，所以执行效率很高。而innodb则不同，由于它支持事务，有`MVCC`（即多版本并发控制）的存在，在同一个时间点的不同事务中，同一条查询sql，返回的记录行数可能是不确定的。

在innodb使用count(*)时，需要从存储引擎中一行行的读出数据，然后累加起来，所以执行效率很低。

如果表中数据量小还好，一旦表中数据量很大，innodb存储引擎使用count(*)统计数据时，性能就会很差。

## 2 如何优化count(*)性能？

从上面得知，既然`count(*)`存在性能问题，那么我们该如何优化呢？

我们可以从以下几个方面着手。

### 1 增加redis缓存

对于简单的count(*)，比如：统计浏览总次数或者浏览总人数，我们可以直接将接口使用redis缓存起来，没必要实时统计。当用户打开指定页面时，在缓存中每次都设置成count = count+1即可。

用户第一次访问页面时，redis中的count值设置成1。用户以后每访问一次页面，都让count加1，最后重新设置到redis中

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212022122288.png" alt="image-20221202212241180" style="zoom:67%;" />

这样在需要展示数量的地方，从redis中查出count值返回即可。

该场景无需从数据埋点表中使用count(*)实时统计数据，性能将会得到极大的提升。

不过在高并发的情况下，可能会存在缓存和数据库的数据不一致的问题。

但对于统计浏览总次数或者浏览总人数这种业务场景，对数据的准确性要求并不高，容忍数据不一致的情况存在。

### 2 加二级缓存

对于有些业务场景，新增数据很少，大部分是统计数量操作，而且查询条件很多。这时候使用传统的count(*)实时统计数据，性能肯定不会好。

假如在页面中可以通过id、name、状态、时间、来源等，一个或多个条件，统计品牌数量。

这种情况下用户的组合条件比较多，增加联合索引也没用，用户可以选择其中一个或者多个查询条件，有时候联合索引也会失效，只能尽量满足用户使用频率最高的条件增加索引。

也就是有些组合条件可以走索引，有些组合条件没法走索引，这些没法走索引的场景，该如何优化呢？

答：使用`二级缓存`。

二级缓存其实就是内存缓存。

我们可以使用`caffine`或者`guava`实现二级缓存的功能。

目前`SpringBoot`已经集成了caffine，使用起来非常方便。

只需在需要增加二级缓存的查询方法中，使用`@Cacheable`注解即可。

```java
@Cacheable(value = "brand", , keyGenerator = "cacheKeyGenerator")
public BrandModel getBrand(Condition condition) {
    return getBrandByCondition(condition);
}
```

然后自定义cacheKeyGenerator，用于指定缓存的key。

```java
public class CacheKeyGenerator implements KeyGenerator {
    @Override
    public Object generate(Object target, Method method, Object... params) {
        return target.getClass().getSimpleName() + UNDERLINE
                + method.getName() + ","
                + StringUtils.arrayToDelimitedString(params, ",");
    }
}
```

这个key是由各个条件组合而成。这样通过某个条件组合查询出品牌的数据之后，会把结果缓存到内存中，设置过期时间为5分钟。后面用户在5分钟内，使用相同的条件，重新查询数据时，可以直接从二级缓存中查出数据，直接返回了。

这样能够极大的提示count(*)的查询效率。但是如果使用二级缓存，可能存在不同的服务器上，数据不一样的情况。我们需要根据实际业务场景来选择，没法适用于所有业务场景。

### 3 多线程执行

不知道你有没有做过这样的需求：统计有效订单有多少，无效订单有多少。

这种情况一般需要写两条sql，统计有效订单的sql如下：

```sql
select count(*) from order where status=1;
```

统计无效订单的sql如下：

```sql
select count(*) from order where status=0;
```

但如果在一个接口中，同步执行这两条sql效率会非常低。

这时候，可以改成成一条sql：

```sql
select count(*),status from order group by status;
```

使用`group by`关键字分组统计相同status的数量，只会产生两条记录，一条记录是有效订单数量，另外一条记录是无效订单数量。但有个问题：status字段只有1和0两个值，重复度很高，区分度非常低，不能走索引，会全表扫描，效率也不高。还有其他的解决方案不？答：使用多线程处理。

我们可以使用`CompleteFuture`使用两个`线程`异步调用统计有效订单的sql和统计无效订单的sql，最后汇总数据，这样能够提升查询接口的性能。

### 4 减少join的表

大部分的情况下，使用count(*)是为了实时统计总数量的。

但如果表本身的数据量不多，但join的表太多，也可能会影响count(*)的效率。

比如在查询商品信息时，需要根据商品名称、单位、品牌、分类等信息查询数据。

这时候写一条sql可以查出想要的数据，比如下面这样的：

```sql
select count(*)
from product p
inner join unit u on p.unit_id = u.id
inner join brand b on p.brand_id = b.id
inner join category c on p.category_id = c.id
where p.name='测试商品' and u.id=123 and b.id=124 and c.id=125;
```

使用product表去`join`了unit、brand和category这三张表。其实这些查询条件，在product表中都能查询出数据，没必要join额外的表。我们可以把sql改成这样：

```sql
select count(*)
from product
where name='测试商品' and unit_id=123 and brand_id=124 and category_id=125;
```

在count(*)时只查product单表即可，去掉多余的表join，让查询效率可以提升不少。

### 5 改成ClickHouse

有些时候，join的表实在太多，没法去掉多余的join，该怎么办呢？

比如上面的例子中，查询商品信息时，需要根据商品名称、单位名称、品牌名称、分类名称等信息查询数据。

这时候根据product单表是没法查询出数据的，必须要去`join`：unit、brand和category这三张表，这时候该如何优化呢？答：可以将数据保存到`ClickHouse`。

ClickHouse是基于`列存储`的数据库，不支持事务，查询性能非常高，号称查询十几亿的数据，能够秒级返回。

为了避免对业务代码的嵌入性，可以使用`Canal`监听`Mysql`的`binlog`日志。当product表有数据新增时，需要同时查询出单位、品牌和分类的数据，生成一个新的结果集，保存到ClickHouse当中。

查询数据时，从ClickHouse当中查询，这样使用count(*)的查询效率能够提升N倍。

> 需要特别提醒一下：使用ClickHouse时，新增数据不要太频繁，尽量批量插入数据。

其实如果查询条件非常多，使用ClickHouse也不是特别合适，这时候可以改成`ElasticSearch`，不过它跟Mysql一样，存在`深分页`问题。

## 3 count的各种用法性能对比

既然说到count(*)，就不能不说一下count家族的其他成员，比如：count(1)、count(id)、count(普通索引列)、count(未加索引列)。那么它们有什么区别呢？

> - count(*) ：它会获取所有行的数据，不做任何处理，行数加1。
> - count(1)：它会获取所有行的数据，每行固定值1，也是行数加1。
> - count(id)：id代表主键，它需要从所有行的数据中解析出id字段，其中id肯定都不为NULL，行数加1。
> - count(普通索引列)：它需要从所有行的数据中解析出普通索引列，然后判断是否为NULL，如果不是NULL，则行数+1。
> - count(未加索引列)：它会全表扫描获取所有数据，解析中未加索引列，然后判断是否为NULL，如果不是NULL，则行数+1。

由此，最后count的性能从高到低是：

> count(*) ≈ count(1) > count(id) > count(普通索引列) > count(未加索引列)

所以，其实`count(*)`是最快的。

意不意外，惊不惊喜？

千万别跟`select *` 搞混了。



# 美团一面：如何优化慢SQL？

在应用开发的早期，数据量少，开发人员开发功能时更重视功能上的实现，随着生产数据的增长，很多 SQL 语句开始暴露出性能问题，对生产的影响也越来越大，有时可能这些有问题的 SQL 就是整个系统性能的瓶颈。



## SQL 优化一般步骤

### 1 慢查询日志定位执行效率低的 SQL 语句

### 2 explain 分析SQL的执行计划

需要重点关注 type、rows、filtered、extra。type 由上至下，效率越来越高：

> - ALL 全表扫描
> - index 索引全扫描
> - range 索引范围扫描，常用语<,<=,>=,between,in 等操作
> - ref 使用非唯一索引扫描或唯一索引前缀扫描，返回单条记录，常出现在关联查询中
> - eq_ref 类似 ref，区别在于使用的是唯一索引，使用主键的关联查询
> - const/system 单条记录，系统会把匹配行中的其他列作为常数处理，如主键或唯一索引查询
> - null MySQL 不访问任何表或索引，直接返回结果
> - 虽然上至下，效率越来越高，但是根据 cost 模型，假设有两个索引 idx1(a, b, c),idx2(a, c)，SQL 为"select * from t where a = 1 and b in (1, 2) order by c";如果走 idx1，那么是 type 为 range，如果走 idx2，那么 type 是 ref；当需要扫描的行数，使用 idx2 大约是 idx1 的 5 倍以上时，会用 idx1，否则会用 idx2

Extra：

> - **Using filesort：**MySQL 需要额外的一次传递，以找出如何按排序顺序检索行。通过根据联接类型浏览所有行并为所有匹配 WHERE 子句的行保存排序关键字和行的指针来完成排序。然后关键字被排序，并按排序顺序检索行；
> - **Using temporary：**使用了临时表保存中间结果，性能特别差，需要重点优化；
> - **Using index：**表示相应的 select 操作中使用了覆盖索引（Coveing Index）,避免访问了表的数据行，效率不错！如果同时出现 using where，意味着无法直接通过索引查找来查询到符合条件的数据；
> - **Using index condition：**MySQL5.6 之后新增的 ICP，using index condtion 就是使用了 ICP（索引下推），在存储引擎层进行数据过滤，而不是在服务层过滤，利用索引现有的数据减少回表的数据。

### 3 show profile 分析

了解 SQL 执行的线程的状态及消耗的时间。默认是关闭的，开启语句“set profiling = 1;”

```sql
SHOW PROFILES ;
SHOW PROFILE FOR QUERY  #{id};
```

### 4 trace

trace 分析优化器如何选择执行计划，通过 trace 文件能够进一步了解为什么优惠券选择 A 执行计划而不选择 B 执行计划。

```sql
set optimizer_trace="enabled=on";
set optimizer_trace_max_mem_size=1000000;
select * from information_schema.optimizer_trace;
```



## 确定问题并采用相应的措施

> - 优化索引
> - 优化 SQL 语句：修改 SQL、IN 查询分段、时间查询分段、基于上一次数据过滤
> - 改用其他实现方式：ES、数仓等
> - 数据碎片处理



## 场景分析

### 1 最左匹配

索引：

```sql
KEY `idx_shopid_orderno` (`shop_id`,`order_no`)
```

SQL 语句：

```sql
select * from _t where orderno=''
```

> 查询匹配从左往右匹配，要使用 order_no 走索引，必须查询条件携带 shop_id 或者索引（shop_id，order_no）调换前后顺序。

### 2 隐式转换

索引：

```sql
KEY `idx_mobile` (`mobile`)
```



SQL 语句：

```sql
select * from _user where mobile=12345678901
```



隐式转换相当于在索引上做运算，会让索引失效。mobile 是字符类型，使用了数字，应该使用字符串匹配，否则 MySQL 会用到隐式替换，导致索引失效。

### 3 大分页

索引

```sql
KEY `idx_a_b_c` (`a`, `b`, `c`)
```

SQL 语句：

```sql
select * from _t where a = 1 and b = 2 order by c desc limit 10000, 10;
```

对于大分页的场景，可以优先让产品优化需求，如果没有优化的，有如下两种优化方式：

> - 一种是把上一次的最后一条数据，也即上面的 c 传过来，然后做“c < xxx”处理，但是这种一般需要改接口协议，并不一定可行
> - 另一种是采用延迟关联的方式进行处理，减少 SQL 回表，但是要记得索引需要完全覆盖才有效果。

SQL 改动如下：

```sql
select t1.* from _t t1, (select id from _t where a = 1 and b = 2 
                         order by c desc limit 10000, 10) t2 
                         where t1.id = t2.id;
```



### 4 in + order by

索引：

```sql
KEY `idx_shopid_status_created` (`shop_id`, `order_status`, `created_at`)
```

SQL 语句：

```sql
select * from _order where shop_id = 1 and order_status in (1, 2, 3) order by created_at desc limit 10
```

in 查询在 MySQL 底层是通过 n*m 的方式去搜索，类似 union，但是效率比 union 高。

in 查询在进行 cost 代价计算时（代价 = 元组数 * IO 平均值），是通过将 in 包含的数值，一条条去查询获取元组数的，因此这个计算过程会比较的慢。

所以 MySQL 设置了个临界值（eq_range_index_dive_limit），5.6 之后超过这个临界值后该列的 cost 就不参与计算了。因此会导致执行计划选择不准确。

默认是 200，即 in 条件超过了 200 个数据，会导致 in 的代价计算存在问题，可能会导致 MySQL 选择的索引不准确。

**处理方式：**可以（order_status，created_at）互换前后顺序，并且调整 SQL 为延迟关联。



### 5 范围查询阻断，后续字段不能走索引

索引：

```sql
KEY `idx_shopid_created_status` (`shop_id`, `created_at`, `order_status`)
```

SQL 语句：

```sql
select * from _order where shop_id = 1 and created_at > '2021-01-01 00:00:00' and order_status = 10
```

范围查询还有“IN、between”。

### 6 不等于不包含不能用到索引的快速搜索

可以用到 ICP：

```sql
select * from _order where shop_id=1 and order_status not in (1,2)
select * from _order where shop_id=1 and order_status != 1
```

> 在索引上，避免使用 NOT、!=、<>、!<、!>、NOT EXISTS、NOT IN、NOT LIKE等。

### 7 优化器选择不使用索引的情况

> 如果要求访问的数据量很小，则优化器还是会选择辅助索引，但是当访问的数据占整个表中数据的蛮大一部分时（一般是 20% 左右），优化器会选择通过聚集索引来查找数据。

```sql
select * from _order where  order_status = 1
```

> 查询出所有未支付的订单，一般这种订单是很少的，即使建了索引，也没法使用索引。

###  8 复杂查询

```sql
select sum(amt) from _t where a = 1 and b in (1, 2, 3) and c > '2020-01-01';
select * from _t where a = 1 and b in (1, 2, 3) and c > '2020-01-01' limit 10;
```

> 如果是统计某些数据，可能改用数仓进行解决；如果是业务上就有那么复杂的查询，可能就不建议继续走 SQL 了，而是采用其他的方式进行解决，比如使用 ES 等进行解决。

### 9 asc 和 desc 混用

```sql
select * from _t where a=1 order by b desc, c asc
```

> desc 和 asc 混用时会导致索引失效。

### 10 大数据

> 对于推送业务的数据存储，可能数据量会很大，如果在方案的选择上，最终选择存储在 MySQL 上，并且做 7 天等有效期的保存。那么需要注意，频繁清理数据，会造成数据碎片，需要联系 DBA 进行数据碎片处理。



# SQL优化思路+经典案例分析

SQL调优这块呢，大厂面试必问的。最近金九银十嘛，所以整理了SQL的调优思路，并且附几个经典案例分析。

## 1 慢SQL优化思路

> 1. 慢查询日志记录慢SQL
> 2. explain分析SQL的执行计划
> 3. profile 分析执行耗时
> 4. Optimizer Trace分析详情
> 5. 确定问题并采用相应的措施

### 1 慢查询日志记录慢SQL

如何定位慢SQL呢、我们可以通过**慢查询日志**来查看慢SQL。默认的情况下呢，MySQL数据库是不开启慢查询日志（`slow query log`）呢。所以我们需要手动把它打开。

查看下慢查询日志配置，我们可以使用`show variables like 'slow_query_log%'`命令，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzbeibC0VsPaGW1uJhayibq7F90j6E5VBBWcJjU5QGLWGfGoLGdhd85ToA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- `slow query log`表示慢查询开启的状态
- `slow_query_log_file`表示慢查询日志存放的位置

我们还可以使用`show variables like 'long_query_time'`命令，查看超过多少时间，才记录到慢查询日志，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzGj602Le7icibbXbObLJajjVT4vTU75Xs5EJjOvUfx2Sia9wXkQApykCEQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- `long_query_time`表示查询超过多少秒才记录到慢查询日志。

> **我们可以通过慢查日志，定位那些执行效率较低的SQL语句，重点关注分析。**

### 2 explain查看分析SQL的执行计划

当定位出查询效率低的SQL后，可以使用`explain`查看`SQL`的执行计划。

当`explain`与`SQL`一起使用时，MySQL将显示来自优化器的有关语句执行计划的信息。即`MySQL`解释了它将如何处理该语句，包括有关如何连接表以及以何种顺序连接表等信息。

一条简单SQL，使用了`explain`的效果如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzXPXladfA4DnzaEHMtJZfpcDt1ibYkCj9B6eM3jiaQo8AA6ibKugI8w8mw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

一般来说，我们需要重点关注`type、rows、filtered、extra、key`。

#### 1 type

type表示**连接类型**，查看索引执行情况的一个重要指标。以下性能从好到坏依次：`system > const > eq_ref > ref > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL`

- system：这种类型要求数据库表中只有一条数据，是`const`类型的一个特例，一般情况下是不会出现的。
- const：通过一次索引就能找到数据，一般用于主键或唯一索引作为条件，这类扫描效率极高，，速度非常快。
- eq_ref：常用于主键或唯一索引扫描，一般指使用主键的关联查询
- ref : 常用于非主键和唯一索引扫描。
- ref_or_null：这种连接类型类似于`ref`，区别在于`MySQL`会额外搜索包含`NULL`值的行
- index_merge：使用了索引合并优化方法，查询使用了两个以上的索引。
- unique_subquery：类似于`eq_ref`，条件用了`in`子查询
- index_subquery：区别于`unique_subquery`，用于非唯一索引，可以返回重复值。
- range：常用于范围查询，比如：between ... and 或 In 等操作
- index：全索引扫描
- ALL：全表扫描

#### 2 rows

> 该列表示MySQL估算要找到我们所需的记录，需要读取的行数。对于InnoDB表，此数字是估计值，并非一定是个准确值。

#### 3 filtered

> 该列是一个百分比的值，表里符合条件的记录数的百分比。简单点说，这个字段表示存储引擎返回的数据在经过过滤后，剩下满足条件的记录数量的比例。

#### 4 extra

该字段包含有关MySQL如何解析查询的其他信息，它一般会出现这几个值：

- Using filesort：表示按文件排序，一般是在指定的排序和索引排序不一致的情况才会出现。一般见于order by语句
- Using index ：表示是否用了覆盖索引。
- Using temporary: 表示是否使用了临时表,性能特别差，需要重点优化。一般多见于group by语句，或者union语句。
- Using where : 表示使用了where条件过滤.
- Using index condition：MySQL5.6之后新增的索引下推。在存储引擎层进行数据过滤，而不是在服务层过滤，利用索引现有的数据减少回表的数据。

#### 5 key

该列表示实际用到的索引。一般配合`possible_keys`列一起看。

### 3 profile 分析执行耗时

`explain`只是看到`SQL`的预估执行计划，如果要了解`SQL`**真正的执行线程状态及消耗的时间**，需要使用`profiling`。开启`profiling`参数后，后续执行的`SQL`语句都会记录其资源开销，包括`IO，上下文切换，CPU，内存`等等，我们可以根据这些开销进一步分析当前慢SQL的瓶颈再进一步进行优化。

`profiling`默认是关闭，我们可以使用`show variables like '%profil%'`查看是否开启，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1Iz2tcFhpAJmNRcWZ1WViavVZib1k0YRums6uibvibUcSNPHicibUiad5P0tvBQQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

可以使用`set profiling=ON`开启。开启后，可以运行几条SQL，然后使用`show profiles`查看一下。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzS3KLpib5PjYVp8Z9CRYcEDVEdtVVfwKCV5Z1fuurC6sZpcXDt8X6KGQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`show profiles`会显示最近发给服务器的多条语句，条数由变量`profiling_history_size`定义，默认是15。如果我们需要看单独某条SQL的分析，可以`show profile`查看最近一条SQL的分析，也可以使用`show profile for query id`（其中id就是show profiles中的QUERY_ID）查看具体一条的SQL语句分析。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzNOEvds6F5Bp4DwGgYYeZiaUqeicmicfshDh8iaC8T9q0FgpP16ftbICu4Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

除了查看profile ，还可以查看cpu和io，如上图。

### 4 Optimizer Trace分析详情

profile只能查看到SQL的执行耗时，但是无法看到SQL真正执行的过程信息，即不知道MySQL优化器是如何选择执行计划。这时候，我们可以使用`Optimizer Trace`，它可以跟踪执行语句的解析优化执行的全过程。

我们可以使用`set optimizer_trace="enabled=on"`打开开关，接着执行要跟踪的SQL，最后执行`select * from information_schema.optimizer_trace`跟踪，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzlkCjMoynGgc4BuPIPcTZWedLqfkRp5FEEbcYduCy6dwBPUUDweJreA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

大家可以查看分析其执行树，会包括三个阶段：

- join_preparation：准备阶段
- join_optimization：分析阶段
- join_execution：执行阶段

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzicK2JhjqkZAghEibiclu9a5aooh4qIZGphLquibt1bvheHoHuicfhicagYIQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 5 确定问题并采用相应的措施

最后确认问题，就采取对应的措施。

- 多数慢SQL都跟索引有关，比如不加索引，索引不生效、不合理等，这时候，我们可以**优化索引**。
- 我们还可以优化SQL语句，比如一些in元素过多问题（分批），深分页问题（基于上一次数据过滤等），进行时间分段查询
- SQl没办法很好优化，可以改用ES的方式，或者数仓。
- 如果单表数据量过大导致慢查询，则可以考虑分库分表
- 如果数据库在刷脏页导致慢查询，考虑是否可以优化一些参数，跟DBA讨论优化方案
- 如果存量数据量太大，考虑是否可以让部分数据归档

我之前写了一篇文章，有关于导致慢查询的12个原因，大家看一看一下哈:[盘点MySQL慢查询的12个原因](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247499624&idx=1&sn=561b9cb7fe831ca7cb2d9fd65691e85e&chksm=cf222041f855a957ac50c0a53baaec6d26be32427259b2974450620f33a8c834419fe535e83d&token=767319274&lang=zh_CN&scene=21#wechat_redirect)

## 2 慢查询经典案例分析

### 1 隐式转换

我们创建一个用户user表

```
CREATE TABLE user (
  id int(11) NOT NULL AUTO_INCREMENT,
  userId varchar(32) NOT NULL,
  age  varchar(16) NOT NULL,
  name varchar(255) NOT NULL,
  PRIMARY KEY (id),
  KEY idx_userid (userId) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

`userId`字段为字串类型，是B+树的普通索引，如果查询条件传了一个数字过去，会导致索引失效。如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1Izia5JUxQBf0sEfUqAFByTTvcnSMgqcgadYxfblklAWhSvsP5fHrcvibGQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

如果给数字加上'',也就是说，传的是一个字符串呢，当然是走索引，如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzqZeo6fSQyvwrHdMCzicyQucxZf3sH6fmEtEcwbIjbxZrsbXtJgWmheA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

> 为什么第一条语句未加单引号就不走索引了呢？这是因为不加单引号时，是字符串跟数字的比较，它们类型不匹配，MySQL会做隐式的类型转换，把它们转换为浮点数再做比较。隐式的类型转换，索引会失效。

### 2 最左匹配

MySQl建立联合索引时，会遵循最左前缀匹配的原则，即最左优先。如果你建立一个`（a,b,c）`的联合索引，相当于建立了`(a)、(a,b)、(a,b,c)`三个索引。

假设有以下表结构：

```
CREATE TABLE user (
  id int(11) NOT NULL AUTO_INCREMENT,
  user_id varchar(32) NOT NULL,
  age  varchar(16) NOT NULL,
  name varchar(255) NOT NULL,
  PRIMARY KEY (id),
  KEY idx_userid_name (user_id,name) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

假设有一个联合索引`idx_userid_name`，我们现在执行以下`SQL`，如果查询列是`name`，索引是无效的：

```
explain select * from user where name ='捡田螺的小男孩';
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1Iz08pJVC0q2r4TRlOtHzfOcTNInkUKvXyic0hQy5icHFGeDFW7neSEP6Bg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因为查询条件列`name`不是联合索引`idx_userid_name`中的第一个列，不满足最左匹配原则，所以索引不生效。在联合索引中，只有查询条件满足最左匹配原则时，索引才正常生效。如下，查询条件列是`user_id`

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1Iz9wphdo7GTf8w1iccyVuQPOVxn75TC3lXFNdaBDKtAUtV17ibLREmkMyA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 3 深分页问题

`limit`深分页问题，会导致慢查询，应该大家都司空见惯了吧。

**limit深分页为什么会变慢呢？** 假设有表结构如下：

```
CREATE TABLE account (
  id int(11) NOT NULL AUTO_INCREMENT COMMENT '主键Id',
  name varchar(255) DEFAULT NULL COMMENT '账户名',
  balance int(11) DEFAULT NULL COMMENT '余额',
  create_time datetime NOT NULL COMMENT '创建时间',
  update_time datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (id),
  KEY idx_name (name),
  KEY idx_create_time (create_time) //索引
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='账户表';
```

以下这个SQL，你知道执行过程是怎样的呢？

```
select id,name,balance from account where create_time> '2020-09-19' limit 100000,10;
```

这个SQL的执行流程酱紫：

1. 通过普通二级索引树`idx_create_time`，过滤`create_time`条件，找到满足条件的主键`id`。
2. 通过主键`id`，回到`id`主键索引树，找到满足记录的行，然后取出需要展示的列（回表过程）
3. 扫描满足条件的`100010`行，然后扔掉前`100000`行，返回。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzLfglfdGPqJFWzSOGvhT75L7JWx8r8XkNMdtnUjMicickhicsBEiaqUPILA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因此，limit深分页，导致SQL变慢原因有两个：

- `limit`语句会先扫描`offset+n`行，然后再丢弃掉前`offset`行，返回后`n`行数据。也就是说`limit 100000,10`，就会扫描`100010`行，而`limit 0,10`，只扫描`10`行。
- `limit 100000,10` 扫描更多的行数，也意味着回表更多的次数。

**如何优化深分页问题?**

我们可以通过减少回表次数来优化。一般有**标签记录法和延迟关联法**。

**标签记录法**

> 就是标记一下上次查询到哪一条了，下次再来查的时候，从该条开始往下扫描。就好像看书一样，上次看到哪里了，你就折叠一下或者夹个书签，下次来看的时候，直接就翻到啦。

假设上一次记录到`100000`，则SQL可以修改为：

```
select  id,name,balance FROM account where id > 100000 limit 10;
```

这样的话，后面无论翻多少页，性能都会不错的，因为命中了id索引。但是这种方式有局限性：需要一种类似连续自增的字段。

**延迟关联法**

延迟关联法，就是把条件转移到**主键索引树**，然后减少回表。如下

```
select  acct1.id,acct1.name,acct1.balance FROM account acct1 INNER JOIN (SELECT a.id FROM account a WHERE a.create_time > '2020-09-19' limit 100000, 10) AS acct2 on acct1.id= acct2.id;
```

**优化思路**就是，先通过`idx_create_time`二级索引树查询到满足条件的`主键ID`，再与原表通过`主键ID`内连接，这样后面直接走了主键索引了，同时也减少了回表。

### 4  in元素过多

如果使用了`in`，即使后面的条件加了索引，还是要注意`in`后面的元素不要过多哈。`in`元素一般建议不要超过`200`个，如果超过了，建议分组，每次200一组进行哈。

**反例:**

```
select user_id,name from user where user_id in (1,2,3...1000000); 
```

如果我们对`in`的条件不做任何限制的话，该查询语句一次性可能会查询出非常多的数据，很容易导致接口超时。尤其有时候，我们是用的子查询，**in后面的子查询**，你都不知道数量有多少那种，更容易采坑.如下这种子查询：

```
select * from user where user_id in (select author_id from artilce where type = 1);
```

如果`type = 1`有1一千，甚至上万个呢？肯定是慢SQL。索引一般建议分批进行，一次200个，比如：

```
select user_id,name from user where user_id in (1,2,3...200);
```

in查询为什么慢呢？

> 这是因为`in`查询在MySQL底层是通过`n*m`的方式去搜索，类似`union`。
>
> in查询在进行cost代价计算时（代价 = 元组数 * IO平均值），是通过将in包含的数值，一条条去查询获取元组数的，因此这个计算过程会比较的慢，所以MySQL设置了个临界值(eq_range_index_dive_limit)，5.6之后超过这个临界值后该列的cost就不参与计算了。因此会导致执行计划选择不准确。默认是200，即in条件超过了200个数据，会导致in的代价计算存在问题，可能会导致Mysql选择的索引不准确。

### 5 order by 走文件排序

如果order by 使用到文件排序，则会可能会产生慢查询。我们来看下下面这个SQL：

```
select name,age,city from staff where city = '深圳' order by age limit 10;
```

它表示的意思就是：查询前10个，来自深圳员工的姓名、年龄、城市，并且按照年龄小到大排序。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzccZfvNm8IGiaRm5MbdCWxCQmF5khK8wCicic5NeJsDpCpibGOtG0JfsR0Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

查看explain执行计划的时候，可以看到Extra这一列，有一个`Using filesort`，它表示用到文件排序。

**order by文件排序效率为什么较低**

大家可以看下这个下面这个图:

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1Izz29STbibSS6BepYE5MG639zORXiblwGJcw3IlZgQdibiaiav1V7TOWAkIYw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`order by`排序，分为`全字段排序`和`rowid排序`。它是拿`max_length_for_sort_data`和结果行数据长度对比，如果结果行数据长度超过`max_length_for_sort_data`这个值，就会走`rowid`排序，相反，则走全字段排序。

#### 1 rowid排序

rowid排序，一般需要回表去找满足条件的数据，所以效率会慢一点。以下这个SQL，使用rowid排序，执行过程是这样：

```sql
select name,age,city from staff where city = '深圳' order by age limit 10;
```

1. `MySQL`为对应的线程初始化`sort_buffer`，放入需要排序的`age`字段，以及`主键id`；
2. 从索引树`idx_city`， 找到第一个满足 `city='深圳’`条件的`主键id`,假设`id`为`X`；
3. 到主键`id索引树`拿到`id=X`的这一行数据， 取age和主键id的值，存到`sort_buffer`；
4. 从索引树`idx_city`拿到下一个记录的`主键id`，假设`id=Y`；
5. 重复步骤 3、4 直到`city`的值不等于深圳为止；
6. 前面5步已经查找到了所有`city`为深圳的数据，在`sort_buffer`中，将所有数据根据`age`进行排序；遍历排序结果，取前10行，并按照id的值回到原表中，取出`city、name 和 age`三个字段返回给客户端。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1Iz1xNWsicI3t7pGnpTGvsd6HTFMc8W66Yu9ZyXw0Z9RkMtXib8qGYzsoFQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### 2 全字段排序

同样的SQL，如果是走全字段排序是这样的：

```
select name,age,city from staff where city = '深圳' order by age limit 10;
```

1. MySQL 为对应的线程初始化`sort_buffer`，放入需要查询的`name、age、city`字段；
2. 从索引树`idx_city`， 找到第一个满足 `city='深圳’`条件的主键 id，假设找到`id=X`；
3. 到主键id索引树拿到`id=X`的这一行数据， 取`name、age、city`三个字段的值，存到`sort_buffer`；
4. 从索引树`idx_city` 拿到下一个记录的主键`id`，假设`id=Y`；
5. 重复步骤 3、4 直到`city`的值不等于深圳为止；
6. 前面5步已经查找到了所有`city`为深圳的数据，在`sort_buffer`中，将所有数据根据age进行排序；
7. 按照排序结果取前10行返回给客户端。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzDOtHulKq6Jxa1QVDnITlGibj9icouQK6TVPPllL2RlwxDEIh0HJVLmkA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`sort_buffer`的大小是由一个参数控制的：`sort_buffer_size`。

- 如果要排序的数据小于`sort_buffer_size`，排序在`sort_buffer`内存中完成
- 如果要排序的数据大于`sort_buffer_size`，则借助磁盘文件来进行排序。

> 借助磁盘文件排序的话，效率就更慢一点。因为先把数据放入`sort_buffer`，当快要满时。会排一下序，然后把`sort_buffer`中的数据，放到临时磁盘文件，等到所有满足条件数据都查完排完，再用归并算法把磁盘的临时排好序的小文件，合并成一个有序的大文件。

#### 3  如何优化order by的文件排序

`order by`使用文件排序，效率会低一点。我们怎么优化呢？

- 因为数据是无序的，所以就需要排序。如果数据本身是有序的，那就不会再用到文件排序啦。而索引数据本身是有序的，我们通过建立索引来优化`order by`语句。
- 我们还可以通过调整`max_length_for_sort_data、sort_buffer_size`等参数优化；

### 6 索引字段上使用is null， is not null，索引可能失效

表结构:

```sql
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `card` varchar(255) DEFAULT NULL,
  `name` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE,
  KEY `idx_card` (`card`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
```

单个`name`字段加上索引，并查询`name`为非空的语句，其实会走索引的，如下:

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzY2yMYdY3W6hXDUT2suuGqic3VuS07oDGL1T0PgMxicfX7MPnr7g6sYqg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

单个`card`字段加上索引，并查询`name`为非空的语句，其实会走索引的，如下:

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzH77Ie3ic0Ax7ia0l1heO5HvAUswiab8hBM6Xp3uZvlVrQd8VK5fFyrd8Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

但是它两用or连接起来，索引就失效了，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzSIcwbMLAanYvLtRlYS1Nx0K38GTB5x0EFeV20MiclECporXa7rn3now/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

很多时候，也是因为数据量问题，导致了MySQL优化器放弃走索引。同时，平时我们用`explain`分析SQL的时候，如果`type=range`,要注意一下哈，因为这个可能因为数据量问题，导致索引无效。

### 7 索引字段上使用（！= 或者 < >），索引可能失效

假设有表结构：

```sql
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `userId` int(11) NOT NULL,
  `age` int(11) DEFAULT NULL,
  `name` varchar(255) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_age` (`age`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
```

虽然age加了索引，但是使用了！= 或者< >，not in这些时，索引如同虚设。如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IztlWINFdAfteJwovqXjY83dW6u76s14LicdbvAGC4Z855PSnNHwibicPLg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

其实这个也是跟mySQL优化器有关，如果优化器觉得即使走了索引，还是需要扫描很多很多行的哈，它觉得不划算，不如直接不走索引。平时我们用！= 或者< >，not in的时候，留点心眼哈。

### 8 左右连接，关联的字段编码格式不一样

新建两个表，一个`user`，一个`user_job`

```sql
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) CHARACTER SET utf8mb4 DEFAULT NULL,
  `age` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;

CREATE TABLE `user_job` (
  `id` int(11) NOT NULL,
  `userId` int(11) NOT NULL,
  `job` varchar(255) DEFAULT NULL,
  `name` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

`user`表的`name`字段编码是`utf8mb4`，而`user_job`表的`name`字段编码为`utf8`。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IztVR6zjFQ5iaATBXlpVMic2QZu8ABb9ul1oibEX8QjzdxictEWbh3MkofJg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

执行左外连接查询,`user_job`表还是走全表扫描，如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IziaHyKqgIMsqGwBXty8Xnh0xGsgNl7p9rggpetl3QiapqZpISagUNZiajA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

如果把它们的name字段改为编码一致，相同的SQL，还是会走索引。

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzRukxvib3cVic4dK50whiaUiam0yWAlVm4m2EibLz0FqpAjugONyngKNSEiaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 9 group by使用临时表

group by一般用于分组统计，它表达的逻辑就是根据一定的规则，进行分组。日常开发中，我们使用得比较频繁。如果不注意，很容易产生慢SQL。

#### 2.9.1 group by执行流程

假设有表结构：

```
CREATE TABLE `staff` (
  `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT '主键id',
  `id_card` varchar(20) NOT NULL COMMENT '身份证号码',
  `name` varchar(64) NOT NULL COMMENT '姓名',
  `age` int(4) NOT NULL COMMENT '年龄',
  `city` varchar(64) NOT NULL COMMENT '城市',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=utf8 COMMENT='员工表';
```

我们查看一下这个SQL的执行计划：

```
explain select city ,count(*) as num from staff group by city;
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzHLkBohRuntiaCQmCoKyqTxKlTPAOLibbVMEibZEhSacvgAPzZ4kSWvByA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- Extra 这个字段的`Using temporary`表示在执行分组的时候使用了临时表
- Extra 这个字段的`Using filesort`表示使用了文件排序

group by是怎么使用到临时表和排序了呢？我们来看下这个SQL的执行流程

```
select city ,count(*) as num from staff group by city;
```

1. 创建内存临时表，表里有两个字段`city和num`；
2. 全表扫描staff的记录，依次取出city = 'X'的记录。

- 判断临时表中是否有为`city='X'`的行，没有就插入一个记录` (X,1)`;
- 如果临时表中有`city='X'`的行，就将X这一行的num值加 1；

1. 遍历完成后，再根据字段`city`做排序，得到结果集返回给客户端。这个流程的执行图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1Izdc4RLpYptslOIGLVJvMnYdKDwFJrT105QRQ6fM74KOsHIpUA2lRGKg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**临时表的排序是怎样的呢？**

就是把需要排序的字段，放到sort buffer，排完就返回。在这里注意一点哈，排序分全字段排序和rowid排序

- 如果是全字段排序，需要查询返回的字段，都放入sort buffer，根据排序字段排完，直接返回
- 如果是rowid排序，只是需要排序的字段放入sort buffer，然后多一次回表操作，再返回。

#### 2.9.2 group by可能会慢在哪里？

`group by`使用不当，很容易就会产生慢`SQL`问题。因为它既用到临时表，又默认用到排序。有时候还可能用到磁盘临时表。

- 如果执行过程中，会发现内存临时表大小到达了上限（控制这个上限的参数就是`tmp_table_size`），会把内存临时表转成磁盘临时表。
- 如果数据量很大，很可能这个查询需要的磁盘临时表，就会占用大量的磁盘空间。

#### 2.9.3 如何优化group by呢

**从哪些方向去优化呢？**

- 方向1：既然它默认会排序，我们不给它排是不是就行啦。
- 方向2：既然临时表是影响group by性能的X因素，我们是不是可以不用临时表？

我们一起来想下，执行`group by`语句为什么需要临时表呢？`group by`的语义逻辑，就是统计不同的值出现的个数。如果这个这些值一开始就是有序的，我们是不是直接往下扫描统计就好了，就不用临时表来记录并统计结果啦?

可以有这些优化方案：

- group by 后面的字段加索引
- order by null 不用排序
- 尽量只使用内存临时表
- 使用SQL_BIG_RESULT

### 10  delete + in子查询不走索引！

之前见到过一个生产慢SQL问题，当delete遇到in子查询时，即使有索引，也是不走索引的。而对应的select + in子查询，却可以走索引。

MySQL版本是5.7，假设当前有两张表account和old_account,表结构如下：

```sql
CREATE TABLE `old_account` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键Id',
  `name` varchar(255) DEFAULT NULL COMMENT '账户名',
  `balance` int(11) DEFAULT NULL COMMENT '余额',
  `create_time` datetime NOT NULL COMMENT '创建时间',
  `update_time` datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='老的账户表';

CREATE TABLE `account` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键Id',
  `name` varchar(255) DEFAULT NULL COMMENT '账户名',
  `balance` int(11) DEFAULT NULL COMMENT '余额',
  `create_time` datetime NOT NULL COMMENT '创建时间',
  `update_time` datetime NOT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (`id`),
  KEY `idx_name` (`name`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1570068 DEFAULT CHARSET=utf8 ROW_FORMAT=REDUNDANT COMMENT='账户表';
```

执行的SQL如下：

```
delete from account where name in (select name from old_account);
```

查看执行计划，发现不走索引：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzUt2KOddhxYBfRhBIPb28d9syibyCSAZ0eJlNzqfRDAkaicB8iaqVFBmzw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)但是如果把delete换成select，就会走索引。如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpxtHVxOiagMUjb3RUGAYD1IzOZpgKVpQvopHgrj26fyHzIqdf81cOibK3Rzpviau9JWoib8B8btkckX9w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

为什么`select + in`子查询会走索引，`delete + in`子查询却不会走索引呢？

我们执行以下SQL看看：

```sql
explain select * from account where name in (select name from old_account);
show WARNINGS; //可以查看优化后,最终执行的sql
```

结果如下：

```sql
select `test2`.`account`.`id` AS `id`,`test2`.`account`.`name` AS `name`,`test2`.`account`.`balance` AS `balance`,`test2`.`account`.`create_time` AS `create_time`,`test2`.`account`.`update_time` AS `update_time` from `test2`.`account` 
semi join (`test2`.`old_account`)
where (`test2`.`account`.`name` = `test2`.`old_account`.`name`)
```

可以发现，实际执行的时候，MySQL对select in子查询做了优化，把子查询改成join的方式，所以可以走索引。但是很遗憾，对于`delete in`子查询，MySQL却没有对它做这个优化。

日常开发中，大家注意一下这个场景哈



# 如何写出一手好 SQL ？很有必要！

## 慢SQL原因

最近频繁出现慢SQL告警，**执行时间最长的竟然高达5分钟**。导出日志后分析，主要原因竟然是**没有命中索引和没有分页处理** 。其实这是非常低级的错误，我不禁后背一凉，团队成员的技术水平亟待提高啊

### 1 最大数据量

**MySQL性能**，**最大数据量**

**抛开数据量和并发数，谈性能都是耍流氓** 。MySQL没有限制单表最大记录数，它取决于操作系统对文件大小的限制。

| 文件系统 |                        单文件大小限制                        |
| :------- | :----------------------------------------------------------: |
| FAT32    |                            最大4G                            |
| NTFS     |                           最大64GB                           |
| NTFS5.0  |                           最大2TB                            |
| EXT2     | 块大小为1024字节，文件最大容量16GB；块大小为4096字节，文件最大容量2TB |
| EXT3     |                块大小为4KB，文件最大容量为4TB                |
| EXT4     |                       理论可以大于16TB                       |

> 阿里巴巴Java开发手册提出单表行数超过500万行或者单表容量超过2GB，才推荐分库分表。性能由综合因素决定，抛开业务复杂度，影响程度依次是硬件配置、MySQL配置、数据表设计、索引优化。
>
> 500万这个值仅供参考，并非铁律。

> 博主曾经操作过超过4亿行数据的单表，分页查询最新的20条记录耗时0.6秒，SQL语句大致是 `select field_1,field_2 from table where id < #{prePageMinId} order by id desc limit 20`，prePageMinId是上一页数据记录的最小ID。虽然当时查询速度还凑合，随着数据不断增长，有朝一日必定不堪重负。

> 分库分表是个周期长而风险高的大活儿，应该尽可能在当前结构上优化，比如升级硬件、迁移历史数据等等，实在没辙了再分。对分库分表感兴趣的同学可以阅读分库分表的基本思想。

### 2 最大并发数

并发数是指同一时刻数据库能处理多少个请求，由max*connections和max*user*connections决定。maxconnections是指MySQL实例的最大连接数，上限值是16384，max*user*connections是指每个数据库用户的最大连接数。

MySQL会为每个连接提供缓冲区，意味着消耗更多的内存。如果连接数设置太高硬件吃不消，太低又不能充分利用硬件。一般要求两者比值超过10%，计算方法如下：

```apl
max_used_connections / max_connections * 100% = 3/100 *100% ≈ 3%
```

查看最大连接数与响应最大连接数：

```sql
show variables like '%max_connections%';show variables like '%max_user_connections%';
```

在配置文件my.cnf中修改最大连接数

```apl
[mysqld]max_connections = 100max_used_connections = 20
```

## 查询耗时0.5秒

> 建议将单次查询耗时控制在0.5秒以内，0.5秒是个经验值，源于用户体验的 **3秒原则** 。如果用户的操作3秒内没有响应，将会厌烦甚至退出。响应时间=客户端UI渲染耗时+网络请求耗时+应用程序处理耗时+查询数据库耗时，0.5秒就是留给数据库1/6的处理时间。

## 实施原则

> 相比NoSQL数据库，MySQL是个娇气脆弱的家伙。它就像体育课上的女同学，一点纠纷就和同学闹别扭(扩容难)，跑两步就气喘吁吁(容量小并发低)，常常身体不适要请假(SQL约束太多)。如今大家都会搞点分布式，应用程序扩容比数据库要容易得多，所以实施原则是 **数据库少干活，应用程序多干活** 。

> - 充分利用但不滥用索引，须知索引也消耗磁盘和CPU。
> - 不推荐使用数据库函数格式化数据，交给应用程序处理。
> - 不推荐使用外键约束，用应用程序保证数据准确性。
> - 写多读少的场景，不推荐使用唯一索引，用应用程序保证唯一性。
> - 适当冗余字段，尝试创建中间表，用应用程序计算中间结果，用空间换时间。
> - 不允许执行极度耗时的事务，配合应用程序拆分成更小的事务。
> - 预估重要数据表（比如订单表）的负载和数据增长态势，提前优化。

### 1 数据类型的选择

> - 如果长度能够满足，整型尽量使用tinyint、smallint、medium_int而非int。
> - 如果字符串长度确定，采用char类型。
> - 如果varchar能够满足，不采用text类型。
> - 精度要求较高的使用decimal类型，也可以使用BIGINT，比如精确两位小数就乘以100后保存。
> - 尽量采用timestamp而非datetime

| 类型      | 占据字节 |                             描述                             |
| :-------- | :------- | :----------------------------------------------------------: |
| datetime  | 8字节    | '1000-01-01 00:00:00.000000' to '9999-12-31 23:59:59.999999  |
| timestamp | 4字节    | '1970-01-01 00:00:01.000000' to '2038-01-19 03:14:07.999999' |

相比datetime，timestamp占用更少的空间，以UTC的格式储存自动转换时区。[MySQL开发的 36 条军规](http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw==&mid=2247486173&idx=2&sn=0603756b36279b5f5f036151af85f9fd&chksm=eb538febdc2406fd70748604c500eeaea584306c137cb2020ab5a72187c9359926015e00a46d&scene=21#wechat_redirect)建议你看下。

### 2 避免空值

> MySQL中字段为NULL时依然占用空间，会使索引、索引统计更加复杂。从NULL值更新到非NULL无法做到原地更新，容易发生索引分裂影响性能。尽可能将NULL值用有意义的值代替，也能避免SQL语句里面包含 `is not null`的判断。

### 3 text类型优化

> 由于text字段储存大量数据，表容量会很早涨上去，影响其他字段的查询性能。建议抽取出来放在子表里，用业务主键关联。

### 4 索引分类

> 1. 普通索引：最基本的索引。
> 2. 组合索引：多个字段上建立的索引，能够加速复合查询条件的检索。
> 3. 唯一索引：与普通索引类似，但索引列的值必须唯一，允许有空值。
> 4. 组合唯一索引：列值的组合必须唯一。
> 5. 主键索引：特殊的唯一索引，用于唯一标识数据表中的某一条记录，不允许有空值，一般用primary key约束。
> 6. 全文索引：用于海量文本的查询，MySQL5.6之后的InnoDB和MyISAM均支持全文索引。由于查询精度以及扩展性不佳，更多的企业选择Elasticsearch。

### 5 索引优化

> 1. 分页查询很重要，如果查询数据量超过30%，MYSQL不会使用索引。
> 2. 单表索引数不超过5个、单个索引字段数不超过5个。
> 3. 字符串可使用前缀索引，前缀长度控制在5-8个字符。
> 4. 字段唯一性太低，增加索引没有意义，如：是否删除、性别。
> 5. 合理使用覆盖索引，如下所示：select login*name, nick*name from member where login_name = ?

login*name, nick*name两个字段建立组合索引，比login_name简单索引要更快

## SQL优化

### 分批处理

博主小时候看到鱼塘挖开小口子放水，水面有各种漂浮物。浮萍和树叶总能顺利通过出水口，而树枝会挡住其他物体通过，有时还会卡住，需要人工清理。MySQL就是鱼塘，最大并发数和网络带宽就是出水口，用户SQL就是漂浮物。

不带分页参数的查询或者影响大量数据的update和delete操作，都是树枝，我们要把它打散分批处理，举例说明：业务描述：更新用户所有已过期的优惠券为不可用状态。

SQL语句：`update status=0 FROM`coupon `WHERE expire_date <= #{currentDate} and status=1;`如果大量优惠券需要更新为不可用状态，执行这条SQL可能会堵死其他SQL，分批处理伪代码如下：

```java
int pageNo = 1;
int PAGE_SIZE = 100;
while(true) {
    List<Integer> batchIdList = queryList('select id FROM `coupon` WHERE expire_date <= #{currentDate} and status = 1 limit #{(pageNo-1) * PAGE_SIZE},#{PAGE_SIZE}');
    if (CollectionUtils.isEmpty(batchIdList)) {
        return;
    }
    update('update status = 0 FROM `coupon` where status = 1 and id in #{batchIdList}')
    pageNo ++;
}
```

### 操作符<>优化

通常<>操作符无法使用索引，举例如下，查询金额不为100元的订单：`select id from orders where amount != 100;`如果金额为100的订单极少，这种数据分布严重不均的情况下，有可能使用索引。鉴于这种不确定性，采用union聚合搜索结果，改写方法如下：

```sql
(select id from orders where amount > 100) 
union all
(select id from orders where amount < 100 and amount > 0)
```

### OR优化

在Innodb引擎下or无法使用组合索引，比如：

```sql
select id，product_name from orders where mobile_no = '13421800407' or user_id = 100;
```

OR无法命中mobile*no + user*id的组合索引，可采用union，如下所示：

```sql
(select id，product_name from orders where mobile_no = '13421800407') 
union
(select id，product_name from orders where user_id = 100);
```

此时id和product_name字段都有索引，查询才最高效。



# 如何科学破解慢SQL

今天和大家聊一个常见的问题：慢SQL。

通过本文你将了解到以下内容：

- 慢SQL的危害
- SQL语句的执行过程
- 存储引擎和索引的那些事儿
- 慢SQL解决之道

后续均以MySQL默认存储引擎InnoDB为例进行展开，话不多说，开搞！

## 1.慢SQL的危害

慢SQL，就是跑得很慢的SQL语句，你可能会问慢SQL会有啥问题吗？试想一个场景：

> 大白和小黑端午出去玩，机票太贵于是买了高铁，火车站的人真是乌央乌央的。

> 马上检票了，大白和小黑准备去厕所清理下库存，坑位不多，排队的人还真不少。

> 小黑发现其中有3个坑的乘客贼慢，其他2个坑位换了好几波人，这3位坑主就是不出来。

> 等在外面的大伙，心里很是不爽，长期占用公共资源，后面的人没法用。

> 小黑苦笑道：这不就是厕所版的慢SQL嘛！

这是实际生活中的例子，换到MySQL服务器也是一样的，毕竟科技源自生活嘛。

MySQL服务器的资源(CPU、IO、内存等)是有限的，尤其在高并发场景下需要快速处理掉请求，否则一旦出现慢SQL就会阻塞掉很多正常的请求，造成大面积的失败/超时等。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101433894.png" alt="image-20221210143302732" style="zoom: 50%;" />

## 2.SQL语句执行过程

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101435588.png" alt="image-20221210143525465" style="zoom:67%;" />

客户端和MySQL服务端的交互过程简介：

> 1. 客户端发送一条SQL语句给服务端，服务端的连接器先进行账号/密码、权限等验证，有异常直接拒绝请求
> 2. 服务端查询缓存，如果SQL语句命中了缓存，则返回缓存中的结果，否则继续处理。
> 3. 服务端对SQL语句进行词法解析、语法解析、预处理来检查SQL语句的合法性。
> 4. 服务端通过优化器对之前生成的解析树进行优化处理，生成最优的物理执行计划。
> 5. 将生成的物理执行计划调用存储引擎的相关接口，进行数据查询和处理。
> 6. 处理完成后将结果返回客户端。

客户端和MySQL服务端的交互过程简图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101437875.png" alt="image-20221210143729673" style="zoom:67%;" />

俗话说"条条大路通罗马"，优化器的作用就是找到这么多路中最优的那一条。

存储引擎更是决定SQL执行的核心组件，适当了解其中原理十分有益。

## 3. 存储引擎和索引的那些事儿

### 1 存储引擎

InnoDB存储引擎(Storage Engine)是MySQL默认之选，所以非常典型。

存储引擎的主要作用是进行数据的存取和检索，也是真正执行SQL语句的组件。

InnoDB的整体架构分为两个部分：内存架构和磁盘架构，如图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101437435.png" alt="image-20221210143752126" style="zoom:50%;" />

> 存储引擎的内容非常多，并不是一篇文章能说清楚的，本文不过多展开，我们在此只需要了解内存架构和磁盘架构的大致组成即可。

InnoDB 引擎是面向行存储的，数据都是存储在磁盘的数据页中，数据页里面按照固定的行格式存储着每一行数据。

> 行格式主要分为四种类型Compact、Redundant、Dynamic和Compressed，默认为Compact格式。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101438871.png" alt="image-20221210143822710" style="zoom:67%;" />

#### 磁盘预读机制和局部性原理

当计算机访问一个数据时，不仅会加载当前数据所在的数据页，还会将当前数据页相邻的数据页一同加载到内存，磁盘预读的长度一般为页的整倍数，从而有效降低磁盘IO的次数。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101438124.png" alt="image-20221210143844973" style="zoom:50%;" />

#### 磁盘和内存的交互

MySQL中磁盘的数据需要被交换到内存，才能完成一次SQL交互，大致如图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101439051.png" alt="image-20221210143903911" style="zoom:50%;" />

- 扇区是硬盘的读写的基本单位，通常情况下每个扇区的大小是 512B
- 磁盘块文件系统读写数据的最小单位，相邻的扇区组合在一起形成一个块，一般是4KB
- 页是内存的最小存储单位，页的大小通常为磁盘块大小的 2^n 倍
- InnoDB页面的默认大小是16KB，是数倍个操作系统的页

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101439815.png" alt="image-20221210143926681" style="zoom:50%;" />

#### 随机磁盘IO

MySQL的数据是一行行存储在磁盘上的，并且这些数据并非物理连续地存储，这样的话要查找数据就无法避免随机在磁盘上读取和写入数据。

对于MySQL来说，当出现大量磁盘随机IO时，大部分时间都被浪费到寻道上，磁盘呼噜呼噜转，就是传输不了多少数据。

> 一次磁盘访问由三个动作组成：
>
> - 寻道：磁头移动定位到指定磁道
> - 旋转：等待指定扇区从磁头下旋转经过
> - 数据传输：数据在磁盘与内存之间的实际传输

对于存储引擎来说，如何有效降低随机IO是个非常重要的问题。

### 2 索引

可以实现增删改查的数据结构非常多，包括：哈希表、二叉搜索树、AVL、红黑树、B树、B+树等，这些都是可以作为索引的候选数据结构。

结合MySQL的实际情况：磁盘和内存交互、随机磁盘IO、排序和范围查找、增删改的复杂度等等，综合考量之下B+树脱颖而出。

B+树作为多叉平衡树，对于范围查找和排序都可以很好地支持，并且更加矮胖，访问数据时的平均磁盘IO次数取决于树的高度，因此B+树可以让磁盘的查找次数更少。

在InnoDB中B+树的高度一般都在2~4层，并且根节点常驻内存中，也就是说查找某值的行记录时最多只需要1~3次磁盘I/O操作。

MyISAM是将数据和索引分开存储的，InnoDB存储引擎的数据和索引没有分开存储，这也就是为什么有人说Innodb索引即数据，数据即索引，如图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101439824.png" alt="image-20221210143956642" style="zoom:50%;" />

说到InnoDB的数据和索引的存储，就提到一个名词：**聚集索引**。

#### 聚集索引

聚集索引将索引和数据完美地融合在一起，是每个Innodb表都会有的一个特殊索引，一般来说是借助于表的主键来构建的B+树。

假设我们有student表，将id作为主键索引，那么聚集索引的B+树结构，如图：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101440795.png" alt="image-20221210144013633" style="zoom:67%;" />

- 非叶子节点不存数据，只有主键和相关指针
- 叶子节点包含主键、行数据、指针
- 叶子节点之间由双向指针串联形成有序双向链表，叶子节点内部也是有序的

聚集索引按照如下规则创建：

- 有主键时InnoDB利用主键来生成
- 没有主键，InnoDB会选择一个非空的唯一索引来创建
- 无主键且非NULL唯一索引时，InnoDB会隐式创建一个自增的列来创建

假如我们要查找id=10的数据，大致过程如下：

- 索引的根结点在内存中，10>9 因此找到P3指针
- P3指向的数据并没有在内存中，因此产生1次磁盘IO读取磁盘块3到内存
- 在内存中对磁盘块3进行二分查找，找到ID=9的全部值

#### 非聚集索引

非聚集索引的叶子节点中存放的是二级索引值和主键键值，非叶子节点和叶子节点都没有存储整行数据值。

假设我们有student表，将name作为二级索引，那么非聚集索引的B+树结构，如图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/wAkAIFs11qaMZss2ia9Bx0yJNIfuKRhiawcfyMslw9iayvVYUmA00ia3xQgP1NRefw2oIWBRhSbqomtK8icuibOC7eiaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

由于非聚集索引的叶子节点没有存储行数据，如果通过非聚集索引来查找非二级索引值，需要分为两步：

- 第一：通过非聚集索引的叶子节点来确定数据行对应的主键
- 第二：通过相应的主键值在聚集索引中查询到对应的行记录

我们把通过非聚集索引找到主键值，再根据主键值从聚集索引找对于行数据的过程称为：**回表查询**。

换句话说：select * from student where name = 'Bob' 将产生回表查询，因为在name索引的叶子节点没有其他值，只能从聚集索引获得。

所以如果查找的字段在非聚集索引就可以完成，就可以避免一次回表过程，这种称为：覆盖索引，所以select * 并不是好习惯，需要什么拿什么就好。

假如我们要查找name=Tom的记录的所有值，大致过程如下：

- 从非聚集索引开始，根节点在内存中，按照name的字典序找到P3指针
- P3指针所指向的磁盘块不在内存中，产生1次磁盘IO加载到内存
- 在内存中对磁盘块3的数据进行搜索，获得name=tom的记录的主键值为4
- 根据主键值4从聚集索引的根节点中获得P2指针
- P2指针所指向的磁盘块不在内存中，产生第2次磁盘IO加载到内存
- 将上一步获得的数据，在内存中进行二分查找获得全部行数据

上述查询就包含了一次回表过程，因此性能比主键查询慢了一倍，因此尽量使用主键查询，一次完事。

## 4. 慢SQL解决思路

出现慢SQL的原因很多，我们抛开单表数亿记录和无索引的特殊情况，来讨论一些更有普遍意义的慢SQL原因和解决之道。

我们从两个方面来进行阐述：

- 数据库表索引设置不合理
- SQL语句有问题，需要优化

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212101444616.png" alt="image-20221210144456493" style="zoom:67%;" />

### 1 索引设置原则

程序员的角度和存储引擎的角度是不一样的，索引写的好，SQL跑得快。

- **索引区分度低**

假如表中有1000w记录，其中有status字段表示状态，可能90%的数据status=1，可以不将status作为索引，因为其对数据记录区分度很低。

- **切忌过多创建索引**

每个索引都需要占用磁盘空间，修改表数据时会对索引进行更新，索引越多，更新越复杂。

> 因为每添加一个索引，.ibd文件中就需要多维护一个B+Tree索引树，如果某一个table中存在10个索引，那么就需要维护10棵B+Tree，写入效率会降低，并且会浪费磁盘空间。

- **常用查询字段建索引**

如果某个字段经常用来做查询条件，那么该字段的查询速度会影响整个表的查询速度，属于热门字段，为其建立索引非常必要。

- **常排序/分组/去重字段建索引**

对于需要经常使用ORDER BY、GROUP BY、DISTINCT和UNION等操作的字段建立索引，可以有效借助B+树的特性来加速执行。

- **主键和外键建索引**

主键可以用来创建聚集索引，外键也是唯一的且常用于表关联的字段，也需要建索引来提高性能。

### 2 SQL的优化

如果数据库表的索引设置比较合理，SQL语句书写不当会造成索引失效，甚至造成全表扫描，迅速拉低性能。

#### 索引失效

我们在写SQL的时候在某些情况下会出现索引失效的情况：

- **对索引使用函数**

> select id from std upper(name) = 'JIM';

- **对索引进行运算**

> select id from std where id+1=10;

- **对索引使用<> 、not in 、not exist、!=**

> select id from std where name != 'jim';

- **对索引进行前导模糊查询**

> select id from std name like '%jim';

- **隐式转换会导致不走索引**

> 比如：字符串类型索引字段不加引号，select id from std name = 100;保持变量类型与字段类型一致

- **非索引字段的or连接**

> 并不是所有的or都会使索引失效，如果or连接的所有字段都设置了索引，是会走索引的，一旦有一个字段没有索引，就会走全表扫描。

- **联合索引仅包含复合索引非前置列**

> 联合索引包含key1，key2，key3三列，但SQL语句没有key1，根据联合索引的最左匹配原则，不会走联合索引。
> select name from table where key2=1 and key3=2;

#### 好的建议

- **使用连接代替子查询**

> 对于数据库来说，在绝大部分情况下，连接会比子查询更快，使用连接的方式，MySQL优化器一般可以生成更佳的执行计划，更高效地处理查询
> 而子查询往往需要运行重复的查询，子查询生成的临时表上也没有索引， 因此效率会更低。

- **LIMIT偏移量过大的优化**

> 禁止分页查询偏移量过大，如limit 100000,10

- **使用覆盖索引**
  减少select * 借助覆盖索引，减少回表查询次数。
- **多表关联查询时，小表在前，大表在后**

> 在MySQL中，执行from后的表关联查询是从左往右执行的，第一张表会涉及到全表扫描，所以将小表放在前面，先扫小表，扫描快效率较高，在扫描后面的大表，或许只扫描大表的前100行就符合返回条件并return了。

- **调整Where字句中的连接顺序**

> MySQL采用从左往右的顺序解析where子句，可以将过滤数据多的条件放在前面，最快速度缩小结果集。

- **使用小范围事务，而非大范围事务**
- **遵循最左匹配原则**
- **使用联合索引，而非建立多个单独索引**

### 3 慢SQL的分析

在分析慢SQL之前需要通过MySQL进行相关设置：

- 开启慢SQL日志
- 设置慢SQL的执行时间阈值

```sql
开启：SET GLOBAL slow_query_log = 1;
开启状态：SHOW VARIABLES LIKE '%slow_query_log%';
设置阈值：SET GLOBAL long_query_time=3;
查看阈值：SHOW GLOBAL VARIABLES LIKE 'long_query_time%'; 
```

#### explain分析SQL

explain命令只需要加在select之前即可，例如:

> explain select * from std where id < 100;

该命令会展示sql语句的详细执行过程，帮助我们定位问题，网上关于explain的用法和讲解很多，本文不再展开。



# 尽量避免使用 IN 和 NOT IN

IN 和 NOT IN 是比较常用的关键字，为什么要尽量避免呢？

## 1、效率低

项目中遇到这么个情况：t1表 和 t2表  都是150w条数据，600M的样子，都不算大。

但是这样一句查询 ↓

```sql
select * from t1 where phone not in (select phone from t2)
```

直接就把我跑傻了。。。

十几分钟，检查了一下  phone在两个表都建了索引，字段类型也是一样的。原来 not in 是不能命中索引的。。。。

改成 NOT EXISTS 之后查询 20s ，效率真的差好多。

```sql
select * from t1
where  not  EXISTS (select phone from t2  where t1.phone =t2.phone)
```

## 2、容易出现问题，或查询结果有误

以 IN 为例。建两个表：test1 和 test2

```sql
create table test1 (id1 int)
create table test2 (id2 int)

insert into test1 (id1) values (1),(2),(3)
insert into test2 (id2) values (1),(2)
```

我想要查询，在test2中存在的  test1中的id 。

使用 IN 的一般写法是：

```sql
select id1 from test1
where id1 in (select id2 from test2)
```

结果是：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TNUwKhV0JpRCHeXZA1QmGEhziccBictFUF9tc8voEhaibDD8QqjjIgT1f1mKqxBKyj4lcl1qVHic73Lb5iaziccBV2Fw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

OK 木有问题！但是如果我一时手滑，写成了：

```sql
select id1 from test1
where id1 in (select id1 from test2)
```

不小心把id2写成id1了 ，会怎么样呢?结果是：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TNUwKhV0JpRCHeXZA1QmGEhziccBictFUFMgS6uicEGibkqgbLiapnskVS20OlBoQdrtzMLbde02gzbcuT7uRzxTHAA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

EXCUSE ME！为什么不报错？

单独查询 `select id1 from test2` 是一定会报错: 消息 207，级别 16，状态 1，第 11 行 列名 'id1' 无效。

然而使用了IN的子查询就是这么敷衍，直接查出 1 2 3

这仅仅是容易出错的情况，自己不写错还没啥事儿，下面来看一下 NOT IN 直接查出错误结果的情况：

给test2插入一个空值：

```sql
insert into test2 (id2) values (NULL)
```

我想要查询，在test2中不存在的  test1中的 id 。

```sql
select id1 from test1
where id1 not in (select id2 from test2)
```

结果是：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/TNUwKhV0JpRCHeXZA1QmGEhziccBictFUF7cuwbJgaTFIdphDs6LbOIetkiaibiaQUibf1LNHG4ZsTeCbib7MNdHNNNow/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

空白！显然这个结果不是我们想要的。我们想要3。为什么会这样呢？

原因是：NULL不等于任何非空的值啊！如果id2只有1和2， 那么3<>1 且 3<>2 所以3输出了，但是 id2包含空值，那么 3也不等于NULL 所以它不会输出。

> 跑题一句：建表的时候最好不要允许含空值，否则问题多多。

## 3、如何替换 IN

### 1、用 EXISTS 或 NOT EXISTS 代替

```sql
select *  from test1
   where EXISTS (select * from test2  where id2 = id1 )

select *  FROM test1
 where NOT EXISTS (select * from test2  where id2 = id1 )
```

### 2、用JOIN 代替

```sql
select id1 from test1 INNER JOIN test2 ON id2 = id1

select id1 from test1
LEFT JOIN test2 ON id2 = id1
where id2 IS NULL
```

妥妥的没有问题了

> PS：那我们死活都不能用 IN 和 NOT IN 了么？并没有，一位大神曾经说过，如果是确定且有限的集合时，可以使用。如 IN （0，1，2）。

# 为什么 SELECT * 效率低

无论在工作还是面试中，关于SQL中不要用“SELECT *”，都是大家听烂了的问题，虽说听烂了，但普遍理解还是在很浅的层面，并没有多少人去追根究底，探究其原理。

## 效率低的原因

先看一下最新《阿里java开发手册（泰山版）》中 MySQL 部分描述： 【强制】在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。说明：

> - 增加查询分析器解析成本。
> - 增减字段容易与 resultMap 配置不一致。
> - 无用字段增加网络 消耗，尤其是 text 类型的字段。

开发手册中比较概括的提到了几点原因，让我们深入一些看看：

### 1. 不需要的列会增加数据传输时间和网络开销

> - 用“SELECT * ”数据库需要解析更多的对象、字段、权限、属性等相关内容，在 SQL 语句复杂，硬解析较多的情况下，会对数据库造成沉重的负担。
> - 增大网络开销；* 有时会误带上如log、IconMD5之类的无用且大文本字段，数据传输size会几何增涨。如果DB和应用程序不在同一台机器，这种开销非常明显。
> - 即使 mysql 服务器和客户端是在同一台机器上，使用的协议还是 tcp，通信也是需要额外的时间。

### 2. 对于无用的大字段，如 varchar、blob、text，会增加 io 操作

> 准确来说，长度超过 728 字节的时候，会先把超出的数据序列化到另外一个地方，因此读取这条记录会增加一次 io 操作。（MySQL InnoDB）

### 3. 失去MySQL优化器“覆盖索引”策略优化的可能性

> SELECT * 杜绝了覆盖索引的可能性，而基于MySQL优化器的“覆盖索引”策略又是速度极快，效率极高，业界极为推荐的查询优化方式。

例如，有一个表为t(a,b,c,d,e,f)，其中，a为主键，b列有索引。

> 那么，在磁盘上有两棵 B+ 树，即聚集索引和辅助索引（包括单列索引、联合索引），分别保存(a,b,c,d,e,f)和(a,b)，如果查询条件中where条件可以通过b列的索引过滤掉一部分记录，查询就会先走辅助索引，如果用户只需要a列和b列的数据，直接通过辅助索引就可以知道用户查询的数据。

> 如果用户使用select *，获取了不需要的数据，则首先通过辅助索引过滤数据，然后再通过聚集索引获取所有的列，这就多了一次b+树查询，速度必然会慢很多。

> 由于辅助索引的数据比聚集索引少很多，很多情况下，通过辅助索引进行覆盖索引（通过索引就能获取用户需要的所有列），都不需要读磁盘，直接从内存取，而聚集索引很可能数据在磁盘（外存）中（取决于buffer pool的大小和命中率），这种情况下，一个是内存读，一个是磁盘读，速度差异就很显著了，几乎是数量级的差异。

## 索引知识延申

上面提到了辅助索引，在MySQL中辅助索引包括单列索引、联合索引（多列联合），单列索引就不再赘述了，这里提一下联合索引的作用。

### 联合索引 (a,b,c)

> 联合索引 (a,b,c) 实际建立了 (a)、(a,b)、(a,b,c) 三个索引

> 我们可以将组合索引想成书的一级目录、二级目录、三级目录，如index(a,b,c)，相当于a是一级目录，b是一级目录下的二级目录，c是二级目录下的三级目录。要使用某一目录，必须先使用其上级目录，一级目录除外。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/VbfrPx9GoVeZQJUaYu0X0xHZpLRjuzxQ3RraCcbr7Ee43MrFOPiaYbyAGg7vicN009LL7YNicicpsa9ZJkiaJJAYS3A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 联合索引的优势

### 1） 减少开销

> 建一个联合索引 (a,b,c) ，实际相当于建了 (a)、(a,b)、(a,b,c) 三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！

### 2）覆盖索引

对联合索引 (a,b,c)，如果有如下 sql 的，

```
SELECT a,b,c from table where a='xx' and b = 'xx';
```

> 那么 MySQL 可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机 io 操作。减少 io 操作，特别是随机 io 其实是 DBA 主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。

### 3）效率高

索引列多，通过联合索引筛选出的数据越少。比如有 1000W 条数据的表，有如下SQL:

```
select col1,col2,col3 from table where col1=1 and col2=2 and col3=3;
```

假设：假设每个条件可以筛选出 10% 的数据。

- A. 如果只有单列索引，那么通过该索引能筛选出 1000W 10%=100w 条数据，然后再回表从 100w 条数据中找到符合 col2=2 and col3= 3 的数据，然后再排序，再分页，以此类推（递归）；
- B. 如果是（col1,col2,col3）联合索引，通过三列索引筛选出 1000w 10% 10% *10%=1w，效率提升可想而知！

### 4)  索引是建的越多越好吗

答案自然是否定的

> - 数据量小的表不需要建立索引，建立会增加额外的索引开销
> - 不经常引用的列不要建立索引，因为不常用，即使建立了索引也没有多大意义
> - 经常频繁更新的列不要建立索引，因为肯定会影响插入或更新的效率
> - 数据重复且分布平均的字段，因此他建立索引就没有太大的效果（例如性别字段，只有男女，不适合建立索引）
> - 数据变更需要维护索引，意味着索引越多维护成本越高。
> - 更多的索引也需要更多的存储空间





# 隐式转换带来的cpu飙升

## 问题背景

本来是一个平静而美好的下午，其他部门的同事要一份数据报表临时汇报使用，因为系统目前没有这个维度的功能，所以需要写个SQL马上出一下，一个同事接到这个任务，于是开始在测试环境拼装这条 SQL，刚过了几分钟，同事已经自信的写好了这条SQL，于是拿给DBA，到线上跑一下，用客户端工具导出Excel 就好了，毕竟是临时方案嘛。

就在SQL执行了之后，意外发生了，先是等了一下，发现还没执行成功，猜测可能是数据量大的原因，但是随着时间滴滴答答流逝，逐渐意识到情况不对了，一看监控，CPU已经上去了，但是线上数据量虽然不小，也不至于跑成这样吧，眼看着要跑死了，赶紧把这个事务结束掉了。

什么原因呢？查询的条件和 join 连接的字段基本都有索引，按道理不应该这样啊，于是赶紧把SQL拿下来，也没看出什么问题，于是限制查询条数再跑了一次，很快出结果了，但是结果却大跌眼镜，出来的查询结果并不是预期的。

经过一番检查之后，最终发现了问题所在，是 join 连接中有一个字段写错了，因为这两个字段有一部分名称是相同的，于是智能的 SQL 客户端给出了提示，顺手就给敲上去了。但是接下来，更让人迷惑了，因为要连接的字段是 int 类型，而写错的这个字段是 varchar 类型，难道不应该报错吗？怎么还能正常执行，并且还有预期外的查询结果？

难道是 MySQL 有 bug 了，必须要研究一下了。

## 复现当时的场景

假设有两张表，这两张表的结构和数据是下面这样的。

第一张 `user`表。

```sql
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(50) COLLATE utf8_bin DEFAULT NULL,
  `age` int(3) DEFAULT NULL,
  `create_time` datetime DEFAULT NULL,
  `update_time` datetime DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 COLLATE=utf8_bin;


INSERT INTO `user` VALUES (1, '张三', 28, '2022-09-06 07:40:56', '2022-09-06 07:40:59');
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212171035441.png" alt="image-20221217103523300" style="zoom:80%;" />

第二张 `order`表

```sql
CREATE TABLE `order` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` int(11) DEFAULT NULL,
  `order_code` varchar(64) COLLATE utf8_bin DEFAULT NULL,
  `money` decimal(20,0) DEFAULT NULL,
  `title` varchar(255) COLLATE utf8_bin DEFAULT NULL,
  `create_time` datetime DEFAULT NULL,
  `update_time` datetime DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 COLLATE=utf8_bin;


INSERT INTO `order` VALUES (1, 2, '1d90530e-6ada-47c1-b2fa-adba4545aabd', 100, 'xxx购买两件商品', '2022-09-06 07:42:25', '2022-09-06 07:42:27');
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/iaWSDo4TfyZgbMHOd9kxozlMumXFVbj32spBNmWCQ8iajNMXyxbVsPnM8SKMEzCBE0ia2icvbs1ciaP6qoD3yQtL7Mg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

目的是查看所有用户的 order 记录，假设数据量比较少，可以直接查，不考虑性能问题。

本来的 SQL 语句应该是这样子的，查询 `order`表中用户id`user_id`在`user`表的记录。

```sql
select o.* from `user` u 
left JOIN `order` o 
on u.id = o.user_id;
```

但是呢，因为手抖，将 on 后面的条件写成了 `u.id = o.order_code`，完全关联错误，这两个字段完全没有联系，而且`u.id`是 int 类型，`o.order_code`是`varchar`类型。

```sql
select o.* from `user` u 
left JOIN `order` o 
on u.id = o.order_code;
```

这样的话， 当我们执行这条语句的时候，会不会查出数据来呢？

我的第一感觉是，不仅不会查出数据，而且还会报错，因为连接的这两个字段类型都不一样，值更不一样。

结果却被啪啪打脸，不仅没有报错，而且还查出了数据。

![图片](https://mmbiz.qpic.cn/mmbiz_png/iaWSDo4TfyZgbMHOd9kxozlMumXFVbj32TToCUwUQicfjRfDnicGSrWIDAb8J5WkJbuc9tb2mDnicMiacwVMeMs3UtA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

可以把这个问题简化一下，简化成下面这条语句，同样也会出现问题。

```sql
select * from `order` where order_code = 1;
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/iaWSDo4TfyZgbMHOd9kxozlMumXFVbj32r7RKWibTdOICjS8kk4JgooIn2CU0C95TvyeLBMIyAo0cu4Z1mohm5TA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

明明这条记录的 order_code 字段的值是 `1d90530e-6ada-47c1-b2fa-adba4545aabd`，怎么用 `order_code=1`的条件就把它给查出来了。

## 根源所在

相信有的同学已经猜出来了，这里是 MySQL 进行了隐式转换，由于查询条件后面跟的查询值是整型的，所以 MySQL 将 `order_code`字段进行了字符串到整数类型的转换，而转换后的结果正好是 `1`。

通过 `cast`函数转换验证一下结果。

```sql
select cast('1d90530e-6ada-47c1-b2fa-adba4545aabd' as unsigned);
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212171037789.png" alt="image-20221217103737667" style="zoom:67%;" />

再用两条 SQL 看一下字符串到整数类型转换的规则。

```sql
select cast('223kkk' as unsigned);
select cast('k223kkk' as unsigned);
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212171037886.png" alt="image-20221217103759658" style="zoom:50%;" />

`223kkk`转换后的结果是 `223`，而`k223kkk`转换后的结果是0。总结一下，转换的规则是：

1、从字符串的左侧开始向右转换，遇到非数字就停止；

2、如果第一个就是非数字，最后的结果就是0；

## 隐式转换的规则

当操作符与不同类型的操作数一起使用的时候，就会发生隐式转换。

例如算数运算符的前后是不同类型时，会将非数字类型转换为数字，比如 '5a'+2，就会将`5a`转换为数字类型，然后和2相加，最后的结果就是 7 。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212171038051.png" alt="image-20221217103824977" style="zoom:67%;" />

再比如 `concat`函数是连接两个字符串的，当此函数的参数出现非字符串类型时，就会将其转换为字符串，例如concat(88,'就是发')，最后的结果就是 `88就是发`。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212171038543.png" alt="image-20221217103841441" style="zoom:50%;" />

MySQL 官方文档有以下几条关于隐式转换的规则：

> 1、两个参数至少有一个是 NULL 时，比较的结果也是 NULL，例外是使用 <=> 对两个 NULL 做比较时会返回 1，这两种情况都不需要做类型转换；也就是两个参数中如果只有一个是NULL，则不管怎么比较结果都是 NULL，而两个 NULL 的值不管是判断大于、小于或等于，其结果都是1。

> 2、两个参数都是字符串，会按照字符串来比较，不做类型转换；

> 3、两个参数都是整数，按照整数来比较，不做类型转换；

> 4、十六进制的值和非数字做比较时，会被当做二进制字符串；

例如下面这条语句，查询 user 表中name字段是 0x61 的记录，`0x`是16进制写法，其对应的字符串是英文的 'a'，也就是它对应的 ASCII 码。

```sql
select * from user where name = 0x61;
```

所以，上面这条语句其实等同于下面这条

```sql
select * from user where name = 'a';
```

可以用 `select 0x61;`验证一下。

> 5、有一个参数是 TIMESTAMP 或 DATETIME，并且另外一个参数是常量，常量会被转换为 时间戳；

例如下面这两条SQL，都是将条件后面的值转换为时间戳再比较了。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.12.30/202212171040223.png" alt="image-20221217104016044" style="zoom:67%;" />

> 6、有一个参数是 decimal 类型，如果另外一个参数是 decimal 或者整数，会将整数转换为 decimal 后进行比较，如果另外一个参数是浮点数（一般默认是 double），则会把 decimal 转换为浮点数进行比较；

> 在不同的数值类型之间，总是会向精度要求更高的那一个类型转换，但是有一点要注意，在MySQL 中浮点数的精度只有53 bit，超过53bit之后的话，如果后面1位是1就进位，如果是0就直接舍弃。所以超大浮点数在比较的时候其实只是取的近似值。

> 7、所有其他情况下，两个参数都会被转换为浮点数再进行比较；

> 如果不符合上面6点规则，则统一转成浮点数再进行运算

## 避免进行隐式转换

我们在平时的开发过程中，尽量要避免隐式转换，因为一旦发生隐式转换除了会降低性能外， 还有很大可能会出现不期望的结果，就像我最开始遇到的那个问题一样。

之所以性能会降低，还有一个原因就是让本来有的索引失效。

```sql
select * from `order` where order_code = 1;
```

> order_code 是 varchar 类型，假设我已经在 order_code 上建立了索引，如果是用“=”做查询条件的话，应该直接命中索引才对，查询速度会很快。但是，当查询条件后面的值类型不是 varchar，而是数值类型的话，MySQL 首先要对 order_code 字段做类型转换，转换为数值类型，这时候，之前建的索引也就不会命中，只能走全表扫描，查询性能指数级下降，搞不好，数据库直接查崩了。

# SQL 效率优化技巧

所有的数据相关工作人员，包括数据开发、数据分析师、数据科学家等，多多少少会使用数据库，我们很多的业务数据也是存放在业务表中。但即使是同一个需求，不同人写出的 SQL 效率上也会有很大差别，而我们在数据岗位面试的时候，也会考察相关的技能和思考，在本篇文章中，将给大家梳理 SQL 中可以用于优化效率和提速的核心要求。

## 使用正则regexp_like代替LIKE

如下例所示，当我们要进行模糊匹配的时候（尤其是匹配项很多的时候），使用regexp_like代替LIKE可以提高效率。

💦 低效代码

```sql
SELECT *
FROM phones
WHERE
    lower(name) LIKE '%samsing&' OR
    lower(name) LIKE '%apple&' OR
    lower(name) LIKE '%htc&' OR
```

💦 高效代码

```sql
SELECT *
FROM phones
WHERE
    REGEXP_LIKE(lower(name),'samsung|apple|htc')
```

## 使用regexp_extract代替 Case-when Like

类似的，使用regexp_extract代替Case-when Like可以提高效率。

💦 低效代码

```sql
SELECT *
CASE
    WHEN concat(' ', name, ' ') LIKE '%acer%' then 'Acer' 
    WHEN concat(' ', name, ' ') LIKE '%samsung%' then 'Samsung'
    WHEN concat(' ', name, ' ') LIKE '%dell%' then 'Dell'
AS brand
FROM laptops
```

💦 高效代码

```sql
SELECT
      regexp_extract(name,'(acer|samsung|dell)')
AS brand
FROM laptops
```

## IN子句转换为临时表

但我们进行数据选择时候，有时候会用到in作为条件选择，如果候选项非常多，利用临时表可能会带来更好的效率

💦 低效代码

```sql
SELECT *
FROM table1 as t1
WHERE
     itemid in (3363134, 5343, 5555555)
```

💦 高效代码

```sql
SELECT *
FROM table 1 as t1
JOIN (
      SELECT
           itemid
      FROM (
            SELECT
                 split('3363134, 5343, 5555555') as bar
           )
           CROSS JOIN
                   UNNEST(bar) AS t(itemid)
      ) AS table2 as t2
ON
  t1.itemid = t2.itemid
```

## 将 JOIN 的表从大到小排序

当我们要进行表关联（join）的时候，我们可以对表基于大小进行一个排序，把大表排在前面，小表排在后面，也会带来效率的提升。

💦 低效代码

```sql
SELECT *
FROM small_table
JOIN large_table
ON small_table.id = large_table.id
```

💦 高效代码

```sql
SELECT *
FROM large_table
JOIN small_table
ON small_table.id = large_table.id
```

## 使用简单的表关联条件

如果我们要基于条件对两个表进行连接，那条件中尽量不要出现复杂函数，如果一定需要使用，那我们可以先用函数对表的数据处理产出用于连接的字段。 如下例中，我们对a和b表进行连接，条件是b表的「年」「月」「日」拼接后和a表的日期一致，那粗糙的写法和优化的写法分别如下：

💦 低效代码

```sql
SELECT *
FROM table1 a
JOIN table2 b
ON a.date = CONCAT(b.year, '-', b.month, '-', b.day)
```

💦 高效代码

```sql
SELECT *
FROM table1 a
JOIN (
     SELECT name, CONCAT(b.year, '-', b.month, '-', b.day) as date
     FROM table2 b
) new
ON a.date = new.date
```

## 分组的字段按照类别取值种类数排序

如果我们需要对数据按照多个字段分组，尤其是字段中有id类这种取值非常多的类别字段，我们应当把它排在最前面，这也可以对效率有一些帮助。

💦 低效代码

```sql
SELECT 
  main_category,
  sub_category,
  itemid
  sum(price)
FROM
  table1
GROUP BY
  main_category, sub_category, itemid
```

💦 高效代码

```sql
SELECT 
  main_category,
  sub_category,
  itemid
  sum(price)
FROM
  table1
GROUP BY
  itemid, sub_category, main_category
```

## 避免 WHERE 子句中的子查询

当我们要查询的语句的where条件中包含子查询时，我们可以通过with语句构建临时表来调整连接条件，提升效率，如下：

💦 错误代码

```sql
SELECT sum(price)
FROM table1
WHERE itemid in (
         SELECT itemid
         FROM table2
)
```

💦 好代码

```sql
WITH t2
     AS (SELECT itemid
         FROM   table2)
SELECT Sum(price)
FROM   table1 AS t1
       JOIN t2
         ON t1.itemid = t2.itemid 
```

## 取最大直接用Max而非Rank后取第1

这一条很好理解，如果我们要取某字段最大取值，我们直接使用 max，而不要用 rank 排序后取第 1，如下代码所示：

💦 低效代码

```sql
SELECt *
FROM (
     SELECT userid, rank() over (order by prdate desc) as rank
     FROM table 1
)
WHERE ranking = 1
```

💦 高效代码

```sql
SELECT userid, max(prdate)
FROM table1
GROUP BY 1
```

## 其他优化点

- 对于大表，利用approx_distinct()代替count(distinct)来计数。
- 对于大表，利用approx_percentie(metric,0.5)代替median。
- 尽可能避免使用UNION。

# 批量插入效率最高

> 我们在操作大型数据表或者日志文件的时候经常会需要写入数据到数据库，那么最合适的方案就是数据库的批量插入。只是我们在执行批量操作的时候，一次插入多少数据才合适呢？

> 假如需要插入的数据有百万条，那么一次批量插入多少条的时候，效率会高一些呢？这里博主和大家一起探讨下这个问题，应用环境为批量插入数据到临时表。

## 批量插入前准备

> 博主本地原本是循环查出来的数据，然后每1000条插入一次，直至完成插入操作。但是为什么要设置1000条呢，实不相瞒，这是因为项目里的其他批量插入都是一次插1000条。。汗，博主不服，所以想要测试下。

首先是查看当前数据库的版本，毕竟各个版本之间存在差异，脱离版本讲数据库就是耍流氓（以前没少耍啊）：

```
mysql> select version();
+------------+
| version()  |
+------------+
| 5.6.34-log |
+------------+
1 row in set (0.00 sec)
```

### 1、插入到数据表的字段

对于手动创建的临时表来说，字段当然是越少越好，而且字段占用的空间要尽量小一些，这样临时表不至于太大，影响表操作的性能。这里需要插入的字段是：

```
字段1 int(10)
字段2 int(10)
字段3 int(10)
字段4 varchar(10)
```

我们一共插入四个字段，分别是3个int类型的，一个varchar类型的，整体来说这些字段都比较小，占用的内存空间会小一些。

### 2、计算一行字段占用的空间

对于innodb引擎来说，int类型可以存储4个字节，里面的`Int(M)`并不会影响存储字节的大小，这个M只是数据的展示位数，和mysql的ZEROFILL属性有关，即在数字长度不够的数据前面填充0，以达到设定的长度。此处不多说，想要了解的朋友可以百度一下，还是很有意思的。

`varchar（10）`代表可以存储10个字符，不管是英文还是中文，最多都是10个，这部分假设存储的是中文，在utf-8mb4下，10个中文占用`10*4 = 40`个字节那么一行数据最多占用：`4+4+4+40 = 52`字节

### 3、插入操作整体时间分配⭐

```
链接耗时 （30%）
发送query到服务器 （20%）
解析query （20%）
插入操作 （10% * 词条数目）
插入index （10% * Index的数目）
关闭链接 （10%）
```

从这里可以看出来，真正耗时的不是操作，而是链接，解析的过程。单条sql的话，会在链接，解析部分耗费大量的时间，因此速度会很慢，所以我们一般都是采用批量插入的操作，争取在一次链接里面写入尽可能多的数据，以此来提升插入的速度。但是这个尽可能多的数据是多少呢？一次到底插入多少才合适呢？

## 批量插入数据测试

开始测试，但是一开始插入多少是合适的呢，是否有上限？查询mysql手册，我们知道sql语句是有大小限制的。

### 1、SQL语句的大小限制

`my.ini` 里有 `max_allowed_packet` 这个参数控制通信的 packet 大小。mysql默认的sql语句的最大限制是1M（mysql5.7的客户端默认是16M，服务端默认是4M），可以根据设置查看。官方解释是适当增大 `max_allowed_packet` 参数可以使client端到server端传递大数据时，系统能够分配更多的扩展内存来处理。

> 官方手册：https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html

### 2、查看服务器上的参数

```
mysql> show variables like '%max_allowed_packet%';
+--------------------------+------------+
| Variable_name            | Value      |
+--------------------------+------------+
| max_allowed_packet       | 33554432   |
| slave_max_allowed_packet | 1073741824 |
+--------------------------+------------+
2 rows in set (0.00 sec)
```

`33554432字节 = 32M` ，也就是规定大小不能超过32M。

### 3、计算一次能插入的最大行记录

1M计算的话，`(1024*1024)/52 ≈ 20165` ，为了防止溢出，最大可一次性插入20000条（根据自己的配置和sql语句大小计算）。那么32M的话就是：`20000 *32 = 640000` 也就是64W条。

### 4、测试插入数据比对

插入11W条数据，按照每次`10,600,1000,20000,80000`来测试：

```
+---------------+
| count(c1.uin) |
+---------------+
|         110000 |
+---------------+
```

有个博客说一次插入10条最快，，我觉得一次插的有点少，咱们试试

> 参考：https://www.cnblogs.com/aicro/p/3851434.html

这个博主测试后，认为一次插10条是性能最快的，他的每条记录是3kb，相当于我的59行数据，取个整数60，那么对于这个博主是插入10条，对我来说插入：600，这几个值都试试。

耗时：

```
11W的数据，每次插入10条。耗时：2.361s
11W的数据，每次插入600条。耗时：0.523s
11W的数据，每次插入1000条。耗时：0.429s
11W的数据，每次插入20000条。耗时：0.426s
11W的数据，每次插入80000条。耗时：0.352s
```

从这部分看，随着批量插入的增加，速度略有提升，最起码一次插10条应该不是最佳的。插入数据量多，减少了循环的次数，也就是在数据库链接部分的耗时有所减少，只是这个8W并不是极限数据，具体一次插入多少条，还有待参考。

加大数据量到24w

```
+---------------+
| count(c1.uin) |
+---------------+
|        241397 |
+---------------+
```

耗时：

```
24W的数据，每次插入10条。耗时：4.445s
24W的数据，每次插入600条。耗时：1.187s
24W的数据，每次插入1000条。耗时：1.13s
24W的数据，每次插入20000条。耗时：0.933s
24W的数据，每次插入80000条。耗时：0.753s
```

一次插入24W反而性能最佳，这么代表我们的测试数据量依然不够。

加大测试量到42W

```
+---------------+
| count(c1.uin) |
+---------------+
|        418859 |
```

耗时：

```
42W的数据，每次插入1000条。耗时：2.216s
42W的数据，每次插入80000条。耗时：1.777s
42W的数据，每次插入16W条。耗时：1.523s
42W的数据，每次插入20W条。耗时：1.432s
42W的数据，每次插入30W条。耗时：1.362s
42W的数据，每次插入40W条。耗时：1.764s
```

随着插入量的增加，批量插入条数多了之后，性能是有所提升的。但是在达到30W以上之后，效率反而有所下降。这部分我的理解是mysql是要分配一定的内存给传过来的数据包使用，当批量插入的数据量到达一定程度之后，一次插入操作的开销就很耗费内存了。

> 个人感觉，最佳大小是`max_allowed_packet`的一半，也就是极限能插入64W，选用32W也许性能会更好一些，同时也不会对mysql的其他操作产生太大的影响。

### 5、达到限制的最大，性能真的好吗？

博主疯狂谷歌百度，都没有找到有人来具体的说一下这个问题，不过在高性能mysql里面发现一句话：

> 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置`max_allowed_packet`参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用`SELECT *`以及加上`LIMIT`限制的原因之一。

后面通过各种百度，博主觉得最大只是代表传输数据包的最大长度，但性能是不是最佳就要从各个方面来分析了。比如下面列出的插入缓冲，以及插入索引时对于缓冲区的剩余空间需求，以及事务占有的内存等，都会影响批量插入的性能。

## 其他影响插入性能的因素

### 1、意缓冲区的大小使用情况

在分析源码的过程中，有一句话：如果`buffer pool`余量不足25%，插入失败，返回`DB_LOCK_TABLE_FULL`。这个错误并不是直接报错：`max_allowed_packet` 不够大之类的，这个错误是因为对于innodb引擎来说，一次插入是涉及到事务和锁的，在插入索引的时候，要判断缓冲区的剩余情况，所以插入并不能仅仅只考虑`max_allowed_packet`的问题，也要考虑到缓冲区的大小。

> 参考淘宝的数据库日报：http://mysql.taobao.org/monthly/2017/09/10/

### 2、插入缓存

另外对于innodb引擎来说，因为存在插入缓存（Insert Buffer）这个概念，所以在插入的时候也是要耗费一定的缓冲池内存的。当写密集的情况下，插入缓冲会占用过多的缓冲池内存，默认最大可以占用到1/2的缓冲池内存，当插入缓冲占用太多缓冲池内存的情况下，会影响到其他的操作。

也就是说，插入缓冲受到缓冲池大小的影响，缓冲池大小为：

```
mysql> show variables like 'innodb_buffer_pool_size';
+-------------------------+-----------+
| Variable_name           | Value     |
+-------------------------+-----------+
| innodb_buffer_pool_size | 134217728 |
+-------------------------+-----------+
```

换算后的结果为：128M，也就是说，插入缓存最多可以占用64M的缓冲区大小。这个大小要超过咱们设置的sql语句大小，所以可以忽略不计。

详细解释：

> 我们都知道，在InnoDB引擎上进行插入操作时，一般需要按照主键顺序进行插入，这样才能获得较高的插入性能。当一张表中存在非聚簇的且不唯一的索引时，在插入时，数据页的存放还是按照主键进行顺序存放，但是对于非聚簇索引叶节点的插入不再是顺序的了，这时就需要离散的访问非聚簇索引页，由于随机读取的存在导致插入操作性能下降。

InnoDB为此设计了Insert Buffer来进行插入优化。对于非聚簇索引的插入或者更新操作，不是每一次都直接插入到索引页中，而是先判断插入的非聚集索引是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个`Insert Buffer`中。

看似数据库这个非聚集的索引已经查到叶节点，而实际没有，这时存放在另外一个位置。然后再以一定的频率和情况进行`Insert Buffer`和非聚簇索引页子节点的合并操作。这时通常能够将多个插入合并到一个操作中，这样就大大提高了对于非聚簇索引的插入性能。

> 参考：https://cloud.tencent.com/developer/article/1200824
>
> 参考：mysql技术内幕 Innodb篇

### 3、使用事务提升效率

还有一种说法，使用事务可以提高数据的插入效率，这是因为进行一个`INSERT`操作时，MySQL内部会建立一个事务，在事务内才进行真正插入处理操作。通过使用事务可以减少创建事务的消耗，所有插入都在执行后才进行提交操作。大概如下：

```
START TRANSACTION;
INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) 
    VALUES ('0', 'userid_0', 'content_0', 0);
INSERT INTO `insert_table` (`datetime`, `uid`, `content`, `type`) 
    VALUES ('1', 'userid_1', 'content_1', 1);
...
COMMIT;
```

> 参考：https://my.oschina.net/songhongxu/blog/163063

事务需要控制大小，事务太大可能会影响执行的效率。MySQL有`innodb_log_buffer_size`配置项，超过这个值会把innodb的数据刷到磁盘中，这时，效率会有所下降。所以比较好的做法是，在数据达到这个这个值前进行事务提交。

查看：`show variables like '%innodb_log_buffer_size%';`

```
+------------------------+----------+
        | Variable_name          | Value    |
        +------------------------+----------+
        | innodb_log_buffer_size | 67108864 |
        +------------------------+----------+
```

大概是：64M

这种写法和批量写入的效果差不多，只不过sql语句还是单句的，然后统一提交。一个瓶颈是SQL语句的大小，一个瓶颈是事务的大小。当我们在提交sql的时候，首先是受到sql大小的限制，其次是受到事务大小的限制。在开启事务的情况下使用批量插入，会节省不少事务的开销，如果要追求极致的速度的话，建议是开着事务插入的。

> 不过需要注意一下，内存是有限且共享的，如果批量插入占用太多的事务内存，那么势必会对其他的业务操作等有一定的影响。

### 4、通过配置提升读写性能

也可以通过增大`innodb_buffer_pool_size` 缓冲区来提升读写性能，只是缓冲区是要占用内存空间的，内存很珍贵，所以这个方案在内存富裕，而性能瓶颈的时候，可以考虑下。

> 参考：https://my.oschina.net/anuodog/blog/3002941

### 5、索引影响插入性能

如果表中存在多个字段索引，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护。这样就降低了数据的插入速度。对于普通的数据表，主键索引是肯定要有的，想要加快性能的话，就是要有序插入，每次插入记录都在索引的最后面，索引的定位效率很高，并且对索引调整较小。如果插入的记录在索引中间，需要B+tree进行分裂合并等处理，会消耗比较多计算资源，并且插入记录的索引定位效率会下降，数据量较大时会有频繁的磁盘操作。

## 总结

博主经过测试+谷歌，最终是选用的一次批量插入数据量为`max_allowed_packet`大小的一半。只是在不断的搜索中，发现影响插入性能的地方挺多的，如果仅仅是拿`max_allowed_packet`这个参数作为分析，其实是没有意义的，这个参数只是设置最大值，但并不是最佳性能。

> 不过需要注意，由于sql语句比较大，所以才执行完插入操作之后，一定要释放变量，不要造成无谓的内存损耗，影响程序性能。

> 对于我们的mysql来说也是一样的，mysql的最佳性能是建立在各个参数的合理设置上，这样协同干活儿的效果最佳。如果其他设置不到位的话，就像是木桶原理一样，哪怕内存缓冲区设置的很大，但是性能取决的反而是设置最差的那个配置。关于mysql的配置调优，我们都在路上，加油！



# 系统上线前SQL脚本的9大坑

> 系统上线时，非常容易出问题。即使之前在测试环境，已经执行过SQL脚本了。但是有时候，在系统上线时，在生产环境执行相同的SQL脚本，还是有可能出现一些问题。

> 有些小公司，SQL脚本是开发自己执行的，有很大的风险。有些大厂，有专业的DBA把关，但DBA也不是万能的，还是有可能会让一些错误的SQL脚本被生产环境执行了，比如：update语句的顺序不对。

今天跟大家一起聊聊，系统上线时SQL脚本的9大坑，以便于大家吸取教训，能够防微杜渐，希望对你会有所帮助。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202302211626830.png" alt="image-20230221162609748" style="zoom:67%;" />

## 1 漏脚本了

我们上线时执行的SQL脚本，出现次数最多的问题，应该是`漏脚本`了。

- 有时候少加了一个字段。
- 有时候字段的注释没有及时修改。
- 有时候有些新表没创建。
- 有时候字段类型忘了修改。

等等。

我们的SQL脚本中漏脚本的情况有很多。

那么，如何解决这个问题呢？

答：将SQL脚本做成代码的一部分。在项目的代码中，创建一个专门的`sql目录`，在该目录下根据每个迭代创建一个子目录，比如：mv3.2.1，将SQL脚本存放到mv3.2.1下。

我们在开发环境`任何`对表的相关操作，比如：增加字段、修改字段类型、修改注释、增加索引、创建表等等，都需要通过SQL语句操作，然后把该SQL语句，整理到SQL脚本中。

最后提交到公司的`GitLab`上，我们在测试环境和生产环境发版时，去GitLab上找相关迭代版本的SQL脚本执行。

通过该方式基本可以解决`漏脚本`的问题。

## 2 脚本语法错误

有些小伙伴看到这个标题可能有点懵，SQL脚本不是已经在测试环境执行过了吗？为什么还会出现语法错误？

比如说有这样的场景：原本你的SQL脚本没问题的，但没有按照规范，给一张表的添加多个字段，你写了多条`ALTER`语句。

例如：

```sql
alter table t_user add column  `work` varchar(30) DEFAULT NULL COMMENT '工作';
alter table t_user add column  `provice` varchar(10) DEFAULT NULL  COMMENT '籍贯';
```

在上线时，你给DBA提SQL工单时，该工单被DBA审核拒绝打回来了。

然后为了赶时间，你急急忙忙把多条`ALTER`语句改成一条`ALTER`语句。

例如：

```sql
alter table t_user add `work` varchar(30) DEFAULT NULL COMMENT '工作',
add `provice` varchar(10) DEFAULT NULL  COMMENT '籍贯';
```

但在修改的过程中，有地方少了一个`逗号`，就可能会出现SQL语法错误。

因此，不管是什么SQL语句，要养成好习惯，只要修改了一定要记得到开发环境的数据库中，先执行测试一下，切勿直接提到生产环境，即使你有很大的把握，也需要再更慎重一些。

这样基本可以避免SQL语法错误的问题。

## 3 脚本顺序不对

有些时候，我们在上线系统时，DBA在执行SQL脚本的时候，没有报错，但最后的数据就是不对。

有可能是`脚本顺序不对`导致的。

比如有这样一种场景：你往某张表通过insert初始化了一条数据。

例如：

```sql
INSERT INTO `sue`.`t_user`(`id`, `code`, `age`, `name`, `height`, `address`, `work`, `provice`) VALUES (1, '101', 21, '周星驰', 173, '香港', NULL, NULL);
```

另外一个人要基于你这条数据，通过update修改数据。

例如：

```sql
update t_user set age=25 where id=1;
```

你们提了两条SQL脚本。

另外一个人先提的，你后提的。

DBA先把他的SQL工单审核通过了，先update数据，此时通过id是没法找到那条数据的，影响行数为0。

然后DBA再审核你的SQL工单，审核通过了，插入了一条数据。

由于SQL脚本的顺序不对，导致最终系统上线时的数据不对。

那么这个问题要如何解决呢？

双方要事先沟通好，把另外一个同事的SQL脚本加到你的初始化脚本中，你的脚本在初始化时，直接去修改数据即可。

例如：

```sql
INSERT INTO `sue`.`t_user`(`id`, `code`, `age`, `name`, `height`, `address`, `work`, `provice`) VALUES (1, '101', 25, '周星驰', 173, '香港', NULL, NULL);
```

这样可以避免执行顺序问题。

## 4 执行时机不对

有些系统功能已经上线了，在后面的迭代中，为了尽量避免少影响线上功能，可以增加一个`pre`（即预生产环境）。

该环境跟生产环境是差不多的，连接了相同的数据库，使用了相同的apollo配置。

但唯一的区别是pre环境没有实际的用户流量，只能公司内部人员才能访问。

一般在迭代版本上线之前，先要把系统功能发布到pre环境中，测试通过之后，才能发布到`prod`（即生产环境）。

但有些SQL脚本，却没法再pre环境中执行，不然会影响生产环境。

比如：修改了字段类型，int改成varchar了，或者初始化数据时，初始化了一条新加的枚举数据。

由于pre环境是运行的最新代码，但prod环境还是运行的老代码。

如果在发布pre环境时，直接执行SQL脚本，可能会导致prod环境的功能异常。

因此要搞清楚SQL脚本的执行时机，哪些是要在pre环境执行的，哪些是要在prod环境执行的。

我们在提SQL工单时，千万不要一股脑就提了，一定要区分时机。

在发pre环境时，要么不提发prod环境的SQL脚本。要么，在工单的名称上做区分，比如增加`prod_`开头的标识。

这样可以解决`SQL脚本执行时机`的问题。

## 5 搞错数据库了

有时候，我们的数据库做了`分库分表`，或者增加`备份库`。

在执行SQL脚本的时候，由于我们自己的疏忽，提SQL工单时选错数据库了，或者DBA的疏忽，在执行SQL工单时搞错数据库了，就会出现问题。

建议我们的SQL脚本增加库名，比如：

```sql
alter table sue.t_user add `work` varchar(30) DEFAULT NULL COMMENT '工作';
```

这里增加库名：sue。

这样基本可以避免`选错数据库`的问题。

## 6 脚本耗时太长

有时候，我们的SQL脚本需要批量修改生产环境的一些数据，正常情况下一条update语句就能搞定。

例如：

```sql
update user set status=0 where status=1;
```

但由于user表的数据量非常大，我们在执行该SQL脚本之前，没有预先评估该SQL脚本的耗时情况，而选择直接在生产环境的数据库中执行。

假如该SQL脚本耗时非常长，比如要10分钟才能执行完，可能会导致user表长期锁表，影响正常的业务功能。

在该SQL脚本执行的过程中，极有可能会出现业务功能操作，导致的死锁问题。

因此，建议这种大批量的数据更新操作，要在用户较少的凌晨，分批多次执行。

我们要尽可能少的影响线上用户的功能。

此外，在生产环境增加字段，增加索引等操作，也能会导致长期锁表。也要避免在用户访问高峰期执行相关的SQL脚本。

## 7 脚本无法回滚

绝大多数系统上线是能够成功的，虽然过程中会遇到很多问题，但如果能够及时解决，也能够`上线成功`。

但如果有些问题，没法再规定的时间内解决，很有可能会导致`上线失败`。

如果上线失败，意味着代码和数据库的SQL脚本要回滚。

如果只回滚了代码，不回滚数据库，可能会导致很多系统异常。

因此，我们在准备SQL语句时，要留点心眼，顺便想想该SQL语句能否回滚。

对于update语句可以加上修改时间：

```sql
update t_user set age=25,time=now(3) where id=1;
```

这样可以通过该时间追溯一次SQL操作修改的数据，方便后面做回滚。

有些时候我们要update的数据，是要通过多条sql语句查询出来的，比如：需要使用的id。

为了方便回滚我们可以增加`临时表`，保存这些id，后面就能追溯了。

当然有些开源的数据库管理平台，比如：Archery，是有自带SQL审核和回滚的功能。

## 8 忘了加索引

我们在增加了字段之后，非常容易忽略的一件事是：`加索引`。

特别是当前表数据量很大，而且增加的字段是另外一张表的id时，这种情况强烈建议增加索引。

如果我们上线系统时，在SQL脚本中，忘了给该字段增加索引。如果该id字段被大批量访问，全部走的全表扫描，可能会导致数据库性能直线下降，出现大量的`超时问题`。

所以建议我们在开发的时候，如果要增加字段的话，要养成良好习惯，想一想这个字段需不需要建`索引`。

如果不确定数据量的话，可以先到生产环境查询一下真实的用户数据，不然后续可能会引起比较大的`生产事故`。

## 9 字段改名

对于生产环境的表字段，通常情况下，我们不允许修改名称。

如果你在发布pre环境时，通过SQL脚本把某张表的某个字段名称修改了，pre环境代码使用了新的名称，系统没有问题。

但prod环境还是使用老的名称，所有使用该名称的sql语句，在代码执行过程中都会报错。

因此，禁止在生产环境通过SQL脚本`修改字段名称`。

当然系统上线时除了SQL脚本的这些坑之外，还有系统发版失败，代码合错分支，mq消息被pre消费了，无法回滚等等，还有很多问题。













































































