



# 文章审核及发布

## 审核流程

### 审核方式

> 自动审核：文章发布之后，系统自动审核，主要是通过第三方接口对文章内容进行审核

> 返回结果：成功、失败、不确定

> 人工审核：待自动审核返回不确定信息时，转到人工审核，由平台管理员进行审核。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131608060.png" alt="image-20230613160837957" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131609603.png" alt="image-20230613160909508" style="zoom:80%;" />



### 自动审核流程

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555512.png" alt="image-20210504211635199" style="zoom:80%;" />

> 1. **自媒体端发布文章后，开始审核文章**
> 2. **审核的主要是审核文章的内容（文本内容和图片）**
> 3. **借助第三方提供的接口审核文本**
> 4. **借助第三方提供的接口审核图片，由于图片存储到minIO中，需要先下载才能审核**
> 5. **如果审核失败，则需要修改自媒体文章的状态，status:2  审核失败    status:3  转到人工审核**
> 6. **如果审核成功，则需要在文章微服务中创建app端需要的文章**

## 阿里云接口-失效


### 准备工作-账号密码

> 您在使用内容检测API之前，需要先注册阿里云账号，添加Access Key并签约云盾内容安全。
>

> 前往[阿里云官网](https://www.aliyun.com/)注册账号。如果已有注册账号，请跳过此步骤。
>

> 进入阿里云首页后，如果没有阿里云的账户需要先进行注册，才可以进行登录。由于注册较为简单，课程和讲义不在进行体现（注册可以使用多种方式，如淘宝账号、支付宝账号、微博账号等...）。需要实名认证和活体认证。打开[云盾内容安全产品试用页面](https://promotion.aliyun.com/ntms/act/lvwangdemo.html)，单击**立即开通**，正式开通服务。
>

![image-20210504213452171](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555517.png)

> 内容安全控制台
>

![image-20210504213510896](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555515.png)

> 在[AccessKey管理页面](https://ak-console.aliyun.com/#/accesskey)管理您的AccessKeyID和AccessKeySecret。
>

![image-20210504213530641](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555526.png)

> 管理自己的AccessKey,可以新建和删除AccessKey
>

![image-20210504213547219](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555528.png)

> 查看自己的AccessKey，AccessKey默认是隐藏的，第一次申请的时候可以保存AccessKey，点击显示，通过验证手机号后也可以查看
>

![image-20210504213605004](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555535.png)

### 文本内容审核接口

文本垃圾内容检测：https://help.aliyun.com/document_detail/70439.html?spm=a2c4g.11186623.6.659.35ac3db3l0wV5k 

![image-20210504213640667](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555117.png)

文本垃圾内容Java SDK: https://help.aliyun.com/document_detail/53427.html?spm=a2c4g.11186623.6.717.466d7544QbU8Lr 

### 图片审核接口

> 图片垃圾内容检测：https://help.aliyun.com/document_detail/70292.html?spm=a2c4g.11186623.6.616.5d7d1e7f9vDRz4 
>

![image-20210504213719323](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555139.png)

> 图片垃圾内容Java SDK: https://help.aliyun.com/document_detail/53424.html?spm=a2c4g.11186623.6.715.c8f69b12ey35j4 
>

### 项目集成

> ①：拷贝资料文件夹中的类到common模块下面，并添加到自动配置，包括了GreenImageScan和GreenTextScan及对应的工具类
>

![image-20210504213812303](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555160.png)

> 添加到自动配置中
>

![image-20210504214013276](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555443.png)

> ②： accessKeyId和secret（需自己申请），在heima-leadnews-wemedia中的nacos配置中心添加配置：
>

```yaml
aliyun:
 accessKeyId: LTAI5tCWHCcfvqQzu8k2oKmX
 secret: auoKUFsghimbfVQHpy7gtRyBkoR4vc
#aliyun.scenes=porn,terrorism,ad,qrcode,live,logo
 scenes: terrorism
```

> ③：在自媒体微服务中测试类中注入审核文本和图片的bean进行测试
>

```java
@SpringBootTest
public class AliyunTest {

    @Autowired
    private GreenTextScan greenTextScan;

    @Autowired
    private GreenImageScan greenImageScan;

    @Autowired
    private FileStorageService fileStorageService;

    @Test
    public void testScanText() throws Exception {
        Map map = greenTextScan.greeTextScan("我是一个好人,冰毒");
        System.out.println(map);
    }

    @Test
    public void testScanImage() throws Exception {
        byte[] bytes = fileStorageService.downLoadFile("http://192/3e55.jpg");
        Map map = greenImageScan.imageScan(Arrays.asList(bytes));
        System.out.println(map);
    }
}
```

## 文章保存

### 表结构

> ap_article 文章信息表
>

![image-20210505005300287](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555463.png)

> ap_article_config  文章配置表
>

![image-20210505005353286](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555487.png)

> ap_article_content 文章内容表
>

![image-20210505005407987](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555783.png)

### 分布式ID

> 随着业务的增长，文章表可能要占用很大的物理存储空间，为了解决该问题，后期使用数据库分片技术。将一个数据库进行拆分，通过数据库中间件连接。如果数据库中该表选用ID自增策略，则可能产生重复的ID，此时应该使用分布式ID生成策略来生成ID。
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131612229.png" alt="image-20230613161202142" style="zoom:80%;" />

#### ID生成方案

> 分布式ID生成方案

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131612731.png" alt="image-20230613161228647" style="zoom:80%;" />

#### snowflake算法

> **snowflake是Twitter开源的分布式ID生成算法，结果是一个long型的ID。其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0**

![image-20210505005509258](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555812.png)

> 文章端相关的表都使用雪花算法生成id,包括ap_article、 ap_article_config、 ap_article_content

> mybatis-plus已经集成了雪花算法，完成以下两步即可在项目中集成雪花算法

> 第一：在实体类中的id上加入如下配置，指定类型为ASSIGN_ID
>

```java
@TableId(value = "id",type = IdType.ASSIGN_ID)
private Long id;
```

### 保存功能

> 在文章审核成功以后需要在app的article库中新增文章数据

> 1. 保存文章信息 ap_article2.保存文章配置信息 
> 2. ap_article_config
> 3. 保存文章内容 ap_article_content

#### 实现思路

![image-20210505005733405](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555086.png)

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131612894.png" alt="image-20230613161259808" style="zoom:80%;" />





#### ArticleDto

```java
@Data
public class ArticleDto  extends ApArticle {
    // 文章内容
    private String content;
}
```

#### feign接口

> 在heima-leadnews-feign-api中新增接口

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630091329813.png" alt="image-20230630091329813" style="zoom: 80%;" />

> 第一：导入feign的依赖
>

```xml
<dependencies>
    <dependency>
        <groupId>com.heima</groupId>
        <artifactId>heima-leadnews-model</artifactId>
    </dependency>
    <dependency>
        <groupId>com.heima</groupId>
        <artifactId>heima-leadnews-common</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-openfeign</artifactId>
    </dependency>
</dependencies>
```

> 第二：定义文章端的接口
>

```java
package com.heima.apis.article;

@FeignClient(value = "leadnews-article")
public interface IArticleClient {

    @PostMapping("/api/v1/article/save")
    public ResponseResult saveArticle(@RequestBody ArticleDto dto) ;
}
```

> 在heima-leadnews-article中实现该方法
>

```java
package com.heima.article.feign;


@RestController
public class ArticleClient implements IArticleClient {

    @Autowired
    private ApArticleService apArticleService;

    @Override
    @PostMapping("/api/v1/article/save")
    public ResponseResult saveArticle(@RequestBody ArticleDto dto) {
        return apArticleService.saveArticle(dto);
    }
}
```

#### mapper

```java
public interface ApArticleConfigMapper extends BaseMapper<ApArticleConfig> {
}
```

#### ApArticleConfig

> 同时，修改ApArticleConfig类，添加如下构造函数

```java
package com.heima.model.article.pojos;


// APP已发布文章配置表
@Data
@NoArgsConstructor
@TableName("ap_article_config")
public class ApArticleConfig implements Serializable {
	
	// 添加默认值
    public ApArticleConfig(Long articleId){
        this.articleId = articleId;
        this.isComment = true;
        this.isForward = true;
        this.isDelete = false;
        this.isDown = false;
    }

    @TableId(value = "id",type = IdType.ASSIGN_ID)
    private Long id;

    // 文章id
    @TableField("article_id")
    private Long articleId;

    /**
     * 是否可评论
     * true: 可以评论   1
     * false: 不可评论  0
     */
    @TableField("is_comment")
    private Boolean isComment;

    /**
     * 是否转发
     * true: 可以转发   1
     * false: 不可转发  0
     */
    @TableField("is_forward")
    private Boolean isForward;

    /**
     * 是否下架
     * true: 下架   1
     * false: 没有下架  0
     */
    @TableField("is_down")
    private Boolean isDown;

    /**
     * 是否已删除
     * true: 删除   1
     * false: 没有删除  0
     */
    @TableField("is_delete")
    private Boolean isDelete;
}
```

#### ApArticleService

> 在ApArticleService中新增方法

```java
/**
     * 保存app端相关文章
     * @param dto
     * @return
     */
ResponseResult saveArticle(ArticleDto dto) ;
```

> 实现类：
>

```java
@Resource
private ApArticleConfigMapper apArticleConfigMapper;
@Resource
private ApArticleContentMapper apArticleContentMapper;

@Override
public ResponseResult saveArticle(ArticleDto dto) {
    //1.检查参数
    if(dto == null){
        return ResponseResult.errorResult(AppHttpCodeEnum.PARAM_INVALID);
    }

    ApArticle apArticle = new ApArticle();
    BeanUtils.copyProperties(dto,apArticle);

    //2.判断是否存在id
    if(dto.getId() == null){
        //2.1 不存在id  保存  文章  文章配置  文章内容
        //保存文章
        save(apArticle);
        //保存配置
        ApArticleConfig apArticleConfig = new ApArticleConfig(apArticle.getId());
        apArticleConfigMapper.insert(apArticleConfig);

        //保存 文章内容
        ApArticleContent apArticleContent = new ApArticleContent();
        apArticleContent.setArticleId(apArticle.getId());
        apArticleContent.setContent(dto.getContent());
        apArticleContentMapper.insert(apArticleContent);

    }else {
        //2.2 存在id   修改  文章  文章内容

        //修改  文章
        updateById(apArticle);

        //修改文章内容
        ApArticleContent apArticleContent = apArticleContentMapper
            .selectOne(Wrappers.<ApArticleContent>lambdaQuery()
            .eq(ApArticleContent::getArticleId, dto.getId()));
        apArticleContent.setContent(dto.getContent());
        apArticleContentMapper.updateById(apArticleContent);
    }

    //3.结果返回  文章的id
    return ResponseResult.okResult(apArticle.getId());
}
```

#### 功能测试

> 启动Article服务

> 新增操作

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630092808856.png" alt="image-20230630092808856" style="zoom:80%;" />

```json
{
    "title":"黑马头条项目背景3211",
    "authoId":1102,
    "layout":1,
    "labels":"黑马头条",
    "publishTime":"2028-03-14T11:35:49.000Z",
    "images": "http://192.168.393b08a47860da275.jpg",
    "content":"1111条项目背景,黑马头条项目背景,黑马头条项目背景，黑马头条项目背景"
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630092820875.png" alt="image-20230630092820875" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630092959641.png" alt="image-20230630092959641" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630093013356.png" alt="image-20230630093013356" style="zoom:80%;" />

> 修改操作

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630093208673.png" alt="image-20230630093208673" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630093155860.png" alt="image-20230630093155860" style="zoom:80%;" />



## 文章审核

### 表结构说明

> wm_news 自媒体文章表
>

![image-20210505010728156](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555101.png)

> status字段：0 草稿  1 待审核  2 审核失败  3 人工审核  4 人工审核通过  8 审核通过（待发布） 9 已发布
>

### 实现流程⭐

![image-20210505010755954](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555118.png)

### WmNewsAutoScanService

> 在heima-leadnews-wemedia中的service新增接口

```java
package com.heima.wemedia.service;

public interface WmNewsAutoScanService {
    // 自媒体文章审核 自媒体文章id
    public void autoScanWmNews(Integer id);
}
```

> 实现类
>

```java
@Service
@Slf4j
public class WmNewsAutoScanServiceImpl implements WmNewsAutoScanService {
    // 内容分解如下
}
```

> 这里是整合了阿里云接口的

```java
@Resource
private WmNewsMapper wmNewsMapper;
// 自媒体文章审核 自媒体文章id
@Override
public void autoScanWmNews(Integer id) {
    //1.查询自媒体文章
    WmNews wmNews = wmNewsMapper.selectById(id);
    if(wmNews == null){
        throw new RuntimeException("WmNewsAutoScanServiceImpl-文章不存在");
    }
    if(wmNews.getStatus().equals(WmNews.Status.SUBMIT.getCode())){
        //从内容中提取纯文本内容和图片
        Map<String,Object> textAndImages = handleTextAndImages(wmNews);
        //2.审核文本内容  阿里云接口
        boolean isTextScan = handleTextScan((String) textAndImages
                                            .get("content"),wmNews);
        if(!isTextScan)return;

        //3.审核图片  阿里云接口
        boolean isImageScan =  handleImageScan((List<String>) textAndImages
                                               .get("images"),wmNews);
        if(!isImageScan)return;

        //4.审核成功，保存app端的相关的文章数据
        ResponseResult responseResult = saveAppArticle(wmNews);
        if(!responseResult.getCode().equals(200)){
            throw new RuntimeException("文章审核，保存app端相关文章数据失败");
        }
        //回填article_id
        wmNews.setArticleId((Long) responseResult.getData());
        updateWmNews(wmNews,(short) 9,"审核成功");
    }
}
```

> 但是我们目前不能用，因此就不使用

```java
@Override
public void autoScanWmNews(Integer id) {
    //1.查询自媒体文章
    WmNews wmNews = wmNewsMapper.selectById(id);
    if(wmNews == null){
        throw new RuntimeException("WmNewsAutoScanServiceImpl-文章不存在");
    }
    if(wmNews.getStatus().equals(WmNews.Status.SUBMIT.getCode())){
        //从内容中提取纯文本内容和图片
        Map<String,Object> textAndImages = handleTextAndImages(wmNews);
        //2.审核文本内容  阿里云接口
        boolean isTextScan = true;
        if(!isTextScan)return;

        //3.审核图片  阿里云接口
        boolean isImageScan =  true;
        if(!isImageScan)return;

        //4.审核成功，保存app端的相关的文章数据
        ResponseResult responseResult = saveAppArticle(wmNews);
        if(!responseResult.getCode().equals(200)){
            throw new RuntimeException("文章审核，保存app端相关文章数据失败");
        }
        //回填article_id
        wmNews.setArticleId((Long) responseResult.getData());
        updateWmNews(wmNews,(short) 9,"审核成功");
    }
}
```

> 下面调用的方法正常，注释掉相关方法即可

```java
// 1. 从自媒体文章的内容中提取文本和图片 2.提取文章的封面图片
private Map<String, Object> handleTextAndImages(WmNews wmNews) {
    //存储纯文本内容
    StringBuilder stringBuilder = new StringBuilder();
    List<String> images = new ArrayList<>();
    //1。从自媒体文章的内容中提取文本和图片
    if(StringUtils.isNotBlank(wmNews.getContent())){
        List<Map> maps = JSONArray.parseArray(wmNews.getContent(), Map.class);
        for (Map map : maps) {
            if (map.get("type").equals("text")){
                stringBuilder.append(map.get("value"));
            }
            if (map.get("type").equals("image")){
                images.add((String) map.get("value"));
            }
        }
    }
    //2.提取文章的封面图片
    if(StringUtils.isNotBlank(wmNews.getImages())){
        String[] split = wmNews.getImages().split(",");
        images.addAll(Arrays.asList(split));
    }
    Map<String, Object> resultMap = new HashMap<>();
    resultMap.put("content",stringBuilder.toString());
    resultMap.put("images",images);
    return resultMap;
}
```

```java
// 审核纯文本内容
private boolean handleTextScan(String content, WmNews wmNews) {
    boolean flag = true;
    if((wmNews.getTitle()+"-"+content).length() == 0){
        return flag;
    }
    try {
        Map map = greenTextScan.greeTextScan((wmNews.getTitle()+"-"+content));
        if(map != null){
            //审核失败
            if(map.get("suggestion").equals("block")){
                flag = false;
                updateWmNews(wmNews, (short) 2, "当前文章中存在违规内容");
            }
            //不确定信息  需要人工审核
            if(map.get("suggestion").equals("review")){
                flag = false;
                updateWmNews(wmNews, (short) 3, "当前文章中存在不确定内容");
            }
        }
    } catch (Exception e) {
        flag = false;
        e.printStackTrace();
    }
    return flag;
}
```

```java
// 审核图片
private boolean handleImageScan(List<String> images, WmNews wmNews) {

    boolean flag = true;
    if(images == null || images.size() == 0){
        return flag;
    }
    //下载图片 minIO
    //图片去重
    images = images.stream().distinct().collect(Collectors.toList());
    List<byte[]> imageList = new ArrayList<>();
    for (String image : images) {
        byte[] bytes = fileStorageService.downLoadFile(image);
        imageList.add(bytes);
    }
    //审核图片
    try {
        Map map = greenImageScan.imageScan(imageList);
        if(map != null){
            //审核失败
            if(map.get("suggestion").equals("block")){
                flag = false;
                updateWmNews(wmNews, (short) 2, "当前文章中存在违规内容");
            }
            //不确定信息  需要人工审核
            if(map.get("suggestion").equals("review")){
                flag = false;
                updateWmNews(wmNews, (short) 3, "当前文章中存在不确定内容");
            }
        }
    } catch (Exception e) {
        flag = false;
        e.printStackTrace();
    }
    return flag;
}
```

```java
// 保存app端相关的文章数据
private ResponseResult saveAppArticle(WmNews wmNews) {

    ArticleDto dto = new ArticleDto();
    //属性的拷贝
    BeanUtils.copyProperties(wmNews,dto);
    //文章的布局
    dto.setLayout(wmNews.getType());
    //频道
    WmChannel wmChannel = wmChannelMapper.selectById(wmNews.getChannelId());
    if(wmChannel != null){
        dto.setChannelName(wmChannel.getName());
    }

    //作者
    dto.setAuthorId(wmNews.getUserId().longValue());
    WmUser wmUser = wmUserMapper.selectById(wmNews.getUserId());
    if(wmUser != null){
        dto.setAuthorName(wmUser.getName());
    }

    //设置文章id
    if(wmNews.getArticleId() != null){
        dto.setId(wmNews.getArticleId());
    }
    dto.setCreatedTime(new Date());

    ResponseResult responseResult = articleClient.saveArticle(dto);
    return responseResult;

}
```

```java
// 修改文章内容
private void updateWmNews(WmNews wmNews, short status, String reason) {
    wmNews.setStatus(status);
    wmNews.setReason(reason);
    wmNewsMapper.updateById(wmNews);
}
```

### 远程接口调用

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131613600.png" alt="image-20230613161330519" style="zoom:80%;" />

> 在heima-leadnews-wemedia服务中已经依赖了heima-leadnews-feign-apis工程，只需要在自媒体的引导类中开启feign的远程调用即可
>

> 注解为：`@EnableFeignClients(basePackages = "com.heima.apis")` 需要指向apis这个包
>

![image-20210507160209926](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555433.png)

### 服务降级处理

![image-20210507160329016](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555448.png)

> 服务降级是服务自我保护的一种方式，或者保护下游服务的一种方式，用于确保服务不会受请求突增影响变得不可用，确保服务不会崩溃,服务降级虽然会导致请求失败，但是不会导致阻塞。

> 实现步骤：在heima-leadnews-feign-api编写降级逻辑

```java
package com.heima.apis.article.fallback;

// feign失败配置
@Component
public class IArticleClientFallback implements IArticleClient {
    @Override
    public ResponseResult saveArticle(ArticleDto dto)  {
        return ResponseResult.errorResult(AppHttpCodeEnum.SERVER_ERROR,"获取数据失败");
    }
}
```

> 在自媒体微服务中添加类，扫描降级代码类的包
>

```java
package com.heima.wemedia.config;

@Configuration
@ComponentScan("com.heima.apis.article.fallback")
public class InitConfig {
}
```

> ②：远程接口中指向降级代码
>

```java
package com.heima.apis.article;

@FeignClient(value = "leadnews-article",fallback = IArticleClientFallback.class)
public interface IArticleClient {

    @PostMapping("/api/v1/article/save")
    public ResponseResult saveArticle(@RequestBody ArticleDto dto);
}
```

> ③：客户端开启降级heima-leadnews-wemedia,在wemedia的nacos配置中心里添加如下内容，开启服务降级，也可以指定服务响应的超时的时间

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630100928246.png" alt="image-20230630100928246" style="zoom:80%;" />

```yaml
feign:
  # 开启feign对hystrix熔断降级的支持
  hystrix:
    enabled: true
  # 修改调用超时时间
  client:
    config:
      default:
        connectTimeout: 2000
        readTimeout: 2000
```

> ④：测试,在ApArticleServiceImpl类中saveArticle方法添加代码
>

```java
try {
    Thread.sleep(3000);
} catch (InterruptedException e) {
    e.printStackTrace();
}
```

> 在自媒体端进行审核测试，会出现服务降级的现象
>

### 异步审核

#### 同步调用与异步调用

> 同步：就是在发出一个调用时，在没有得到结果之前， 该调用就不返回（实时处理）
>
> 异步：调用在发出之后，这个调用就直接返回了，没有返回结果（分时处理）

![image-20210507160912993](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555752.png)

> 异步线程的方式审核文章
>

#### Springboot集成异步线程调用

> ①：在自动审核的方法上加上@Async注解（标明要异步调用）

```java
@Override // 是在WmNewsAutoScanServiceImpl类上
@Async  //标明当前方法是一个异步方法
public void autoScanWmNews(Integer id) {
	//代码略
}
```

> ②：在文章发布成功后调用审核的方法:WmnewsServiceImpl
>

```java
@Resource
private WmNewsAutoScanService wmNewsAutoScanService;

// 发布修改文章或保存为草稿
@Override
public ResponseResult submitNews(WmNewsDto dto) {
    //代码略
    //审核文章
    wmNewsAutoScanService.autoScanWmNews(wmNews.getId());
    return ResponseResult.okResult(AppHttpCodeEnum.SUCCESS);
}
```

> ③：在自媒体引导类中使用@EnableAsync注解开启异步调用
>

```java
@SpringBootApplication
@EnableDiscoveryClient
@MapperScan("com.heima.wemedia.mapper")
@EnableFeignClients(basePackages = "com.heima.apis")
@EnableAsync  //开启异步调用
public class WemediaApplication {

    public static void main(String[] args) {
        SpringApplication.run(WemediaApplication.class,args);
    }

    @Bean
    public MybatisPlusInterceptor mybatisPlusInterceptor() {
        MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor();
        interceptor.addInnerInterceptor(new PaginationInnerInterceptor(DbType.MYSQL));
        return interceptor;
    }
}
```

### 综合测试

#### 服务启动列表

> 1，nacos服务端
>
> 2，article微服务
>
> 3，wemedia微服务
>
> 4，启动wemedia网关微服务
>
> 5，启动前端系统wemedia

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630101733719.png" alt="image-20230630101733719" style="zoom:80%;" />

#### 测试情况列表

> 1. 自媒体前端发布一篇正常的文章,审核成功后，app端的article相关数据是否可以正常保存，自媒体文章状态和app端文章id是否回显
> 2. 自媒体前端发布一篇包含敏感词的文章,正常是审核失败， wm_news表中的状态是否改变，成功和失败原因正常保存
> 3. 自媒体前端发布一篇包含敏感图片的文章,正常是审核失败， wm_news表中的状态是否改变，成功和失败原因正常保存

#### 功能测试

> conetent字段内容如下，是通过区别文字和图片的方式存入的

```json
[
    {
        "type": "text",
        "value": "找工作，企业重点问的是项目经验，更是hr筛选的“第一门槛”，。"
    },
    {
        "type": "image",
        "value": "http://192.168.200.130:91a3745efa8509a87f234813c.jpg"
    },
    {
        "type": "text",
        "value": "很多经过培训期望快行吗?”"
    }
]
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630102144086.png" alt="image-20230630102144086" style="zoom:80%;" />

```json
{
  "title": "这里是标题:撒旦java123456",
  "type": "3",
  "labels": "java",
  "publishTime": "2023-06-15T04:00:00.000Z",
  "channelId": 1,
  "images": [
    "http://120988e706941.png",
    "http://192.1680a0d0c3fd3bd.jpg",
    "http://192.168.88.101:9000/heimaleadne8b2b1b9305.jpg"
  ],
  "status": 1,
  "content": "[{\"type\":\"image\",\"value\":\"http://192.168.88.101:9000/heimaleadnews/2023/06/29/69bc3622e5cb4b5798280a0d0c3fd3bd.jpg\"},{\"type\":\"text\",\"value\":\"大撒大撒的撒大苏打\"},{\"type\":\"text\",\"value\":\"请在这里输入正文\"}]"
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630102207725.png" alt="image-20230630102207725" style="zoom:80%;" />

> 审核成功

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630102055622.png" alt="image-20230630102055622" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630102113591.png" alt="image-20230630102113591" style="zoom:80%;" />

## 新需求-自管理敏感词

### 需求分析

> 文章审核功能已经交付了，文章也能正常发布审核。突然，产品经理过来说要开会。
>

> 会议的内容核心有以下内容：

> 文章审核不能过滤一些敏感词：私人侦探、针孔摄象、信用卡提现、广告代理、代开发票、刻章办

> 需要完成的功能：需要自己维护一套敏感词，在文章审核的时候，需要验证文章是否包含这些敏感词
>

### 敏感词过滤技术

#### 技术选型

| **方案**               | **说明**                     |
| ---------------------- | ---------------------------- |
| 数据库模糊查询         | 效率太低                     |
| String.indexOf("")查找 | 数据库量大的话也是比较慢     |
| 全文检索               | 分词再匹配                   |
| DFA算法                | 确定有穷自动机(一种数据结构) |

#### DFA实现原理

> DFA全称为：Deterministic Finite Automaton,即确定有穷自动机。
>

> 存储：一次性的把所有的敏感词存储到了多个map中，就是下图表示这种结构
>

> 敏感词：冰毒、大麻、大坏蛋
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131615527.png" alt="image-20230613161516435" style="zoom:80%;" />

> 检索的过程
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131615220.png" alt="image-20230613161545121" style="zoom: 67%;" />



#### DFA工具类

```java
public class SensitiveWordUtil {

    public static Map<String, Object> dictionaryMap = new HashMap<>();

    // 生成关键词字典库
    public static void initMap(Collection<String> words) {
        if (words == null) {
            System.out.println("敏感词列表不能为空");
            return ;
        }
        // map初始长度words.size()，整个字典库的入口字数(小于words.size()，因为不同的词可能会有相同的首字)
        Map<String, Object> map = new HashMap<>(words.size());
        // 遍历过程中当前层次的数据
        Map<String, Object> curMap = null;
        Iterator<String> iterator = words.iterator();
        while (iterator.hasNext()) {
            String word = iterator.next();
            curMap = map;
            int len = word.length();
            for (int i =0; i < len; i++) {
                // 遍历每个词的字
                String key = String.valueOf(word.charAt(i));
                // 当前字在当前层是否存在, 不存在则新建, 当前层数据指向下一个节点, 继续判断是否存在数据
                Map<String, Object> wordMap = (Map<String, Object>) curMap.get(key);
                if (wordMap == null) {
                    // 每个节点存在两个数据: 下一个节点和isEnd(是否结束标志)
                    wordMap = new HashMap<>(2);
                    wordMap.put("isEnd", "0");
                    curMap.put(key, wordMap);
                }
                curMap = wordMap;
                // 如果当前字是词的最后一个字，则将isEnd标志置1
                if (i == len -1) {
                    curMap.put("isEnd", "1");
                }
            }
        }
        dictionaryMap = map;
    }

    // 搜索文本中某个文字是否匹配关键词
    private static int checkWord(String text, int beginIndex) {
        if (dictionaryMap == null) {
            throw new RuntimeException("字典不能为空");
        }
        boolean isEnd = false;
        int wordLength = 0;
        Map<String, Object> curMap = dictionaryMap;
        int len = text.length();
        // 从文本的第beginIndex开始匹配
        for (int i = beginIndex; i < len; i++) {
            String key = String.valueOf(text.charAt(i));
            // 获取当前key的下一个节点
            curMap = (Map<String, Object>) curMap.get(key);
            if (curMap == null) {
                break;
            } else {
                wordLength ++;
                if ("1".equals(curMap.get("isEnd"))) {
                    isEnd = true;
                }
            }
        }
        if (!isEnd) {
            wordLength = 0;
        }
        return wordLength;
    }

    // 获取匹配的关键词和命中次数
    public static Map<String, Integer> matchWords(String text) {
        Map<String, Integer> wordMap = new HashMap<>();
        int len = text.length();
        for (int i = 0; i < len; i++) {
            int wordLength = checkWord(text, i);
            if (wordLength > 0) {
                String word = text.substring(i, i + wordLength);
                // 添加关键词匹配次数
                if (wordMap.containsKey(word)) {
                    wordMap.put(word, wordMap.get(word) + 1);
                } else {
                    wordMap.put(word, 1);
                }

                i += wordLength - 1;
            }
        }
        return wordMap;
    }

    public static void main(String[] args) {
        // 添加敏感词
        List<String> list = new ArrayList<>();
        list.add("法轮");
        list.add("法轮功");
        list.add("冰毒");
        initMap(list);
        String content="我是一个好人，并不会卖冰毒，也不操练法轮功,我真的不卖冰毒";
        Map<String, Integer> map = matchWords(content);
        System.out.println(map);
    }
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630103126015.png" alt="image-20230630103126015" style="zoom:80%;" />

### 数据库表 - 实体类

> 创建敏感词表，导入资料中wm_sensitive到leadnews_wemedia库中

```sql
drop table if exists `wm_sensitive`;
create table `wm_sensitive` (
  `id` int(11) unsigned not null auto_increment comment '主键',
  `sensitives` varchar(10) collate utf8mb4_unicode_ci default null comment '敏感词',
  `created_time` datetime default null comment '创建时间',
  primary key (`id`) using btree
) engine=innodb auto_increment=3201 default charset=utf8mb4 comment='敏感词信息表';

insert into `wm_sensitive` values ('3104', '冰毒', '2021-05-23 15:38:51');
insert into `wm_sensitive` values ('3105', '法轮功', '2021-05-23 15:38:51');
insert into `wm_sensitive` values ('3106', '私人侦探', '2021-05-23 11:09:22');
insert into `wm_sensitive` values ('3107', '针孔摄象', '2021-05-23 11:09:52');
insert into `wm_sensitive` values ('3108', '信用卡提现', '2021-05-23 11:10:11');
insert into `wm_sensitive` values ('3109', '无抵押贷款', '2021-05-23 11:10:41');
insert into `wm_sensitive` values ('3110', '广告代理', '2021-05-23 11:10:59');
insert into `wm_sensitive` values ('3111', '代开发票', '2021-05-23 11:11:18');
insert into `wm_sensitive` values ('3112', '蚁力神', '2021-05-23 11:11:39');
insert into `wm_sensitive` values ('3113', '售肾', '2021-05-23 11:12:08');
insert into `wm_sensitive` values ('3114', '刻章办', '2021-05-23 11:12:24');
insert into `wm_sensitive` values ('3116', '套牌车', '2021-05-23 11:12:37');
insert into `wm_sensitive` values ('3117', '足球投注', '2021-05-23 11:12:51');
insert into `wm_sensitive` values ('3118', '地下钱庄', '2021-05-23 11:13:07');
insert into `wm_sensitive` values ('3119', '出售答案', '2021-05-23 11:13:24');
insert into `wm_sensitive` values ('3200', '小额贷款', '2021-05-23 11:13:40');
```

![image-20210524160611338](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555079.png)

```java
@Data
@TableName("wm_sensitive")
public class WmSensitive implements Serializable {

    private static final long serialVersionUID = 1L;

    //主键
    @TableId(value = "id", type = IdType.AUTO)
    private Integer id;

    //敏感词
    @TableField("sensitives")
    private String sensitives;

    //创建时间
    @TableField("created_time")
    private Date createdTime;
}
```

### WmSensitiveMapper

```java
package com.heima.wemedia.mapper;

@Mapper
public interface WmSensitiveMapper extends BaseMapper<WmSensitive> {
}
```

### WmNewsAutoScanServiceImpl

③：在文章审核的代码中添加自管理敏感词审核

第一：在WmNewsAutoScanServiceImpl中的autoScanWmNews方法上添加如下代码

```java
//从内容中提取纯文本内容和图片
//.....省略

//自管理的敏感词过滤
boolean isSensitive = handleSensitiveScan((String) textAndImages.get("content"), 
                                          wmNews);
if(!isSensitive) return;

//2.审核文本内容  阿里云接口
//.....省略
```

新增自管理敏感词审核代码

```java
@Resource
private WmSensitiveMapper wmSensitiveMapper;

// 自管理的敏感词审核
private boolean handleSensitiveScan(String content, WmNews wmNews) {

    boolean flag = true;

    //获取所有的敏感词
    List<WmSensitive> wmSensitives = wmSensitiveMapper
         .selectList(Wrappers.<WmSensitive>lambdaQuery()                                                              .select(WmSensitive::getSensitives));
    List<String> sensitiveList = wmSensitives.stream().map(WmSensitive::getSensitives)
                                             .collect(Collectors.toList());
    //初始化敏感词库
    SensitiveWordUtil.initMap(sensitiveList);
    //查看文章中是否包含敏感词
    Map<String, Integer> map = SensitiveWordUtil.matchWords(content);
    if(map.size() >0){
        updateWmNews(wmNews,(short) 2,"当前文章中存在违规内容"+map);
        flag = false;
    }
    return flag;
}
```

### 功能测试

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630105157247.png" alt="image-20230630105157247" style="zoom:80%;" />

```json
{
  "title": "针孔摄象:撒旦java123456",
  "type": "3",
  "labels": "java",
  "publishTime": "2023-06-15T04:00:00.000Z",
  "channelId": 1,
  "images": [
    "http://192.168.88.101:9000/heimaleadnews/2023/06/29/beede78e706941.png",
    "http://192.168.88.101:9000/heimaleadnews/2023/06/29/69bc3c3fd3bd.jpg",
    "http://192.168.88.101:9000/heimaleadnews/2023/06/29/a0dc8b2b1b9305.jpg"
  ],
  "status": 1,
  "content": "[{\"type\":\"image\",\"value\":\"http://192.168.88.101:9000/heimaleadnews/2023/06/29/69bc3622e5cb4b5798280a0d0c3fd3bd.jpg\"},{\"type\":\"text\",\"value\":\"大撒大撒的撒大苏打\"},{\"type\":\"text\",\"value\":\"出售答案\"}]"
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630105207806.png" alt="image-20230630105207806" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630105145350.png" alt="image-20230630105145350" style="zoom:80%;" />

## 新需求-图片识别审核敏感词

### 需求分析

> 产品经理召集开会，文章审核功能已经交付了，文章也能正常发布审核。对于上次提出的自管理敏感词也很满意，这次会议核心的内容如下：文章中包含的图片要识别文字，过滤掉图片文字的敏感词
>

![image-20210524161243572](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555104.png)

### 图片文字识别

> 什么是OCR?OCR （Optical Character Recognition，光学字符识别）是指电子设备（例如扫描仪或数码相机）检查纸上打印的字符，通过检测暗、亮的模式确定其形状，然后用字符识别方法将形状翻译成计算机文字的过程
>

| **方案**      | **说明**                                            |
| ------------- | --------------------------------------------------- |
| 百度OCR       | 收费                                                |
| Tesseract-OCR | Google维护的开源OCR引擎，支持Java，Python等语言调用 |
| Tess4J        | 封装了Tesseract-OCR  ，支持Java调用                 |

Tesseract-OCR 特点：

> Tesseract支持UTF-8编码格式，并且可以“开箱即用”地识别100多种语言。
>
> Tesseract支持多种输出格式：纯文本，hOCR（HTML），PDF等
>
> 官方建议，为了获得更好的OCR结果，最好提供给高质量的图像。
>
> Tesseract进行识别其他语言的训练
>
> 具体的训练方式，请参考官方提供的文档：https://tesseract-ocr.github.io/tessdoc/
>
> 数据文件：[tesseract-ocr/tessdat (github.com)](https://github.com/tesseract-ocr/tessdata)

### Tess4j实战

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630210514654.png" alt="image-20230630210514654" style="zoom:80%;" />

> ①：创建项目导入tess4j对应的依赖
>

```xml
<dependency>
    <groupId>net.sourceforge.tess4j</groupId>
    <artifactId>tess4j</artifactId>
    <version>4.1.1</version>
</dependency>
```

> ②：导入中文字体库， 把资料中的tessdata文件夹拷贝到自己的工作空间下(任意)[数据文件](https://github.com/tesseract-ocr/tessdata)
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630155200016.png" alt="image-20230630155200016" style="zoom:80%;" />

> ③：编写测试类进行测试:143.png

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630155510168.png" alt="image-20230630155510168" style="zoom:80%;" />

```java
public class test4j {
    public static void main(String[] args) {
        try {
            //获取本地图片
            File file = new File("D:\\springbootProject\\heima-leadresources\\143.png");
            //创建Tesseract对象
            ITesseract tesseract = new Tesseract();
            //设置字体库路径，目录即可
            tesseract.setDatapath("D:\\spt\\heima-leadnews\\resources\\tessdata");
            //中文识别
            tesseract.setLanguage("chi_sim");
            //执行ocr识别
            String result = tesseract.doOCR(file);
            //替换回车和tal键  使结果为一行
            result = result.replaceAll("\\r|\\n","-").replaceAll(" ","");
            System.out.println("识别的结果为："+result);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630163739274.png" alt="image-20230630163739274" style="zoom:80%;" />

### 代码整合

> ①：创建工具，简单封装一下tess4j，需要先导入pom

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230630220327964.png" alt="image-20230630220327964" style="zoom:80%;" />

```xml
<dependencies>
    <dependency>
        <groupId>net.sourceforge.tess4j</groupId>
        <artifactId>tess4j</artifactId>
        <version>4.1.1</version>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-configuration-processor</artifactId>
        <optional>true</optional>
    </dependency>
</dependencies>
```

> 工具类
>

```java
package com.heima.common.tess4j;

import lombok.Getter;
import lombok.Setter;
import net.sourceforge.tess4j.ITesseract;
import net.sourceforge.tess4j.Tesseract;
import net.sourceforge.tess4j.TesseractException;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.stereotype.Component;

import java.awt.image.BufferedImage;

@Getter
@Setter
@Component
@ConfigurationProperties(prefix = "tess4j")
public class Tess4jClient {

    private String dataPath;
    private String language;

    public String doOCR(BufferedImage image) throws TesseractException {
        //创建Tesseract对象
        ITesseract tesseract = new Tesseract();
        //设置字体库路径
        tesseract.setDatapath(dataPath);
        //中文识别
        tesseract.setLanguage(language);
        //执行ocr识别
        String result = tesseract.doOCR(image);
        //替换回车和tal键  使结果为一行
        result = result.replaceAll("\\r|\\n", "-").replaceAll(" ", "");
        return result;
    }
}
```

> 在spring.factories配置中添加该类,完整如下：然后再去clean和install
>

```java
org.springframework.boot.autoconfigure.EnableAutoConfiguration=\
  com.heima.test4j.Tess4jClient
```

> ②：在heima-leadnews-wemedia中的nacos配置中添加两个属性
>

```yaml
tess4j:
  data-path: D:\workspace\tessdata
  language: chi_sim
```

> 导入依赖

```xml
<dependency>
    <groupId>com.heima</groupId>
    <artifactId>test4j-demo</artifactId>
    <version>1.0-SNAPSHOT</version>
</dependency>
```

> ③：在WmNewsAutoScanServiceImpl中的handleImageScan方法上添加如下代码
>

```java
@Resource
private Tess4jClient tess4jClient;

try {
    for (String image : images) {
        byte[] bytes = fileStorageService.downLoadFile(image);
        //图片识别文字审核---begin-----
        //从byte[]转换为butteredImage
        ByteArrayInputStream in = new ByteArrayInputStream(bytes);
        BufferedImage imageFile = ImageIO.read(in);
        //识别图片的文字
        String result = tess4jClient.doOCR(imageFile);
        System.out.println(result);
        //审核是否包含自管理的敏感词
        boolean isSensitive = handleSensitiveScan(result, wmNews);
        if(!isSensitive){
            return isSensitive;
        }
        //图片识别文字审核---end-----
        imageList.add(bytes);

    } 
}catch (Exception e){
    e.printStackTrace();
}
```

> 上面的完整方法

```java
private boolean handleImageScan(List<String> images, WmNews wmNews) {

    boolean flag = true;
    if(images == null || images.size() == 0){
        return flag;
    }
    //下载图片 minIO
    //图片去重
    images = images.stream().distinct().collect(Collectors.toList());
    List<byte[]> imageList = new ArrayList<>();
    try {
        for (String image : images) {
            byte[] bytes = fileStorageService.downLoadFile(image);
            //图片识别文字审核---begin-----
            //从byte[]转换为butteredImage
            ByteArrayInputStream in = new ByteArrayInputStream(bytes);
            BufferedImage imageFile = ImageIO.read(in);
            //识别图片的文字
            String result = tess4jClient.doOCR(imageFile);
            System.out.println(result);
            //审核是否包含自管理的敏感词
            boolean isSensitive = handleSensitiveScan(result, wmNews);
            if(!isSensitive){
                return isSensitive;
            }
            //图片识别文字审核---end-----
            imageList.add(bytes);
        }
    }catch (Exception e){
        e.printStackTrace();
    }
    //审核图片
    try {

    } catch (Exception e) {
        flag = false;
        e.printStackTrace();
    }
    return flag;
}
```

> 注意修改第一个方法的调用，因为阿里云接口没实现，我就设置成true了

```java
//3.审核图片  阿里云接口
boolean isImageScan =  handleImageScan((List<String>) textAndImages
        .get("images"),wmNews);;
if(!isImageScan)return;
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701093057819.png" alt="image-20230701093057819" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701093119751.png" alt="image-20230701093119751" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701093137122.png" alt="image-20230701093137122" style="zoom:80%;" />

## 文章详情-静态文件生成上传

### 思路分析

文章端创建app相关文章时，生成文章详情静态页上传到MinIO中

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701093358984.png" alt="image-20230701093358984" style="zoom:80%;" />

### 实现步骤

> 1.新建ArticleFreemarkerService创建静态文件并上传到minIO中

```java
package com.heima.article.service;

public interface ArticleFreemarkerService {

    // 生成静态文件上传到minIO中
    public void buildArticleToMinIO(ApArticle apArticle,String content);
}
```

> 实现
>

```java
package com.heima.article.service.impl;

@Service
@Slf4j
public class ArticleFreemarkerServiceImpl implements ArticleFreemarkerService {

    @Resource
    private ApArticleContentMapper apArticleContentMapper;

    @Autowired
    private Configuration configuration;

    @Autowired
    private FileStorageService fileStorageService;

    @Autowired
    private ApArticleService apArticleService;

    // 生成静态文件上传到minIO中
    @Async
    @Override
    public void buildArticleToMinIO(ApArticle apArticle, String content) {
        //已知文章的id
        //4.1 获取文章内容
        if(StringUtils.isNotBlank(content)){
            //4.2 文章内容通过freemarker生成html文件
            Template template = null;
            StringWriter out = new StringWriter();
            try {
                template = configuration.getTemplate("article.ftl");
                //数据模型
                Map<String,Object> contentDataModel = new HashMap<>();
                contentDataModel.put("content", JSONArray.parseArray(content));
                //合成
                template.process(contentDataModel,out);
            } catch (Exception e) {
                e.printStackTrace();
            }

            //4.3 把html文件上传到minio中
            InputStream in = new ByteArrayInputStream(out.toString().getBytes());
            String path = fileStorageService.uploadHtmlFile("", apArticle.getId() + 
                                                            ".html", in);


            //4.4 修改ap_article表，保存static_url字段
            apArticleService.update(Wrappers.<ApArticle>lambdaUpdate()
                                    .eq(ApArticle::getId,apArticle.getId())
                    .set(ApArticle::getStaticUrl,path));
        }
    }
}
```

> 在ApArticleService的saveArticle实现方法中添加调用生成文件的方法
>

```java
@Resource
private ArticleFreemarkerService articleFreemarkerService;

// 保存app端相关文章
@Override
public ResponseResult saveArticle(ArticleDto dto) {

    //1.检查参数
    if(dto == null){
        return ResponseResult.errorResult(AppHttpCodeEnum.PARAM_INVALID);
    }

    ApArticle apArticle = new ApArticle();
    BeanUtils.copyProperties(dto,apArticle);

    //2.判断是否存在id
    if(dto.getId() == null){
        //2.1 不存在id  保存  文章  文章配置  文章内容

        //保存文章
        save(apArticle);

        //保存配置
        ApArticleConfig apArticleConfig = new ApArticleConfig(apArticle.getId());
        apArticleConfigMapper.insert(apArticleConfig);

        //保存 文章内容
        ApArticleContent apArticleContent = new ApArticleContent();
        apArticleContent.setArticleId(apArticle.getId());
        apArticleContent.setContent(dto.getContent());
        apArticleContentMapper.insert(apArticleContent);

    }else {
        //2.2 存在id   修改  文章  文章内容

        //修改  文章
        updateById(apArticle);

        //修改文章内容
        ApArticleContent apArticleContent = apArticleContentMapper
            .selectOne(Wrappers.<ApArticleContent>lambdaQuery()                                     .eq(ApArticleContent::getArticleId, dto.getId()));
        apArticleContent.setContent(dto.getContent());
        apArticleContentMapper.updateById(apArticleContent);
    }

    //异步调用 生成静态文件上传到minio中⭐
    articleFreemarkerService.buildArticleToMinIO(apArticle,dto.getContent());


    //3.结果返回  文章的id
    return ResponseResult.okResult(apArticle.getId());
}
```

> 3.文章微服务开启异步调用

![image-20210709111445360](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555418.png)



# 文章定时发布

## 延迟任务概述

### 什么是延迟任务

> 定时任务：有固定周期的，有明确的触发时间

> 延迟队列：没有固定的开始时间，它常常是由一个事件触发的，而在这个事件触发之后的一段时间内触发另一个事件，任务可以立即执行，也可以延迟

![image-20210513145942962](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555386.png)

应用场景：

> 场景一：订单下单之后30分钟后，用户没有付钱，则系统自动取消订单；如果期间下单成功，任务取消
>

> 场景二：接口对接出现网络问题，1分钟后重试，如果失败，2分钟重试，直到出现阈值终止
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131617993.png" alt="image-20230613161725897" style="zoom:80%;" />

### 技术对比

#### DelayQueue

> JDK自带DelayQueue 是一个支持延时获取元素的阻塞队列， 内部采用优先队列 PriorityQueue 存储元素，同时元素必须实现 Delayed 接口；在创建元素时可以指定多久才可以从队列中获取当前元素，只有在延迟期满时才能从队列中提取元素
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131618060.png" alt="image-20230613161818977" style="zoom:80%;" />

> DelayQueue属于排序队列，它的特殊之处在于队列的元素必须实现Delayed接口，该接口需要实现compareTo和getDelay方法
>

> getDelay方法：获取元素在队列中的剩余时间，只有当剩余时间为0时元素才可以出队列。
>
> compareTo方法：用于排序，确定元素出队列的顺序。

**实现：**

> 1：在测试包jdk下创建延迟任务元素对象DelayedTask，实现compareTo和getDelay方法，
>
> 2：在main方法中创建DelayQueue并向延迟队列中添加三个延迟任务，
>
> 3：循环的从延迟队列中拉取任务

```java
public class DelayedTask  implements Delayed{
    
    // 任务的执行时间
    private int executeTime = 0;
    
    public DelayedTask(int delay){
        Calendar calendar = Calendar.getInstance();
        calendar.add(Calendar.SECOND,delay);
        this.executeTime = (int)(calendar.getTimeInMillis() /1000 );
    }

    // 元素在队列中的剩余时间
    @Override
    public long getDelay(TimeUnit unit) {
        Calendar calendar = Calendar.getInstance();
        return executeTime - (calendar.getTimeInMillis()/1000);
    }

    // 元素排序
    @Override
    public int compareTo(Delayed o) {
        long val = this.getDelay(TimeUnit.NANOSECONDS) - 
                                  o.getDelay(TimeUnit.NANOSECONDS);
        return val == 0 ? 0 : ( val < 0 ? -1: 1 );
    }


    public static void main(String[] args) {
        DelayQueue<DelayedTask> queue = new DelayQueue<DelayedTask>();
        
        queue.add(new DelayedTask(5));
        queue.add(new DelayedTask(10));
        queue.add(new DelayedTask(15));

        System.out.println(System.currentTimeMillis()/1000+" start consume ");
        while(queue.size() != 0){
            DelayedTask delayedTask = queue.poll();
            if(delayedTask !=null ){
                System.out.println(System.currentTimeMillis()/1000+" cosume task");
            }
            //每隔一秒消费一次
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }     
    }
}
```

> DelayQueue实现完成之后思考一个问题：使用线程池或者原生DelayQueue程序挂掉之后，任务都是放在内存，需要考虑未处理消息的丢失带来的影响，如何保证数据不丢失，需要持久化（磁盘）
>

#### RabbitMQ实现延迟任务

> TTL：Time To Live (消息存活时间)

> 死信队列：Dead Letter Exchange(死信交换机)，当消息成为Dead message后，可以重新发送另一个交换机（死信交换机）

![image-20210513150319742](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555392.png)

#### Redis实现

> zset数据类型的去重有序（分数排序）特点进行延迟。例如：时间戳作为score进行排序
>

![image-20210513150352211](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555394.png)

> 例如：
>
> 生产者添加到4个任务到延迟队列中，时间毫秒值分别为97、98、99、100。当前时间的毫秒值为90
>
> 消费者端进行监听，如果当前时间的毫秒值匹配到了延迟队列中的毫秒值就立即消费

### Redis实现延迟任务

> 实现思路
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131619667.png" alt="image-20230613161912554" style="zoom:80%;" />

> 问题思路
>

> 为什么任务需要存储在数据库中？

> **延迟任务是一个通用的服务，任何需要延迟得任务都可以调用该服务，需要考虑数据持久化的问题，存储数据库中是一种数据安全的考虑**。

> 为什么redis中使用两种数据类型，list和zset？**效率问题，算法的时间复杂度**
>

> 在添加zset数据的时候，为什么不需要预加载？

> **任务模块是一个通用的模块，项目中任何需要延迟队列的地方，都可以调用这个接口，要考虑到数据量的问题，如果数据量特别大，为了防止阻塞，只需要把未来几分钟要执行的数据存入缓存即可**。

## 延迟任务服务实现

### 搭建heima-leadnews-schedule模块

> leadnews-schedule是一个通用的服务，单独创建模块来管理任何类型的延迟任务

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131619922.png" alt="image-20230613161942838" style="zoom:80%;" />

> ①：heima-leadnews-schedule模块到heima-leadnews-service下，如下图所示：

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701095249518.png" alt="image-20230701095249518" style="zoom: 50%;" />

![image-20210513151649297](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555405.png)

②：添加bootstrap.yml

```yaml
server:
  port: 51701
spring:
  application:
    name: leadnews-schedule
  cloud:
    nacos:
      discovery:
        server-addr: 192.168.88.101:8848
      config:
        server-addr: 192.168.88.101:8848
        file-extension: yml
```

③：在nacos中添加对应配置，并添加数据库及mybatis-plus的配置

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701095744633.png" alt="image-20230701095744633" style="zoom:80%;" />

```yaml
spring:
  datasource:
    driver-class-name: com.mysql.jdbc.Driver
    # useUnicode=true&characterEncoding=UTF-8&
    url: jdbc:mysql://localhost:3306/leadnews_schedule?serverTimezone=UTC
    username: root
    password: 123456
# 设置Mapper接口所对应的XML文件位置，如果你在Mapper接口中有自定义方法，需要进行该配置
mybatis-plus:
  mapper-locations: classpath*:mapper/*.xml
  # 设置别名包扫描路径，通过该属性可以给包中的类注册别名
  type-aliases-package: com.heima.model.schedule.pojos
```

> ScheduleApplication

```java
@SpringBootApplication
@MapperScan("com.heima.schedule.mapper")
public class ScheduleApplication {

    public static void main(String[] args) {
        SpringApplication.run(ScheduleApplication.class,args);
    }

    // mybatis-plus乐观锁支持
    @Bean
    public MybatisPlusInterceptor optimisticLockerInterceptor(){
        MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor();
        interceptor.addInnerInterceptor(new OptimisticLockerInnerInterceptor());
        return interceptor;
    }

}
```

### 表结构 - 实体类

> 导入资料中leadnews_schedule数据库
>

#### taskinfo 任务表

```sql
create database leadnews_schedule character set utf8mb4;
use leadnews_schedule;

drop table if exists `taskinfo`;
create table `taskinfo` (
  `task_id` bigint(20) not null comment '任务id',
  `execute_time` datetime(3) not null comment '执行时间',
  `parameters` longblob comment '参数',
  `priority` int(11) not null comment '优先级',
  `task_type` int(11) not null comment '任务类型',
  primary key (`task_id`),
  key `index_taskinfo_time` (`task_type`,`priority`,`execute_time`)
) engine=innodb default charset=utf8;
```

![image-20210513151812858](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555909.png)

> 实体类
>

```java
@Data
@TableName("taskinfo")
public class Taskinfo implements Serializable {

    private static final long serialVersionUID = 1L;

    //任务id
    @TableId(type = IdType.ID_WORKER)
    private Long taskId;

    //执行时间
    @TableField("execute_time")
    private Date executeTime;

    //参数
    @TableField("parameters")
    private byte[] parameters;

    //优先级
    @TableField("priority")
    private Integer priority;

    //任务类型
    @TableField("task_type")
    private Integer taskType;
}
```

#### taskinfo_logs 任务日志表

```sql
drop table if exists `taskinfo_logs`;
create table `taskinfo_logs` (
  `task_id` bigint(20) not null comment '任务id',
  `execute_time` datetime(3) not null comment '执行时间',
  `parameters` longblob comment '参数',
  `priority` int(11) not null comment '优先级',
  `task_type` int(11) not null comment '任务类型',
  `version` int(11) not null comment '版本号,用乐观锁',
  `status` int(11) default '0' comment '状态 0=初始化状态 1=executed 2=cancelled',
  primary key (`task_id`)
) engine=innodb default charset=utf8;
```

![image-20210513151835752](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555924.png)

> 实体类
>

```java
@Data
@TableName("taskinfo_logs")
public class TaskinfoLogs implements Serializable {

    private static final long serialVersionUID = 1L;

    //任务id
    @TableId(type = IdType.ID_WORKER)
    private Long taskId;

    //执行时间
    @TableField("execute_time")
    private Date executeTime;

    //参数
    @TableField("parameters")
    private byte[] parameters;

    //优先级
    @TableField("priority")
    private Integer priority;

    //任务类型
    @TableField("task_type")
    private Integer taskType;

    //版本号,用乐观锁
    @Version
    private Integer version;

    //状态 0=int 1=EXECUTED 2=CANCELLED
    @TableField("status")
    private Integer status;
}
```

### 悲观锁 & 乐观锁

#### 基本概念

> 悲观锁(Pessimistic Lock)：每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁.

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701100801852.png" alt="image-20230701100801852" style="zoom:80%;" />

> 乐观锁(Optimistic Lock)：每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701100834654.png" alt="image-20230701100834654" style="zoom:80%;" />

#### 乐观锁支持

> ①：在TaskIntoLog实体类中使用@Version标明是一个版本的字段

```java
/**版本号,用乐观锁 */
@Version
private Integer version;
```

> ②：mybatis-plus对乐观锁的支持，在启动类中向容器中放入乐观锁的拦截器

```java
// mybatis-plus乐观锁支持
@Bean
public MybatisPlusInterceptor optimisticLockerInterceptor(){
    MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor();
    interceptor.addInnerInterceptor(new OptimisticLockerInnerInterceptor());
    return interceptor;
}
```

### 安装Redis

> ①拉取镜像
>

```shell
docker pull redis
```

> ② 创建容器
>

```shell
docker run -d --name redis --restart=always -p 6380:6379 redis --requirepass "leadnews"
```

> ③链接测试：打开资料中的Redis Desktop Manager，输入host、port、password链接测试
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701101533856.png" alt="image-20230701101533856" style="zoom:80%;" />

> 能链接成功，即可
>

### 项目集成Redis

> ① 在项目导入redis相关依赖，已经完成
>

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
<!-- redis依赖commons-pool 这个依赖一定要添加 -->
<dependency>
    <groupId>org.apache.commons</groupId>
    <artifactId>commons-pool2</artifactId>
</dependency>
```

> ② 在heima-leadnews-schedule中集成redis,添加以下nacos配置，链接上redis

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701101735640.png" alt="image-20230701101735640" style="zoom:80%;" />

```yaml
spring:
  redis:
    host: 192.168.88.101
    password: leadnews
    port: 6380
```

> ④：测试List和Zset集合
>

```java
@SpringBootTest(classes = ScheduleApplication.class)
@RunWith(SpringRunner.class)
public class RedisTest {

    @Resource
    private StringRedisTemplate stringRedisTemplate;


    @Test
    public void testList(){
        //在list的左边添加元素
        stringRedisTemplate.opsForList().leftPush("list_001","hello,redis");
        //在list的右边获取元素，并删除
        String list_0011 = stringRedisTemplate.opsForList().rightPop("list_001");
        System.out.println(list_0011);
    }

    @Test
    public void testZset(){
        //添加数据到zset中  分值
        stringRedisTemplate.opsForZSet().add("zset_key_001","hello zset 001",1000);
        stringRedisTemplate.opsForZSet().add("zset_key_001","hello zset 002",8888);
        stringRedisTemplate.opsForZSet().add("zset_key_001","hello zset 003",7777);
        stringRedisTemplate.opsForZSet().add("zset_key_001","hello zset 004",999999);
        //按照分值获取数据
        Set<String> zset_key_001 = stringRedisTemplate.opsForZSet()
                                     .rangeByScore("zset_key_001", 0, 8888);
        zset_key_001.forEach(System.out::println);
    }
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701105206064.png" alt="image-20230701105206064" style="zoom:80%;" />

### 添加任务

#### mapper

> ①：拷贝mybatis-plus生成的文件，mapper

```java
@Mapper
public interface TaskinfoMapper extends BaseMapper<Taskinfo> {

    public List<Taskinfo> queryFutureTime(@Param("taskType")int type, 
                                          @Param("priority")int priority, 
                                          @Param("future")Date future);
}
```

```java
@Mapper
public interface TaskinfoLogsMapper extends BaseMapper<TaskinfoLogs> {

}
```

> maper/TaskinfoMapper.xml

```java
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.heima.schedule.mapper.TaskinfoMapper">

    <select id="queryFutureTime" resultType="com.heima.model.schedule.pojos.Taskinfo">
        select *
        from taskinfo
        where task_type = #{taskType}
          and priority = #{priority}
          and execute_time <![CDATA[<]]> #{future,javaType=java.util.Date}
    </select>

</mapper>
```

> ②：model模块创建task类，用于接收添加任务的参数

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701105531493.png" alt="image-20230701105531493" style="zoom:80%;" />

```java
@Data
public class Task implements Serializable {

    //任务id
    private Long taskId;
    //类型
    private Integer taskType;

    //优先级
    private Integer priority;

    //执行id
    private long executeTime;

    //task参数
    private byte[] parameters;
}
```

#### TaskService

```java
package com.heima.schedule.service;
import com.heima.model.schedule.dtos.Task;

// 对外访问接口
public interface TaskService {
    // 添加任务任务对象 任务id
    public long addTask(Task task) ;
}
```

> 实现类
>

```java
@Service
@Slf4j
public class TaskServiceImpl implements TaskService {
    // 添加延迟任务
    @Override
    public long addTask(Task task) {
        //1.添加任务到数据库中
        boolean success = addTaskToDb(task);
        if (success) {
            //2.添加任务到redis
            addTaskToCache(task);
        }
        return task.getTaskId();
    }

    @Resource
    private StringRedisTemplate stringRedisTemplate;

    // 把任务添加到redis中
    private void addTaskToCache(Task task) {
        String key = task.getTaskType() + "_" + task.getPriority();
        //获取5分钟之后的时间  毫秒值
        LocalDateTime currentTime = LocalDateTime.now();
        LocalDateTime nextSchedule = currentTime.plus(5, ChronoUnit.MINUTES);
        long nextScheduleTime = nextSchedule.toInstant(ZoneOffset.UTC).toEpochMilli();
        //2.1 如果任务的执行时间小于等于当前时间，存入list
        if (task.getExecuteTime() <= System.currentTimeMillis()) {
            //在list的左边添加元素
            stringRedisTemplate.opsForList()
                .leftPush(ScheduleConstants.TOPIC + key, JSON.toJSONString(task));
        } else if (task.getExecuteTime() <= nextScheduleTime) {
            //2.2 如果任务的执行时间大于当前时间 && 小于等于预设时间（未来5分钟） 存入zset中
            stringRedisTemplate.opsForZSet()
                .add(ScheduleConstants.FUTURE + key, JSON.toJSONString(task), 
                     task.getExecuteTime());
        }
    }

    @Resource
    private TaskinfoMapper taskinfoMapper;

    @Resource
    private TaskinfoLogsMapper taskinfoLogsMapper;

    // 添加任务到数据库中
    private boolean addTaskToDb(Task task) {
        boolean flag = false;
        try {
            //保存任务表
            Taskinfo taskinfo = new Taskinfo();
            BeanUtils.copyProperties(task, taskinfo);
            taskinfo.setExecuteTime(new Date(task.getExecuteTime()));
            taskinfoMapper.insert(taskinfo);
            //设置taskID
            task.setTaskId(taskinfo.getTaskId());
            //保存任务日志数据
            TaskinfoLogs taskinfoLogs = new TaskinfoLogs();
            BeanUtils.copyProperties(taskinfo, taskinfoLogs);
            taskinfoLogs.setVersion(1);
            taskinfoLogs.setStatus(ScheduleConstants.SCHEDULED);
            taskinfoLogsMapper.insert(taskinfoLogs);
            flag = true;
        } catch (Exception e) {
            e.printStackTrace();
        }
        return flag;
    }
}
```

#### ScheduleConstants常量类

```java
package com.heima.common.constants;

public class ScheduleConstants {

    //task状态
    public static final int SCHEDULED=0;   //初始化状态
    public static final int EXECUTED=1;       //已执行状态
    public static final int CANCELLED=2;   //已取消状态
    public static String FUTURE="future_";   //未来数据key前缀
    public static String TOPIC="topic_";     //当前数据key前缀
}
```

#### 功能测试

```java
@Resource
private TaskService taskService;

@Test
public void taskServiceTest(){
    Task task = new Task();
    task.setTaskType(100);
    task.setPriority(50);
    task.setParameters("task test".getBytes(StandardCharsets.UTF_8));
    task.setExecuteTime(new Date().getTime());
    long taskId = taskService.addTask(task);
    System.out.println(taskId);
}
```

![](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701111254619.png)

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701111245349.png" alt="image-20230701111245349" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701111108704.png" alt="image-20230701111108704" style="zoom:80%;" />

### 取消任务

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701111711684.png" alt="image-20230701111711684" style="zoom:80%;" />

> 在TaskService中添加方法
>

```java
// 取消任务 任务id取消结果
public boolean cancelTask(long taskId);
```

> 实现类
>

```java
// 取消任务
@Override
public boolean cancelTask(long taskId) {
    boolean flag = false;
    //删除任务，更新日志
    Task task = updateDb(taskId,ScheduleConstants.EXECUTED);
    //删除redis的数据
    if(task != null){
        removeTaskFromCache(task);
        flag = true;
    }
    return false;
}

// 删除redis中的任务数据
private void removeTaskFromCache(Task task) {
    String key = task.getTaskType()+"_"+task.getPriority();
    if(task.getExecuteTime()<=System.currentTimeMillis()){
        stringRedisTemplate.opsForList()
            .remove(ScheduleConstants.TOPIC+key,0,JSON.toJSONString(task));
    }else {
        stringRedisTemplate.opsForZSet()
            .remove(ScheduleConstants.FUTURE+key, JSON.toJSONString(task));
    }
}

// 删除任务，更新任务日志状态
private Task updateDb(long taskId, int status) {
    Task task = null;
    try {
        //删除任务
        taskinfoMapper.deleteById(taskId);
        TaskinfoLogs taskinfoLogs = taskinfoLogsMapper.selectById(taskId);
        taskinfoLogs.setStatus(status);
        taskinfoLogsMapper.updateById(taskinfoLogs);

        task = new Task();
        BeanUtils.copyProperties(taskinfoLogs,task);
        task.setExecuteTime(taskinfoLogs.getExecuteTime().getTime());
    }catch (Exception e){
        log.error("task cancel exception taskid={}",taskId);
    }
    return task;
}
```

> 功能测试：缓存删除，数据库日志更新，数据库任务删除

```java
@Test
public void cancelTaskTest(){
    taskService.cancelTask(1674978681272864769L);
}
```

### 消费任务

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701113302890.png" alt="image-20230701113302890" style="zoom:80%;" />

> 在TaskService中添加方法
>

```java
// 按照类型和优先级来拉取任务
public Task poll(int type,int priority);
```

> 实现类
>

```java
// 按照类型和优先级拉取任务
@Override
public Task poll(int type,int priority) {
    Task task = null;
    try {
        String key = type+"_"+priority;
        String task_json = stringRedisTemplate.opsForList()
                              .rightPop(ScheduleConstants.TOPIC + key);
        if(StringUtils.isNotBlank(task_json)){
            task = JSON.parseObject(task_json, Task.class);
            //更新数据库信息
            updateDb(task.getTaskId(),ScheduleConstants.EXECUTED);
        }
    }catch (Exception e){
        e.printStackTrace();
        log.error("poll task exception");
    }

    return task;
}
```

> 功能测试：先执行上面的新增方法，然后执行消费方法

```java
@Test
public void pollTaskTest(){
    Task task = taskService.poll(100, 50);
    System.out.println(task);
}
```



## 未来数据定时刷新

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701113820398.png" alt="image-20230701113820398" style="zoom:80%;" />

### Reids key值匹配

#### 方案1：keys 模糊匹配

> keys的模糊匹配功能很方便也很强大，但是在生产环境需要慎用！开发中使用keys的模糊匹配却发现redis的CPU使用率极高，所以公司的redis生产环境将keys命令禁用了！redis是单线程，会被堵塞
>

![image-20210515162329679](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555216.png)

#### 方案2：scan 

> SCAN 命令是一个基于游标的迭代器，SCAN命令每次被调用之后， 都会向用户返回一个新的游标， 用户在下次迭代时需要使用这个新游标作为SCAN命令的游标参数， 以此来延续之前的迭代过程。
>

![image-20210515162419548](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555233.png)

代码案例：

```java
@Test
public void testKeys(){
    // keys *
    Set<String> keys = stringRedisTemplate.keys("future_*");
    System.out.println(keys);
    // scan
    Set<String> scan = scan("future_*");
    System.out.println(scan);
}

public Set<String> scan(String patten){
    Set<String> keys = stringRedisTemplate.execute((RedisCallback<Set<String>>) 
                                                   connection -> {
        Set<String> result = new HashSet<>();
        try (Cursor<byte[]> cursor = connection.scan(new 
                                                     ScanOptions.ScanOptionsBuilder()
                .match(patten).count(10000).build())) {
            while (cursor.hasNext()) {
                result.add(new String(cursor.next()));
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return result;
    });
    return  keys;
}
```

### Reids管道-批量执行

> 普通redis客户端和服务器交互模式
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701120008753.png" alt="image-20230701120008753" style="zoom:67%;" />

> Pipeline请求模型
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701120118286.png" alt="image-20230701120118286" style="zoom:80%;" />

> 官方测试结果数据对比
>

![image-20210515162621928](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555541.png)

> 测试案例对比：
>

```java
//耗时6151
@Test
public  void testPiple1(){
    long start =System.currentTimeMillis();
    for (int i = 0; i <10000 ; i++) {
        Task task = new Task();
        task.setTaskType(1001);
        task.setPriority(1);
        task.setExecuteTime(new Date().getTime());
        cacheService.lLeftPush("1001_1", JSON.toJSONString(task));
    }
    System.out.println("耗时"+(System.currentTimeMillis()- start));
}


@Test
public void testPiple2(){
    long start  = System.currentTimeMillis();
    //使用管道技术
    List<Object> objectList = cacheService.getstringRedisTemplate()
        .executePipelined(new RedisCallback<Object>() {
        @Nullable
        @Override
        public Object doInRedis(RedisConnection redisConnection) throws 
            DataAccessException {
            for (int i = 0; i <10000 ; i++) {
                Task task = new Task();
                task.setTaskType(1001);
                task.setPriority(1);
                task.setExecuteTime(new Date().getTime());
                redisConnection.lPush("1001_1".getBytes(), 
                                      JSON.toJSONString(task).getBytes());
            }
            return null;
        }
    });
    System.out.println("使用管道技术执行10000次自增操作共耗时:"+
                       (System.currentTimeMillis()-start)+"毫秒");
}
```

```java
// 管道技术，提高性能
public List<Object> lRightPushPipeline(String type,Collection<String> values){
    List<Object> results = stringRedisTemplate.executePipelined(new 
                                              RedisCallback<Object>() {
                public Object doInRedis(RedisConnection connection) throws 
                    DataAccessException {
                    StringRedisConnection stringRedisConn = 
                        (StringRedisConnection)connection;
                    //集合转换数组
                    String[] strings = values.toArray(new String[values.size()]);
                    //直接批量发送
                    stringRedisConn.rPush(type, strings);
                    return null;
                }
            });
    return results;
}
```

### 未来数据定时刷新-功能完成

> 在TaskServiceImpl中添加方法
>

```java
// 每分钟执行一次
@Scheduled(cron = "0 */1 * * * ?")
public void refresh() {
    System.out.println(System.currentTimeMillis() / 1000 + "执行了定时任务");
    // 获取所有未来数据集合的key值,// future_*
    Set<String> futureKeys = scan(ScheduleConstants.FUTURE + "*");
    for (String futureKey : futureKeys) { // future_250_250
        String topicKey = ScheduleConstants.TOPIC +
                futureKey.split(ScheduleConstants.FUTURE)[1];
        //获取该组key下当前需要消费的任务数据
        Set<String> tasks = stringRedisTemplate.opsForZSet().rangeByScore(futureKey, 0,
                System.currentTimeMillis());
        if (!tasks.isEmpty()) {
            //将这些任务数据添加到消费者队列中
            refreshWithPipeline(futureKey, topicKey, tasks);
            System.out.println("成功的将" + futureKey + "下的当前需要执行的任务数据刷新到" +
                    topicKey + "下");
        }
    }
}
```

```java
public Set<String> scan(String patten){
    Set<String> keys = stringRedisTemplate.execute((RedisCallback<Set<String>>)
            connection -> {
                Set<String> result = new HashSet<>();
                try (Cursor<byte[]> cursor = connection.scan(new
                        ScanOptions.ScanOptionsBuilder()
                        .match(patten).count(10000).build())) {
                    while (cursor.hasNext()) {
                        result.add(new String(cursor.next()));
                    }
                } catch (IOException e) {
                    e.printStackTrace();
                }
                return result;
            });
    return  keys;
}
```

```java
public List<Object> refreshWithPipeline(String future_key,
                                        String topic_key,Collection<String> values){

    List<Object> objects = stringRedisTemplate.executePipelined(new 
                                                          RedisCallback<Object>() {
        @Nullable
        @Override
        public Object doInRedis(RedisConnection redisConnection) throws 
            DataAccessException {
            StringRedisConnection stringRedisConnection = 
                (StringRedisConnection)redisConnection;
            String[] strings = values.toArray(new String[values.size()]);
            stringRedisConnection.rPush(topic_key,strings);
            stringRedisConnection.zRem(future_key,strings);
            return null;
        }
    });
    return objects;
}
```

> 在引导类中添加开启任务调度注解：`@EnableScheduling`
>

> 功能测试：添加数据，此时不要启动服务

```java
@Test
public void taskServiceTest(){
    for (int i = 0; i < 5; i++) {
        Task task = new Task();
        task.setTaskType(100);
        task.setPriority(50);
        task.setParameters("task test".getBytes(StandardCharsets.UTF_8));
        // 未来的数据
        task.setExecuteTime(new Date().getTime()+500*i);
        long taskId = taskService.addTask(task);
        System.out.println(taskId);
    }
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701121148475.png" alt="image-20230701121148475" style="zoom:80%;" />

> 启动schedule服务

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701121450508.png" alt="image-20230701121450508" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701121353608.png" alt="image-20230701121353608" style="zoom:80%;" />



## 分布式锁解决集群方法抢占

### 问题描述

> 启动两台heima-leadnews-schedule服务，每台服务都会去执行refresh定时任务方法
>

![image-20210516112243712](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555818.png)

### 分布式锁

> 分布式锁：控制分布式系统有序的去对共享资源进行操作，通过互斥来保证数据的一致性。
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701121555526.png" alt="image-20230701121555526" style="zoom:80%;" />

### Redis分布式锁

> sexnx （SET if Not eXists） 命令在指定的 key 不存在时，为 key 设置指定的值。
>

![image-20210516112612399](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555853.png)

> 这种加锁的思路是，如果 key 不存在则为 key 设置 value，如果 key 已存在则 SETNX 命令不做任何操作

> - 客户端A请求服务器设置key的值，如果设置成功就表示加锁成功
> - 客户端B也去请求服务器设置key的值，如果返回失败，那么就代表加锁失败
> - 客户端A执行代码完成，删除锁
> - 客户端B在等待一段时间后再去请求设置key的值，设置成功
> - 客户端B执行代码完成，删除锁

> 在ScheduleServiceImpl添加方法

```java
// 加锁
public String tryLock(String name, long expire) {
    name = name + "_lock";
    String token = UUID.randomUUID().toString();
    RedisConnectionFactory factory = stringRedisTemplate.getConnectionFactory();
    RedisConnection conn = factory.getConnection();
    try {
        //参考redis命令：
        //set key value [EX seconds] [PX milliseconds] [NX|XX]
        Boolean result = conn.set(
                name.getBytes(),
                token.getBytes(),
                Expiration.from(expire, TimeUnit.MILLISECONDS),
                RedisStringCommands.SetOption.SET_IF_ABSENT //NX
        );
        if (result != null && result)
            return token;
    } finally {
        RedisConnectionUtils.releaseConnection(conn, factory,false);
    }
    return null;
}
```

> 修改未来数据定时刷新的方法，如下：
>

```java
// 每分钟执行一次
@Scheduled(cron = "0 */1 * * * ?")
public void refresh() {
    // 上锁
    String token = tryLock("FUTURE_TASK_SYNC", 1000 * 30);
    if(StringUtils.isNotBlank(token)){
        log.info("未来数据定时刷新---定时任务");
        //获取所有未来数据的集合key
        Set<String> futureKeys = scan(ScheduleConstants.FUTURE + "*");
        for (String futureKey : futureKeys) {//future_100_50

            //获取当前数据的key  topic
            String topicKey = ScheduleConstants.TOPIC+futureKey
                    .split(ScheduleConstants.FUTURE)[1];

            //按照key和分值查询符合条件的数据
            Set<String> tasks = stringRedisTemplate.opsForZSet()
                                .rangeByScore(futureKey, 0,
                    System.currentTimeMillis());
            //同步数据
            if(!tasks.isEmpty()){
                refreshWithPipeline(futureKey,topicKey,tasks);
                log.info("成功的将"+futureKey+"刷新到了"+topicKey);
            }
        }
    }
}
```

## 数据库同步到Redis

![image-20210721013255332](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131555121.png)

```java
// 1、清理缓存中的数据
private void clearCache(){
    // 删除缓存中未来数据集合和当前消费者队列的所有key
    Set<String> futurekeys = scan(ScheduleConstants.FUTURE + "*");// future_
    Set<String> topickeys = scan(ScheduleConstants.TOPIC + "*");// topic_
    stringRedisTemplate.delete(futurekeys);
    stringRedisTemplate.delete(topickeys);
}
```

```java
@Scheduled(cron = "0 */5 * * * ?")
@PostConstruct // 启动微服务立即启动该方法
public void reloadData() {
    // 1、清理缓存中的数据
    clearCache();
    log.info("数据库数据同步到缓存");
    Calendar calendar = Calendar.getInstance();
    calendar.add(Calendar.MINUTE, 5);
    // 2、查看小于未来5分钟的所有任务
    List<Taskinfo> allTasks = taskinfoMapper.selectList(Wrappers.<Taskinfo>lambdaQuery().lt(Taskinfo::getExecuteTime,calendar.getTime()));
    // 3、新增任务到redis
    if(allTasks != null && allTasks.size() > 0){
        for (Taskinfo taskinfo : allTasks) {
            Task task = new Task();
            BeanUtils.copyProperties(taskinfo,task);
            task.setExecuteTime(taskinfo.getExecuteTime().getTime());
            addTaskToCache(task);
        }
    }
}
```



## 精准时间发布文章

### IScheduleClient

> 提供远程的feign接口，在heima-leadnews-feign-api编写类如下：
>

```java
package com.heima.apis.schedule;


@FeignClient("leadnews-schedule")
public interface IScheduleClient {

    //添加任务 任务对象 任务id
    @PostMapping("/api/v1/task/add")
    public ResponseResult  addTask(@RequestBody Task task);

    // 取消任务 任务id   取消结果
    @GetMapping("/api/v1/task/cancel/{taskId}")
    public ResponseResult cancelTask(@PathVariable("taskId") long taskId);

    // 按照类型和优先级来拉取任务
    @GetMapping("/api/v1/task/poll/{type}/{priority}")
    public ResponseResult poll(@PathVariable("type") int type,
                               @PathVariable("priority")  int priority);
}
```

> 在heima-leadnews-schedule微服务下提供对应的实现
>

```java
package com.heima.schedule.feign;

@RestController
public class ScheduleClient  implements IScheduleClient {

    @Autowired
    private TaskService taskService;

    // 添加任务 任务对象 任务id
    @PostMapping("/api/v1/task/add")
    @Override
    public ResponseResult addTask(@RequestBody Task task) {
        return ResponseResult.okResult(taskService.addTask(task));
    }

    // 取消任务taskId 任务id取消结果
    @GetMapping("/api/v1/task/cancel/{taskId}")
    @Override
    public ResponseResult cancelTask(@PathVariable("taskId") long taskId) {
        return ResponseResult.okResult(taskService.cancelTask(taskId));
    }

    // 按照类型和优先级来拉取任务
    @GetMapping("/api/v1/task/poll/{type}/{priority}")
    @Override
    public ResponseResult poll(@PathVariable("type") int type, 
                               @PathVariable("priority") int priority) {
        return ResponseResult.okResult(taskService.poll(type,priority));
    }
}
```

### WmNewsTaskService

> 在创建WmNewsTaskService
>

```java
package com.heima.wemedia.service;

public interface WmNewsTaskService {

    /**
     * 添加任务到延迟队列中
     * @param id  文章的id
     * @param publishTime  发布的时间  可以做为任务的执行时间
     */
    public void addNewsToTask(Integer id, Date publishTime);

}
```

> 实现
>

```java
@Service
@Slf4j
public class WmNewsTaskServiceImpl  implements WmNewsTaskService {


    @Autowired
    private IScheduleClient scheduleClient;
    
    @Override
    @Async
    public void addNewsToTask(Integer id, Date publishTime) {
        log.info("添加任务到延迟服务中----begin");
        Task task = new Task();
        task.setExecuteTime(publishTime.getTime());
        task.setTaskType(TaskTypeEnum.NEWS_SCAN_TIME.getTaskType());
        task.setPriority(TaskTypeEnum.NEWS_SCAN_TIME.getPriority());
        WmNews wmNews = new WmNews();
        wmNews.setId(id);
        task.setParameters(ProtostuffUtil.serialize(wmNews));
        scheduleClient.addTask(task);
        log.info("添加任务到延迟服务中----end");
    }
}
```

> 前面已经开启过异步调用了

```java
@EnableAsync  //开启异步调用
```

### TaskTypeEnum

> 枚举类
>

```java
package com.heima.model.common.enums;

import lombok.AllArgsConstructor;
import lombok.Getter;

@Getter
@AllArgsConstructor
public enum TaskTypeEnum {

    NEWS_SCAN_TIME(1001, 1,"文章定时审核"),
    REMOTEERROR(1002, 2,"第三方接口调用失败，重试");
    private final int taskType; //对应具体业务
    private final int priority; //业务不同级别
    private final String desc; //描述信息
}
```

### 序列化工具

> JdkSerialize：java内置的序列化能将实现了Serilazable接口的对象进行序列化和反序列化， ObjectOutputStream的writeObject()方法可序列化对象生成字节数组

> Protostuff：google开源的protostuff采用更为紧凑的二进制数组，表现更加优异，然后使用protostuff的编译工具生成pojo类

> Protostuff需要引导依赖：
>

```xml
<dependency>
    <groupId>io.protostuff</groupId>
    <artifactId>protostuff-core</artifactId>
    <version>1.6.0</version>
</dependency>

<dependency>
    <groupId>io.protostuff</groupId>
    <artifactId>protostuff-runtime</artifactId>
    <version>1.6.0</version>
</dependency>
```

> 序列化工具类

```java
public class JdkSerializeUtil {

    // 序列化
    public static <T> byte[] serialize(T obj) {

        if (obj  == null){
            throw new NullPointerException();
        }
        ByteArrayOutputStream bos = new ByteArrayOutputStream();
        try {
            ObjectOutputStream oos = new ObjectOutputStream(bos);

            oos.writeObject(obj);
            return bos.toByteArray();
        } catch (Exception ex) {
            ex.printStackTrace();
        }
        return new byte[0];
    }

    // 反序列化
    public static <T> T deserialize(byte[] data, Class<T> clazz) {
        ByteArrayInputStream bis = new ByteArrayInputStream(data);

        try {
            ObjectInputStream ois = new ObjectInputStream(bis);
            T obj = (T)ois.readObject();
            return obj;
        } catch (Exception ex) {
            ex.printStackTrace();
        }

        return  null;
    }
}
```

> Protostuff序列化

```java
public class ProtostuffUtil {

    // 序列化
    public static <T> byte[] serialize(T t){
        Schema schema = RuntimeSchema.getSchema(t.getClass());
        return ProtostuffIOUtil.toByteArray(t,schema,
                LinkedBuffer.allocate(LinkedBuffer.DEFAULT_BUFFER_SIZE));
 
    }

    // 反序列化
    public static <T> T deserialize(byte []bytes,Class<T> c) {
        T t = null;
        try {
            t = c.newInstance();
            Schema schema = RuntimeSchema.getSchema(t.getClass());
             ProtostuffIOUtil.mergeFrom(bytes,t,schema);
        } catch (InstantiationException e) {
            e.printStackTrace();
        } catch (IllegalAccessException e) {
            e.printStackTrace();
        }
        return t;
    }

    // jdk序列化与protostuff序列化对比
    public static void main(String[] args) {
        long start =System.currentTimeMillis();
        for (int i = 0; i <1000000 ; i++) {
            WmNews wmNews =new WmNews();
            JdkSerializeUtil.serialize(wmNews);
        }
        System.out.println(" jdk 花费 "+(System.currentTimeMillis()-start));

        start =System.currentTimeMillis();
        for (int i = 0; i <1000000 ; i++) {
            WmNews wmNews =new WmNews();
            ProtostuffUtil.serialize(wmNews);
        }
        System.out.println(" protostuff 花费 "+(System.currentTimeMillis()-start));
    }
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701125025527.png" alt="image-20230701125025527" style="zoom:80%;" />



### WmNewsServiceImpl

> 修改发布文章代码：把之前的异步调用修改为调用延迟任务

```java
@Autowired
private WmNewsTaskService wmNewsTaskService;
 
// 发布修改文章或保存为草稿
@Override
public ResponseResult submitNews(WmNewsDto dto) {
    //0.条件判断
    if(dto == null || dto.getContent() == null){
        return ResponseResult.errorResult(AppHttpCodeEnum.PARAM_INVALID);
    }
    //1.保存或修改文章
    WmNews wmNews = new WmNews();
    //属性拷贝 属性名词和类型相同才能拷贝
    BeanUtils.copyProperties(dto,wmNews);
    //封面图片  list---> string
    if(dto.getImages() != null && dto.getImages().size() > 0){
        //[1dddfsd.jpg,sdlfjldk.jpg]-->   1dddfsd.jpg,sdlfjldk.jpg
        String imageStr = StringUtils.join(dto.getImages(), ",");
        wmNews.setImages(imageStr);
    }
    //如果当前封面类型为自动 -1
    if(dto.getType().equals(WemediaConstants.WM_NEWS_TYPE_AUTO)){
        wmNews.setType(null);
    }
    saveOrUpdateWmNews(wmNews);
    //2.判断是否为草稿  如果为草稿结束当前方法
    if(dto.getStatus().equals(WmNews.Status.NORMAL.getCode())){
        return ResponseResult.okResult(AppHttpCodeEnum.SUCCESS);
    }
    //3.不是草稿，保存文章内容图片与素材的关系
    //获取到文章内容中的图片信息
    List<String> materials =  ectractUrlInfo(dto.getContent());
    saveRelativeInfoForContent(materials,wmNews.getId());

    //4.不是草稿，保存文章封面图片与素材的关系，如果当前布局是自动，需要匹配封面图片
    saveRelativeInfoForCover(dto,wmNews,materials);

    //审核文章
    //这里进行🚗🚗
    wmNewsTaskService.addNewsToTask(wmNews.getId(),wmNews.getPublishTime());
    return ResponseResult.okResult(AppHttpCodeEnum.SUCCESS);
}
```

### 启动测试

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701125901649.png" alt="image-20230701125901649" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701125949051.png" alt="image-20230701125949051" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701130024849.png" alt="image-20230701130024849" style="zoom:80%;" />

## 消费任务进行审核文章

> WmNewsTaskService

```java
// 消费延迟队列数据
public void scanNewsByTask();
```

> 实现
>

```java
@Autowired
private WmNewsAutoScanServiceImpl wmNewsAutoScanService;

@Autowired
private IScheduleClient scheduleClient;
// 消费延迟队列数据
@Scheduled(fixedRate = 1000)
@Override
@SneakyThrows
public void scanNewsByTask() {

    log.info("文章审核---消费任务执行---begin---");
    ResponseResult responseResult = scheduleClient.poll(TaskTypeEnum.
                           NEWS_SCAN_TIME.getTaskType(),                                                            TaskTypeEnum.NEWS_SCAN_TIME.getPriority());
    if(responseResult.getCode().equals(200) && responseResult.getData() != null){
        String json_str = JSON.toJSONString(responseResult.getData());
        Task task = JSON.parseObject(json_str, Task.class);
        byte[] parameters = task.getParameters();
        WmNews wmNews = ProtostuffUtil.deserialize(parameters, WmNews.class);
        System.out.println(wmNews.getId()+"-----------");
        wmNewsAutoScanService.autoScanWmNews(wmNews.getId());
    }
    log.info("文章审核---消费任务执行---end---");
}
```

> 在WemediaApplication自媒体的引导类中添加开启任务调度注解`@EnableScheduling`
>



# 消息中间件-kafka

## 基础概述

### 消息中间件对比          

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131621593.png" alt="image-20230613162121498" style="zoom:80%;" />                    

> 消息中间件对比-选择建议
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131621898.png" alt="image-20230613162144811" style="zoom:80%;" />

### kafka介绍

Kafka 是一个分布式流媒体平台,类似于消息队列或企业消息传递系统。kafka官网：http://kafka.apache.org/  

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556099.png" alt="image-20210525181028436" style="zoom:80%;" />

kafka介绍-名词解释

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556101.png" alt="image-20210525181100793" style="zoom:80%;" />

> - producer：发布消息的对象称之为主题生产者（Kafka topic producer）
>
> - topic：Kafka将消息分门别类，每一类的消息称之为一个主题（Topic）
>
> - consumer：订阅消息并处理发布的消息的对象称之为主题消费者（consumers）
>
> - broker：已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理（Broker）。 消费者可以订阅一个或多个主题（topic），并从Broker拉数据，从而消费这些已发布的消息。

## kafka安装启动

> Kafka对于zookeeper是强依赖，保存kafka相关的节点数据，所以安装Kafka之前必须先安装zookeeper

### Docker安装

> Docker安装zookeeper，下载镜像：
>

```shell
docker pull zookeeper:3.4.14
```

> 创建容器
>

```shell
docker run -d --name zookeeper -p 2181:2181 zookeeper:3.4.14
```

> Docker安装kafka，下载镜像：
>

```shell
docker pull wurstmeister/kafka:2.12-2.3.1
```

> 创建容器
>

```shell
docker run -d --name kafka \
--env KAFKA_ADVERTISED_HOST_NAME=192.168.88.101 \
--env KAFKA_ZOOKEEPER_CONNECT=192.168.88.101:2181 \
--env KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.88.101:9092 \
--env KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 \
--env KAFKA_HEAP_OPTS="-Xmx256M -Xms256M" \
--net=host wurstmeister/kafka:2.12-2.3.1
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131622419.png" alt="image-20230613162231344" style="zoom:80%;" />

### 压缩包安装

> 首先我们需要下载Kafka的安装包，下载网站：https://www.apache.org/dyn/closer.cgi?path=/kafka/3.2.0/kafka_2.13-3.2.0.tgz，或者直接使用命令行下载

```sh
wget https://archive.apache.org/dist/kafka/3.2.0/kafka_2.13-3.2.0.tgz
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207271608757.png" alt="image-20220727160815635" style="zoom:67%;" />

下载完成后将Kafka解压到指定目录：解压完成后进入到解压目录：

```c
mkdir -p /mydata/kafka/ 
tar -zxvf kafka_2.13-3.2.0.tgz -C /mydata/kafka/
```

```c
cd /mydata/kafka/kafka_2.13-3.2.0
```

> 虽然有消息称Kafka即将移除Zookeeper，但是在Kafka最新版本中尚未移除，所以启动Kafka前还是需要先启Zookeeper；启动Zookeeper服务，服务将运行在`2181`端口；
>

```c
// 后台运行服务，并把日志输出到当前文件夹下的zookeeper-out.file文件中
nohup bin/zookeeper-server-start.sh config/zookeeper.properties > zookeeper-out.file 2>&1 &
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2022.6.30/202207271612068.png" alt="image-20220727161252012" style="zoom:67%;" />

> 最后启动Kafka服务，服务将运行在`9092`端口。

```c
// 后台运行服务，并把日志输出到当前文件夹下的kafka-out.file文件中
nohup bin/kafka-server-start.sh config/server.properties > kafka-out.file 2>&1 &
```

## kafka入门

### 生产消费

> 生产者发送消息，**同一个组**内的多个消费者只能有一个消费者接收到消息

![image-20210525181412230](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556103.png)

> 生产者发送消息，**多个组**的多个消费者都可以接收到消息

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131624597.png" alt="image-20230613162410509" style="zoom:80%;" />

> - 同一个组，生产者发送消息，多个消费者订阅同一个主题，只有一个消费者收到消息（一对一）
> - 多个组，生产者发送消息，多个消费者订阅同一个主题，所有消费者都能收到消息（一对多）

### 分区机制

> Kafka 中的分区机制指的是将每个主题划分成多个分区（Partition）
>
> 可以处理更多的消息，不受单台服务器的限制，可以不受限的处理更多的数据

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701161634892.png" alt="image-20230701161634892" style="zoom:80%;" />

### topic剖析

> 每一个分区都是一个顺序的、不可变的消息队列， 并且可以持续的添加。分区中的消息都被分了一个序列号，称之为偏移量(offset)，在每个分区中此偏移量都是唯一的。

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131626789.png" alt="image-20230613162641702" style="zoom:80%;" />

### 分区策略

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131626984.png" alt="image-20230613162659896" style="zoom:80%;" />

## kafka高可用设计

### 集群

![image-20210530223101568](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556108.png)

> Kafka 的服务器端由被称为 Broker 的服务进程构成，即一个 Kafka 集群由多个 Broker 组成，这样如果集群中某一台机器宕机，其他机器上的 Broker 也依然能够对外提供服务。这其实就是 Kafka 提供高可用的手段之一

### 备份机制(Replication）-同步方式

![image-20210530223218580](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556663.png)

> Kafka 中消息的备份又叫做 副本（Replica）
>

Kafka 定义了两类副本：

> - 领导者副本（Leader Replica）
>
> - 追随者副本（Follower Replica）

![image-20210530223316815](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556680.png)

> ISR（in-sync replica）需要同步复制保存的follower
>

如果leader失效后，需要选出新的leader，选举的原则如下：

> 第一：选举时优先从ISR中选定，因为这个列表中follower的数据是与leader同步的
>
> 第二：如果ISR列表中的follower都不行了，就只能从其他follower中选取

极端情况，就是所有副本都失效了，这时有两种方案

> 第一：等待ISR中的一个活过来，选为Leader，数据可靠，但活过来的时间不确定
>

> 第二：选择第一个活过来的Replication，不一定是ISR中的，选为leader，以最快速度恢复可用性，但数据不一定完整
>

## kafka生产者详解 

### 发送类型

> 同步发送:使用send()方法发送，它会返回一个Future对象，调用get()方法进行等待，就可以知道消息是否发送成功
>

```java
RecordMetadata recordMetadata = producer.send(kvProducerRecord).get();
System.out.println(recordMetadata.offset());
```

> 异步发送：调用send()方法，并指定一个回调函数，服务器在返回响应时调用函数
>

```java
//异步消息发送
producer.send(kvProducerRecord, new Callback() {
    @Override
    public void onCompletion(RecordMetadata recordMetadata, Exception e) {
        if(e != null){
            System.out.println("记录异常信息到日志表中");
        }
        System.out.println(recordMetadata.offset());
    }
});
```

### 参数详解

#### 确认机制ack

![image-20210530224302935](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556698.png)

代码的配置方式：

```java
//ack配置  消息确认机制
prop.put(ProducerConfig.ACKS_CONFIG,"all");
```

参数的选择说明

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131628825.png" alt="image-20230613162816740" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131628284.png" alt="image-20230613162831195" style="zoom:80%;" />

#### 重试次数retries

![image-20210530224406689](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556960.png)

> 生产者从服务器收到的错误有可能是临时性错误，在这种情况下，retries参数的值决定了生产者可以重发消息的次数，如果达到这个次数，生产者会放弃重试返回错误，默认情况下，生产者会在每次重试之间等待100ms代码中配置方式：
>

```java
//重试次数
prop.put(ProducerConfig.RETRIES_CONFIG,10);
```

### 消息压缩

> 默认情况下， 消息发送时不会被压缩。代码中配置方式：
>

```java
//数据压缩
prop.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,"lz4");
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131630827.png" alt="image-20230613163026737" style="zoom:80%;" />

> 使用压缩可以降低网络传输开销和存储开销，而这往往是向 Kafka 发送消息的瓶颈所在。
>

## kafka消费者详解

### 消费者组

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556971.png" alt="image-20210530224706747" style="zoom:80%;" />

> 消费者组（Consumer Group） ：指的就是由一个或多个消费者组成的群体
>
> 一个发布在Topic上消息被分发给此消费者组中的一个消费者

> - 所有的消费者都在一个组中，那么这就变成了queue模型
>
> - 所有的消费者都在不同的组中，那么就完全变成了发布-订阅模型

### 消息有序性

应用场景：

> - 即时消息中的单对单聊天和群聊，保证发送方消息发送顺序与接收方的顺序一致
>
> - 充值转账两个渠道在同一个时间进行余额变更，短信通知必须要有顺序

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131632291.png" alt="image-20230613163201170" style="zoom:80%;" />

> topic分区中消息只能由消费者组中的唯一一个消费者处理，所以消息肯定是按照先后顺序进行处理的。但是它也仅仅是保证Topic的一个分区顺序处理，不能保证跨分区的消息先后处理顺序。 所以，如果你想要顺序的处理Topic的所有消息，那就只提供一个分区。
>

### 提交和偏移量

> kafka不会像其他JMS队列那样需要得到消费者的确认，消费者可以使用kafka来追踪消息在分区的位置（偏移量）,消费者会往一个叫做_consumer_offset的特殊主题发送消息，消息里包含了每个分区的偏移量。如果消费者发生崩溃或有新的消费者加入群组，就会触发再均衡
>

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556288.png" alt="image-20210530225021266" style="zoom:80%;" />

> 正常的情况
>

![image-20210530224959350](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556302.png)

> 如果消费者2挂掉以后，会发生再均衡，消费者2负责的分区会被其他消费者进行消费
>

> 再均衡后不可避免会出现一些问题
>

> 问题一：
>

![image-20210530225215337](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556321.png)

> 如果提交偏移量小于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息就会被重复处理。
>

> 问题二：
>

![image-20210530225239897](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556591.png)

> 如果提交的偏移量大于客户端的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。
>
> 如果想要解决这些问题，还要知道目前kafka提交偏移量的方式：
>
> 提交偏移量的方式有两种，分别是自动提交偏移量和手动提交

> 自动提交偏移量

> 当enable.auto.commit被设置为true，提交方式就是让消费者自动提交偏移量，每隔5秒消费者会自动把从poll()方法接收的最大偏移量提交上去
>

> 手动提交 ，当enable.auto.commit被设置为false可以有以下三种提交方式

> - 提交当前偏移量（同步提交）
>
> - 异步提交
>
> - 同步和异步组合提交

1.提交当前偏移量（同步提交）

> 把`enable.auto.commit`设置为false,让应用程序决定何时提交偏移量。使用commitSync()提交偏移量，commitSync()将会提交poll返回的最新的偏移量，所以在处理完所有记录后要确保调用了commitSync()方法。否则还是会有消息丢失的风险。
>

> 只要没有发生不可恢复的错误，commitSync()方法会一直尝试直至提交成功，如果提交失败也可以记录到错误日志里。
>

```java
while (true){
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
    for (ConsumerRecord<String, String> record : records) {
        System.out.println(record.value());
        System.out.println(record.key());
        try {
            consumer.commitSync();//同步提交当前最新的偏移量
        }catch (CommitFailedException e){
            System.out.println("记录提交失败的异常："+e);
        }

    }
}
```

2.异步提交

> 手动提交有一个缺点，那就是当发起提交调用时应用会阻塞。当然我们可以减少手动提交的频率，但这个会增加消息重复的概率（和自动提交一样）。另外一个解决办法是，使用异步提交的API。
>

```java
while (true){
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
    for (ConsumerRecord<String, String> record : records) {
        System.out.println(record.value());
        System.out.println(record.key());
    }
    consumer.commitAsync(new OffsetCommitCallback() {
        @Override
        public void onComplete(Map<TopicPartition, OffsetAndMetadata> map, Exception e) {
            if(e!=null){
                System.out.println("记录错误的提交偏移量："+ map+",异常信息"+e);
            }
        }
    });
}
```

3.同步和异步组合提交

> 异步提交也有个缺点，那就是如果服务器返回提交失败，异步提交不会进行重试。相比较起来，同步提交会进行重试直到成功或者最后抛出异常给应用。异步提交没有实现重试是因为，如果同时存在多个异步提交，进行重试可能会导致位移覆盖。
>

> 举个例子，假如我们发起了一个异步提交commitA，此时的提交位移为2000，随后又发起了一个异步提交commitB且位移为3000；commitA提交失败但commitB提交成功，此时commitA进行重试并成功的话，会将实际上将已经提交的位移从3000回滚到2000，导致消息重复消费。
>

```java
try {
    while (true){
        ConsumerRecords<String, String> records = 
            consumer.poll(Duration.ofMillis(1000));
        for (ConsumerRecord<String, String> record : records) {
            System.out.println(record.value());
            System.out.println(record.key());
        }
        consumer.commitAsync();
    }
}catch (Exception e){+
    e.printStackTrace();
    System.out.println("记录错误信息："+e);
}finally {
    try {
        consumer.commitSync();
    }finally {
        consumer.close();
    }
}
```

## Springboot集成kafka🚗

### 依赖坐标

> 1.导入spring-kafka依赖信息

```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
    <!-- kafkfa -->
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka</artifactId>
        <exclusions>
            <exclusion>
                <groupId>org.apache.kafka</groupId>
                <artifactId>kafka-clients</artifactId>
            </exclusion>
        </exclusions>
    </dependency>
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-clients</artifactId>
    </dependency>
    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
    </dependency>
</dependencies>
```

### application.yml

> 2.在resources下创建文件application.yml

```yaml
server:
  port: 9991
spring:
  application:
    name: kafka-demo
  kafka:
    bootstrap-servers: 192.168.88.101:9092
    producer:
      retries: 10
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      compression-type: snappy # 消息压缩方式
    consumer:
      group-id: ${spring.application.name}-test
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
```

### HelloController

> 3.消息生产者

```java
package com.heima.kafka.controller;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class HelloController {

    @Autowired
    private KafkaTemplate<String,String> kafkaTemplate;

    @GetMapping("/hello")
    public String hello(){
        kafkaTemplate.send("itcast-topic","黑马程序员");
        return "ok";
    }
}
```

### HelloListener

> 4.消息消费者

```java
package com.heima.kafka.listener;

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;
import org.springframework.util.StringUtils;

@Component
public class HelloListener {

    @KafkaListener(topics = "itcast-topic")
    public void onMessage(String message){
        if(!StringUtils.isEmpty(message)){
            System.out.println(message);
        }
    }
}
```

### KafkaApplication

```java
@SpringBootApplication
public class KafkaApplication {
    public static void main(String[] args) {
        SpringApplication.run(KafkaApplication.class,args);
    }
}
```

### 功能测试

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701160545995.png" alt="image-20230701160545995" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701160824324.png" alt="image-20230701160824324" style="zoom:80%;" />

### 传递对象消息

> 目前springboot整合后的kafka，因为序列化器是StringSerializer，这个时候如果需要传递对象可以有两种方式方式一：可以自定义序列化器，对象类型众多，这种方式通用性不强，本章节不介绍
>

> 方式二：可以把要传递的对象进行转json字符串，接收消息后再转为对象即可，本项目采用这种方式
>

> 发送消息

```java
@Data
public class User {
    private String username;
    private Integer age;
}
```

```java
@GetMapping("/hello")
public String hello1(){
    User user = new User();
    user.setUsername("xiaowang");
    user.setAge(18);
    kafkaTemplate.send("user-topic", JSON.toJSONString(user));
    return "ok";
}
```

> 接收消息

```java
package com.heima.kafka.listener;

@Component
public class HelloListener {

    @KafkaListener(topics = "user-topic")
    public void onMessage1(String message){
        if(!StringUtils.isEmpty(message)){
            User user = JSON.parseObject(message, User.class);
            System.out.println(user);
        }
    }
}
```

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701161302423.png" alt="image-20230701161302423" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701161246083.png" alt="image-20230701161246083" style="zoom:80%;" />



# 自媒体文章上下架

## 整体思路

### 需求分析

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556090.png" alt="image-20210525180731705" style="zoom:80%;" />

> 使用Kafka进行上下架

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556096.png" alt="image-20210525180757907" style="zoom:80%;" />

> - 已发表且已上架的文章可以下架
>
> - 已发表且已下架的文章可以上架

### 流程说明

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701164657314.png" alt="image-20230701164657314" style="zoom:80%;" />

### 接口定义

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131633074.png" alt="image-20230613163349928" style="zoom:80%;" />

ResponseResult  

![image-20210528112150495](https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.3.30/202306131556929.png)

## 上下架实现

### WmNewsDto

```java
@Data
public class WmNewsDto {
    private Integer id;
    // 之前有的一大堆属性...
    // 是否上架  0 下架  1 上架
    private Short enable;             
}
```

```java
@Data
public class WmNewsDto {
    private Integer id;
    //标题
    private String title;
    //频道id
    private Integer channelId;
    //标签
    private String labels;
    //发布时间
    private Date publishTime;
    //文章内容
    private String content;
    //文章封面类型  0 无图 1 单图 3 多图 -1 自动
    private Short type;
    //提交时间
    private Date submitedTime;
    //状态 提交为1  草稿为0
    private Short status;
    //封面图片列表 多张图以逗号隔开
    private List<String> images;
    // 是否上架  0 下架  1 上架
    private Short enable;
}
```

### WmNewsController

> 在heima-leadnews-wemedia工程下的WmNewsController新增方法
>

```java
@PostMapping("/down_or_up")
public ResponseResult downOrUp(@RequestBody WmNewsDto dto){
    return wmNewsService.downOrUp(dto);
}
```

### WmNewsService

> 在WmNewsService新增方法
>

```java
// 文章的上下架
public ResponseResult downOrUp(WmNewsDto dto);
```

> 实现方法
>

```java
// 文章的上下架
@Override
public ResponseResult downOrUp(WmNewsDto dto) {
    //1.检查参数
    if(dto.getId() == null){
        return ResponseResult.errorResult(AppHttpCodeEnum.PARAM_INVALID);
    }
    //2.查询文章
    WmNews wmNews = getById(dto.getId());
    if(wmNews == null){
        return ResponseResult.errorResult(AppHttpCodeEnum.DATA_NOT_EXIST,"文章不存在");
    }

    //3.判断文章是否已发布
    if(!wmNews.getStatus().equals(WmNews.Status.PUBLISHED.getCode())){
        return ResponseResult.errorResult(AppHttpCodeEnum.PARAM_INVALID,
                                          "当前文章不是发布状态，不能上下架");
    }
    //4.修改文章enable
    if(dto.getEnable() != null && dto.getEnable() > -1 && dto.getEnable() < 2){
        update(Wrappers.<WmNews>lambdaUpdate().set(WmNews::getEnable,dto.getEnable())
                .eq(WmNews::getId,wmNews.getId()));
    }
    return ResponseResult.okResult(AppHttpCodeEnum.SUCCESS);
}
```

### 功能测试

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701171340739.png" alt="image-20230701171340739" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701171349240.png" alt="image-20230701171349240" style="zoom:80%;" />

## 消息通知-数据接收

### 消息发送

> 在heima-leadnews-common模块下导入kafka依赖

```xml
<!-- kafkfa -->
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
</dependency>
```

> 在自媒体端wemedia的nacos配置中心配置kafka的生产者

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701171635031.png" alt="image-20230701171635031" style="zoom:80%;" />

```yaml
spring:
  kafka:
    bootstrap-servers: 192.168.88.101:9092
    producer:
      retries: 10
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
```

> 在自媒体端文章上下架后发送消息
>

```java
//发送消息，通知article端修改文章配置
if(wmNews.getArticleId() != null){
    Map<String,Object> map = new HashMap<>();
    map.put("articleId",wmNews.getArticleId());
    map.put("enable",dto.getEnable());
    kafkaTemplate.send(WmNewsMessageConstants.WM_NEWS_UP_OR_DOWN_TOPIC,
                       JSON.toJSONString(map));
}
```

> 完整代码

```java
// 文章的上下架
@Override
public ResponseResult downOrUp(WmNewsDto dto) {
    //1.检查参数
    if(dto.getId() == null){
        return ResponseResult.errorResult(AppHttpCodeEnum.PARAM_INVALID);
    }

    //2.查询文章
    WmNews wmNews = getById(dto.getId());
    if(wmNews == null){
        return ResponseResult.errorResult(AppHttpCodeEnum.DATA_NOT_EXIST,"文章不存在");
    }

    //3.判断文章是否已发布
    if(!wmNews.getStatus().equals(WmNews.Status.PUBLISHED.getCode())){
        return ResponseResult.errorResult(AppHttpCodeEnum.PARAM_INVALID,
                "当前文章不是发布状态，不能上下架");
    }

    //4.修改文章enable
    if(dto.getEnable() != null && dto.getEnable() > -1 && dto.getEnable() < 2){
        update(Wrappers.<WmNews>lambdaUpdate().set(WmNews::getEnable,dto.getEnable())
                .eq(WmNews::getId,wmNews.getId()));
    }

    //发送消息，通知article端修改文章配置
    if(wmNews.getArticleId() != null){
        Map<String,Object> map = new HashMap<>();
        map.put("articleId",wmNews.getArticleId());
        map.put("enable",dto.getEnable());
        kafkaTemplate.send(WmNewsMessageConstants.WM_NEWS_UP_OR_DOWN_TOPIC,
                JSON.toJSONString(map));
    }
    
    return ResponseResult.okResult(AppHttpCodeEnum.SUCCESS);
}
```

> 常量类
>

```java
public class WmNewsMessageConstants {
    public static final String WM_NEWS_UP_OR_DOWN_TOPIC="wm.news.up.or.down.topic";
}
```

### 消息接收

> 在article端的nacos配置中心配置kafka的消费者

```yaml
spring:
  kafka:
    bootstrap-servers: 192.168.88.101:9092
    consumer:
      group-id: ${spring.application.name}
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
```

> 在article端编写监听，接收数据
>

```java
package com.heima.article.listener;

@Component
@Slf4j
public class ArtilceIsDownListener {

    @Respurce
    private ApArticleConfigService apArticleConfigService;

    @KafkaListener(topics = WmNewsMessageConstants.WM_NEWS_UP_OR_DOWN_TOPIC)
    public void onMessage(String message){
        System.out.println(message);
        if(StringUtils.isNotBlank(message)){
            Map map = JSON.parseObject(message, Map.class);
            apArticleConfigService.updateByMap(map);
            log.info("article端文章配置修改，articleId={}",map.get("articleId"));
        }
    }
}
```

> 新建ApArticleConfigService
>

```java
package com.heima.article.service;

public interface ApArticleConfigService extends IService<ApArticleConfig> {

    // 修改文章配置
    public void updateByMap(Map map);
}
```

> 实现类：
>

```java
package com.heima.article.service.impl;

@Service
@Slf4j
public class ApArticleConfigServiceImpl extends ServiceImpl<ApArticleConfigMapper, ApArticleConfig> implements ApArticleConfigService {


    // 修改文章配置
    @Override
    public void updateByMap(Map map) {
        //0 下架 1 上架
        Object enable = map.get("enable");
        boolean isDown = true;
        if(enable.equals(1)){
            isDown = false;
        }
        //修改文章配置
        update(Wrappers.<ApArticleConfig>lambdaUpdate()
               .eq(ApArticleConfig::getArticleId,map                                                    .get("articleId")).set(ApArticleConfig::getIsDown,isDown));
    }
}
```

### 功能测试

> 重启所有服务，进行功能测试

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701172755939.png" alt="image-20230701172755939" style="zoom:80%;" />

<img src="https://edu-8673.oss-cn-beijing.aliyuncs.com/img2023.6.17/image-20230701172816959.png" alt="image-20230701172816959" style="zoom:80%;" />
